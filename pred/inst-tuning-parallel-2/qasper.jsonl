{"pred": "        The ground truth for fake news is established by manually inspecting the text field within the tweets to label them as containing fake news, or not containing them (according to the characterization presented before). This annotated dataset is publicly available and can be freely reused. (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no)", "answers": ["Ground truth is not established in the paper"], "all_classes": null, "length": 3141}
{"pred": "        The GhostVLAD approach is a pooling strategy that uses ghost clusters to improve the performance of language identification. It was originally proposed for face recognition but has been adapted for language identification. The approach adds ghost clusters to the NetVLAD clusters to map noisy or irrelevant content into ghost clusters. During feature aggregation, the contribution of noisy and unwanted features to normal VLAD clusters is assigned less weight, while ghost clusters absorb most of the weight. This helps to improve the performance of language identification by reducing the impact of noisy and irrelevant content. The GhostVLAD approach has been shown to outperform other pooling strategies for language identification. (unanswer", "answers": ["extension of the NetVLAD, adds Ghost clusters along with the NetVLAD clusters", "An extension of NetVLAD which replaces hard assignment-based clustering with soft assignment-based clustering with the additon o fusing Ghost clusters to deal with noisy content."], "all_classes": null, "length": 2454}
{"pred": "        68.8% to 71.8% when applied to the IEMOCAP dataset. (The model outperforms previous state-of-the-art methods by 68.8% to 71.8% when applied to the IEMOCAP dataset.) (The model outperforms previous state-of-the-art methods by 68.8% to 71.8% when applied to the IEMOCAP dataset.) (The model outperforms previous state-of-the-art methods by 68.8% to 71.8% when applied to the IEMOCAP dataset.) (The model out", "answers": ["the attention model, MDREA, also outperforms the best existing research results (WAP 0.690 to 0.688)"], "all_classes": null, "length": 3207}
{"pred": "        The additional features and context proposed are context tweets, which are the tweets that the abusive language is in response to or quotes. The context tweets are used as additional features in neural network models to improve the accuracy of classification. The context tweets are also used to better understand the intention of the user and the target of the malice. The context tweets are important in identifying the target of the abusive language. The context tweets are used to improve the baseline LSTM model for sentiment analysis. The context tweets are used to avoid data dependency and to provide more information to the neural network models. The context tweets are used to improve the accuracy of classification for", "answers": ["using tweets that one has replied or quoted to as contextual information", "text sequences of context tweets"], "all_classes": null, "length": 2060}
{"pred": "    FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney. (Note that thankful was only available during specific time spans related to certain events, as Mother's Day in May 2016.) (Section SECREF2) (Figure FIGREF3) (Table TABREF8) (Figure FIGREF9) (Section SECREF4) (Table TABREF20) (Table TABREF26) (Section SECREF6) (Section SECREF7) (Table TAB", "answers": ["FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney", "FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney."], "all_classes": null, "length": 3411}
{"pred": "        Yes, both datasets contain only English data. The SemEval dataset is a subset of the Stanford Sentiment Analysis Dataset, which consists of tweets in English. The hashtags in the SemEval dataset are also in English. The STAN dataset contains only English hashtags and their associated tweets. The STAN dataset is a subset of the Stanford Sentiment Analysis Dataset, which consists of tweets in English. The hashtags in the STAN dataset are also in English. Therefore, both datasets contain only English data. (Unanswerable) (Yes) (No) (Unanswerable) (Yes) (No) (Unanswerable) (", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3735}
{"pred": "    The proposed evaluation is based on the concept map structure and the importance of the included propositions. The goal is to assess the quality of the generated concept maps and their ability to summarize the most important content of the document cluster. The evaluation metrics include the number of concepts and relations, the connectedness of the graph, and the importance of the included propositions. The evaluation is performed by comparing the generated concept maps to gold-standard reference maps created by experts. The evaluation results are used to assess the performance of different summarization methods and to identify areas for improvement. (Unanswerable) (Unanswerable) (Unanswerable) (Un", "answers": ["Answer with content missing: (Evaluation Metrics section) Precision, Recall, F1-scores, Strict match, METEOR, ROUGE-2"], "all_classes": null, "length": 4263}
{"pred": "    The datasets used for evaluation are CNN/DailyMail, New York Times Annotated Corpus (NYT), and XSum. These datasets represent different summary styles ranging from highlights to very brief one sentence summaries. The summaries also vary with respect to the type of rewriting operations they exemplify. (CNN/DailyMail and NYT are somewhat abstractive, while XSum is highly abstractive). (CNN/DailyMail and NYT are somewhat abstractive, while XSum is highly abstractive). (CNN/DailyMail and NYT are somewhat abstractive, while XSum is highly abstractive). (CNN/Daily", "answers": ["CNN/DailyMail news highlights, New York Times Annotated Corpus, XSum", "the CNN/DailyMail news highlights dataset BIBREF24, the New York Times Annotated Corpus (NYT; BIBREF25), XSum BIBREF22"], "all_classes": null, "length": 4369}
{"pred": "    The proposed approach uses a KL divergence based energy function to learn multi-sense word embedding distributions modeled as Gaussian mixtures. It outperforms existing approaches on benchmark word similarity and entailment datasets. The KL divergence based energy function captures asymmetry in entailment datasets and enables capturing lexical relationships between senses of child words and parent words. The approach also encourages capturing different possible senses of a word by minimizing KL divergence between mixture components. The approximate KL divergence function used is stricter than previous approximations, making it more suitable for high dimensional embedding spaces. The approach also uses diagonal covariance and constrained mixture probabilities to improve computational efficiency. Overall, the proposed", "answers": ["GM$\\_$KL achieves better correlation than existing approaches for various metrics on SCWS dataset."], "all_classes": null, "length": 2189}
{"pred": "    The ensemble method works by selecting the best performing model according to validation performance and then adding the best performing model that has not been previously tried. The process is repeated until a certain number of models have been selected for the final ensemble. The resulting model is a greedy ensemble that is formed by simply averaging the predictions from the constituent single models. The algorithm is offered 10 models and selects 5 of them for the final ensemble. The ensemble method is used to improve the performance of the model by combining the strengths of multiple models. The authors believe that this approach can help to overcome the limitations of individual models and achieve better results. The ensemble", "answers": ["simply averaging the predictions from the constituent single models"], "all_classes": null, "length": 4212}
{"pred": "    The sources of the datasets are the Friends TV sitcom and Facebook messenger chats. The Friends dataset is composed of dialogues from the scripts of the Friends TV sitcom, while the EmotionPush dataset is made up of Facebook messenger chats. Both datasets are annotated by five annotators on a crowd-sourcing platform (Amazon Mechanical Turk), and the labeling work is only based on the textual content. The Friends dataset is speech-based, while the EmotionPush dataset is chat-based. The personality of a character often affects the way of speaking in the Friends dataset, while the EmotionPush dataset does not have this trait due to the anonymous mechanism.", "answers": ["Friends TV sitcom, Facebook messenger chats"], "all_classes": null, "length": 3185}
{"pred": "        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English", "answers": ["English", "Simple English"], "all_classes": null, "length": 2243}
{"pred": "    The IMDb dataset of movie reviews is used for sentiment analysis. (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable)", "answers": ["IMDb dataset of movie reviews", "IMDb"], "all_classes": null, "length": 2327}
{"pred": "    89.2% on the development set and 88.3% on the test set. (unanswerable)\n\nQuestion: What are the main contributions of the paper?\n\nAnswer: The main contributions of the paper are: (1) proposing a new approach to crowd annotation learning based on adversarial training, (2) creating two new datasets for Chinese NER tasks in the dialog and e-commerce domains, and (3) achieving better performance than strong baseline systems. (unanswerable)\n\nQuestion: What are the main differences between the baseline and proposed systems?\n\nAnswer: The main differences between the baseline and proposed systems are:", "answers": ["F1 scores of 85.99 on the DL-PS data,  75.15 on the EC-MT data and 71.53 on the EC-UQ data ", "F1 of 85.99 on the DL-PS dataset (dialog domain); 75.15 on EC-MT and 71.53 on EC-UQ (e-commerce domain)"], "all_classes": null, "length": 5310}
{"pred": "        Yes, they experimented with this new dataset. They recorded data from 19 participants and discarded the data of one of them due to technical difficulties with the eye-tracking calibration. Hence, they share the data of 18 participants. All participants are healthy adults (mean age = 34 (SD=8.3), 10 females). Their native language is English, originating from Australia, Canada, UK, USA or South Africa. Two participants are left-handed and three participants wear glasses for reading. Details on subject demographics can be found in Table 4. All participants gave written consent for their participation and the re-use of the data prior", "answers": ["No"], "all_classes": null, "length": 3445}
{"pred": "        The datasets used are Twitter posts and news articles related to finance. The tweets are crawled from the feeds of blog users who are considered experts in the finance domain, and the news articles are extracted from links included in these tweets. The set contains a total of 63,270,124 word occurrences, with a vocabulary of 97,616 distinct words. The intent classifier is trained on a set of 124 questions that the users asked, which were manually classified into a set of intent classes. The training set is increased to 415 samples, with samples per class ranging from 3 to 37. The intent classifier is trained on", "answers": ["Custom dataset with user questions; set of documents, twitter posts and news articles, all related to finance.", "a self-collected financial intents dataset in Portuguese"], "all_classes": null, "length": 13401}
{"pred": "    Energy sector achieves the best performance. (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no", "answers": ["Energy with accuracy of 0.538", "Energy"], "all_classes": null, "length": 10349}
{"pred": "    RNN-based NMT and Transformer-NMT\n\n        RNN-based NMT and Transformer-NMT\n\n    RNN-based NMT and Transformer-NMT\n\n        RNN-based NMT and Transformer-NMT\n\n    RNN-based NMT and Transformer-NMT\n\n        RNN-based NMT and Transformer-NMT\n\n    RNN-based NMT and Transformer-NMT\n\n        RNN-based NMT and Transformer-NMT\n\n    RNN-based NMT and Transformer-NMT\n\n        RNN-based NMT and Transformer-NMT\n\n    RNN-based NMT and Transformer-NMT\n\n        RNN-based NMT and", "answers": ["RNN-based NMT model, Transformer-NMT"], "all_classes": null, "length": 3708}
{"pred": "    1. A regularization term associated with neutral features 2. The maximum entropy of class distribution regularization term 3. The KL divergence between reference and predicted class distribution regularization term\n\nQuestion: How do these regularization terms help make the model more robust?\n\nAnswer: davidjl\n\n     davidjl\n\n    1. Neutral features prevent the model from biasing to the class with the dominate number of labeled features 2. Maximum entropy constrains the predicted class distribution on unlabeled data 3. KL divergence controls the unbalance in labeled features and in the dataset\n\nQuestion: What is the main contribution of this work?\n\nAnswer: davidjl\n\n    ", "answers": ["a regularization term associated with neutral features, the maximum entropy of class distribution regularization term, the KL divergence between reference and predicted class distribution", "a regularization term associated with neutral features,  the maximum entropy of class distribution, KL divergence between reference and predicted class distribution"], "all_classes": null, "length": 3604}
{"pred": "    1) SVM with unigram, bigram, and trigram features, which is a standard yet rather strong classifier for text features; 2) SVM with average word embedding, where a document is represented as a continuous representation by averaging the embeddings of the composite words; 3) SVM with average transformed word embeddings (the INLINEFORM0 in equation EQREF6), where a document is represented as a continuous representation by averaging the transformed embeddings of the composite words; 4) two mature deep learning models on text classification, CNN and Recurrent Convolutional Neural Networks (RCNN), where the hyperparameters are based on their", "answers": ["SVM with unigram, bigram, and trigram features, SVM with average word embedding, SVM with average transformed word embeddings, CNN, ecurrent Convolutional Neural Networks, SVM and deep learning models with comment information", "SVM with unigram, bigram, trigram features, with average word embedding, with average transformed word embeddings, CNN and RCNN, SVM, CNN, RCNN with comment information"], "all_classes": null, "length": 4512}
{"pred": "        5.2 points (from 59.1 to 64.3) (in terms of macro-averaged Mean Absolute Error) (in the fine-grained setting) (using the nbow+ representation) (compared to the baseline systems) (with the biLSTM network) (with multitask learning) (compared to the state-of-the-art system of BIBREF2) (with the nbow+ representation) (compared to the baseline systems) (with the biLSTM network) (with multitask learning) (compared to the state-of-the-art system of BIBREF", "answers": ["They decrease MAE in 0.34"], "all_classes": null, "length": 2735}
{"pred": "    Their model improves interpretability by allowing different attention heads to learn different sparsity behaviors, which can be analyzed to identify head specializations. The sparsity of the attention weights also allows for more confident representations of certain types of tokens, such as positional heads. Additionally, the adaptivity of the model allows for continuous and dynamic changes in the shape of the attention heads, which can be learned automatically. Overall, the adaptively sparse transformers provide a more interpretable and flexible model for NLP tasks.", "answers": ["the attention heads in the proposed adaptively sparse Transformer can specialize more and with higher confidence", "We introduce sparse attention into the Transformer architecture"], "all_classes": null, "length": 4902}
{"pred": "    The baseline is the context-agnostic MT system that produces sentence-level translations. The DocRepair model is trained to correct inconsistencies between these translations. The baseline is used as input for the DocRepair model. The DocRepair model is trained to correct inconsistencies between sentence-level translations of adjacent sentences. The key idea is to use monolingual data to imitate typical inconsistencies between context-agnostic translations of isolated sentences. The DocRepair model is trained to map inconsistent groups of sentences into consistent ones. The consistent groups come from the original training data; the inconsistent groups are obtained by sampling round-trip translations for each isolated sentence. The DocRepair model", "answers": [" MT system on the data released by BIBREF11", "Transformer base, two-pass CADec model"], "all_classes": null, "length": 3716}
{"pred": "    The metrics used for evaluation are Labeled Attachment Scores (LAS) for zero-shot dependency parsing and accuracy for cross-lingual natural language inference (XNLI). The LAS metric measures the percentage of correctly labeled dependencies in a treebank, while the accuracy metric measures the percentage of correctly classified sentences in the XNLI dataset. The authors also report the results of previous work for comparison. (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable)", "answers": ["translation probabilities, Labeled Attachment Scores (LAS)", "accuracy, Labeled Attachment Scores (LAS)"], "all_classes": null, "length": 3402}
{"pred": "    The attention module is pretrained on the ASR and MT tasks. The attention module is task-specific, so it is pretrained on the tasks that it will be used for. This allows the attention module to learn the alignments between the input and output sequences for each task. The attention module is not pretrained on the ST task, as it is not used for that task. However, the attention module is still able to learn the alignments between the input and output sequences for the ST task, as it is a general-purpose attention module that can be used for any task. The attention module is not pretrained on the ST task, but it is still", "answers": ["the model is pre-trained on CTC-based ASR task and MT task in the pre-training stage."], "all_classes": null, "length": 4656}
{"pred": "    The stylistic features obtained are emoticons, laughter expressions such as \"lol\", and patterns related to situational disparity. These features are used to detect sarcasm in text. However, detecting sarcasm in linguistically well-formed structures, in absence of explicit cues or information, proves to be hard using such linguistic/stylistic features alone. (BIBREF6, BIBREF7) (BIBREF6, BIBREF7) (BIBREF6, BIBREF7) (BIBREF6, BIBREF7) (BIBREF6, BIBREF7) (BIB", "answers": ["Unanswerable"], "all_classes": null, "length": 3543}
{"pred": "    The encoder has an LSTM layer. (1) The encoder has an LSTM layer. (2) The encoder has an LSTM layer. (3) The encoder has an LSTM layer. (4) The encoder has an LSTM layer. (5) The encoder has an LSTM layer. (6) The encoder has an LSTM layer. (7) The encoder has an LSTM layer. (8) The encoder has an LSTM layer. (9) The encoder has an LSTM layer. (10) The encoder has an LSTM layer. (11) The encoder has an LSTM layer. (12) The encoder has an LSTM layer.", "answers": ["LSTM", "LSTM"], "all_classes": null, "length": 2289}
{"pred": "    Yes, WordNet is useful for taxonomic reasoning for this task. It provides a set of triples that represent a directed, edge-labeled graph, which allows for the construction of natural language questions that contextualize the types of concepts being probed. The availability of glosses and example sentences in WordNet allows for the creation of natural language questions that contextualize the types of concepts being probed. The use of WordNet also allows for the construction of questions that involve complex forms of reasoning, such as ISA reasoning, which requires several inferential steps or hops. The availability of glosses and example sentences in WordNet allows for the", "answers": ["Unanswerable", "Yes"], "all_classes": null, "length": 6391}
{"pred": "        The baselines were wav2letter and other non end-to end models. The goal was to match or outperform these baselines. The architecture was designed to be highly efficient for training and inference on GPUs. The authors found that ReLU and batch normalization outperformed other activation and normalization schemes for convolutional ASR. The authors also investigated a number of residual options and proposed a new residual connection topology called Dense Residual (DR). The authors integrated their best acoustic model with a Transformer-XL language model to obtain new state-of-the-art results on LibriSpeech and competitive results on other benchmarks. The authors also used Novo", "answers": ["Unanswerable", "LF-MMI Attention\nSeq2Seq \nRNN-T \nChar E2E LF-MMI \nPhone E2E LF-MMI \nCTC + Gram-CTC"], "all_classes": null, "length": 1856}
{"pred": "        22,880 users", "answers": ["22,880 users", "20,000"], "all_classes": null, "length": 4160}
{"pred": "    BPE perplexity, BLEU-1/4, ROUGE-L, Distinct-1/2, Recipe Level Coherence, Recipe Step Entailment, Human Evaluation\n\nQuestion: How do the personalized models perform compared to the baseline?\n\nAnswer:?\"\n\n\n\n        The personalized models outperform the baseline in BPE perplexity, but perform worse in BLEU-1/4 and ROUGE-L. They generate more diverse and acceptable recipes, but the metrics are not the most appropriate for evaluating generation quality. The Prior Name model performs the best in BPE perplexity and generates more unigram-diverse recipes than other personalized models", "answers": ["Byte-Pair Encoding perplexity  (BPE PPL),\nBLEU-1,\nBLEU-4,\nROUGE-L,\npercentage of distinct unigram (D-1),\npercentage of distinct bigrams(D-2),\nuser matching accuracy(UMA),\nMean Reciprocal Rank(MRR)\nPairwise preference over baseline(PP)", "BLEU-1/4 and ROUGE-L, likelihood of generated recipes using identical input specifications but conditioned on ten different user profiles, user matching accuracy (UMA), Mean Reciprocal Rank (MRR), neural scoring model from BIBREF33 to measure recipe-level coherence", " Distinct-1/2, UMA = User Matching Accuracy, MRR\n= Mean Reciprocal Rank, PP = Pairwise preference over baseline (evaluated for 310 recipe pairs per model)"], "all_classes": null, "length": 2673}
{"pred": "        They create labels on their dataset by specifying a symptom and an attribute, and then labeling the groundtruth output of the QA system based on the template generation rules. Moreover, they adopt the unanswerable design when the patient does not mention a particular symptom, and the answer is defined as \"No Answer\". This process is repeated until all logical permutations of symptoms and attributes are exhausted. (unanswerable)\r\r\n\r\r\n        They create labels on their dataset by specifying a symptom and an attribute, and then labeling the groundtruth output of the QA system based on the template generation rules. Moreover, they adopt the unanswerable design when the patient", "answers": ["(1) the time the patient has been experiencing the symptom, (2) activities that trigger the symptom (to occur or worsen), (3) the extent of seriousness, (4) the frequency occurrence of the symptom, and (5) the location of symptom, No Answer", "the time the patient has been experiencing the symptom, activities that trigger the symptom, the extent of seriousness, the frequency occurrence of the symptom, the location of symptom, 9 symptoms"], "all_classes": null, "length": 3424}
{"pred": "        1000 sentences (approximately 5k abstracts) with expert annotations and 5k abstracts with crowd annotations. The difficulty of the sentences is predicted using a neural model. The model is trained on 9/10ths of the data and the remaining 1/10th is used to predict the difficulty of the sentences. The sentences are then sorted by predicted difficulty and the most difficult sentences are removed from the training set. The model is retrained on the remaining data and the performance is evaluated on a nested development set. The model is also retrained on the full training set with sentences re-weighted inversely to", "answers": ["57,505 sentences", "57,505 sentences"], "all_classes": null, "length": 4371}
{"pred": "    Neural machine translation (NMT) and other tasks such as improving the state of the art of Neural Machine Translation, BERT, and GPT-2. The tasks involve translating between different languages and analyzing the behavior of attention heads.΄\n\nQuestion: What are the main contributions of the work?\n\nAnswer: 노출등록\n\n     노출등록\n\n    The main contributions of the work are the introduction of sparse attention into the Transformer architecture, the proposal of an adaptive version of sparse attention, and an extensive analysis of the added interpretability of these models.΄\n\nQuestion: What are the main challenges in the work?\n\nAnswer: 노출등록\n\n     노출등록", "answers": ["four machine translation tasks: German -> English, Japanese -> English, Romanian -> English, English -> German", " four machine translation tasks, IWSLT 2017 German $\\rightarrow $ English BIBREF27, KFTT Japanese $\\rightarrow $ English BIBREF28, WMT 2016 Romanian $\\rightarrow $ English BIBREF29, WMT 2014 English $\\rightarrow $ German BIBREF30"], "all_classes": null, "length": 4898}
{"pred": "    The improvement in performance for Estonian in the NER task is 0.12. The baseline fastText embeddings achieve a Macro F1 score of 0.68, while the ELMo embeddings achieve a score of 0.80. This represents a 12% improvement over the baseline. The ELMo embeddings are able to capture more contextual information and perform better on the NER task. The results show that the newly produced contextual embeddings produce substantially better results compared to the non-contextual fastText baseline. (unanswerable) (yes) (no) (unanswerable) (yes) (no)", "answers": ["5 percent points.", "0.05 F1"], "all_classes": null, "length": 3290}
{"pred": "    They have experience in analyzing text as social and cultural data and have consolidated their experiences in this article. They come from different disciplines and have different perspectives on the research process. They discuss how the research process often unfolds and the challenges they face. They also provide insights into how computational text analysis can help us better understand social and cultural phenomena. (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable)", "answers": ["Unanswerable"], "all_classes": null, "length": 8506}
{"pred": "    Yes, the paper introduces an unsupervised approach to spam detection using LDA to extract topic-based features. The proposed features are used to distinguish human-like spammers from legitimate users. The paper claims that the proposed features are effective in detecting \"smart\" spammers who post seemingly legitimate tweets. The paper also compares the proposed features with other state-of-the-art methods and shows that they get excellent performance on human-like spammer classification. The paper concludes that the proposed features are a promising direction for future research in spam detection. (unanswerable) (yes) (no) (unanswerable) (yes) (no)", "answers": ["No", "No"], "all_classes": null, "length": 2239}
{"pred": "        The Nguni languages are similar to each other and harder to distinguish. The same is true of the Sotho languages. (Section 2.1) (Article: Introduction) (Answer: The Nguni languages are similar to each other and harder to distinguish. The same is true of the Sotho languages. (Section 2.1)) (Article: Introduction) (Question: Which languages are similar to each other?) (Answer: The Nguni languages are similar to each other and harder to distinguish. The same is true of the Sotho languages. (Section 2.1)) (", "answers": ["Nguni languages (zul, xho, nbl, ssw), Sotho languages (nso, sot, tsn)", "The Nguni languages are similar to each other, The same is true of the Sotho languages"], "all_classes": null, "length": 1877}
{"pred": "    They compared the performance of 6-layers and 9-layers sMBR models. The 9-layers model performs better than the 6-layers model, with a relative 12.6% decrease in CER. The averaged CER of sMBR models with different layers also decreases by about 0.73% compared with CE models, indicating the effectiveness of sequence discriminative learning. The 9-layers model is initialized by the 8-layers sMBR model, while the other teacher models are CE models. The 8-layers sMBR model is chosen as the teacher model for", "answers": ["Unidirectional LSTM networks with 2, 6, 7, 8, and  9 layers."], "all_classes": null, "length": 4286}
{"pred": "    The Wikipedia dataset consists of articles from English Wikipedia, with quality class labels assigned by the Wikipedia community. The quality class of a Wikipedia article is assigned by Wikipedia reviewers or any registered user, who can discuss through the article's talk page to reach consensus. The dataset is randomly partitioned into training, development, and test splits based on a ratio of 8:1:1. Details of the dataset are summarized in Table 1. The arXiv dataset consists of three subsets of academic articles under the arXiv repository of Computer Science (cs), from the three subject areas of: Artificial Intelligence (cs.ai), Computation and", "answers": ["a sample of  29,794 wikipedia articles and 2,794 arXiv papers "], "all_classes": null, "length": 4187}
{"pred": "        A group of 50 native people who were well-versed in both English and Tamil languages acted as annotators for the evaluation. A collection of samples of about 100 sentences were taken from the test set results for comparison. This set included a randomized selection of the translation results to ensure the objectivity of evaluation. Fluency and adequacy results for the RNNMorph results are tabulated. Adequacy rating was calculated on a 5-point scale of how much of the meaning is conveyed by the translation (All, Most, Much, Little, None). The fluency rating was calculated based on grammatical correctness on a", "answers": ["50 human annotators ranked a random sample of 100 translations by Adequacy, Fluency and overall ranking on a 5-point scale.", "adequacy, precision and ranking values"], "all_classes": null, "length": 5344}
{"pred": "    Yes, they test their framework performance on commonly used language pairs, such as English-to-German. They also test their framework performance on other language pairs, such as French-to-German and English-to-French. Their results show that their framework can improve the performance of NMT on these language pairs. They also show that their framework can help reduce the number of rare words in the target language. Their results are promising and show that their framework can be a useful tool for improving the performance of NMT on multilingual translation tasks. (unanswerable) (yes) (no) (unanswerable) (yes) (", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4472}
{"pred": "    The models are evaluated based on the efficiency of the communication scheme and the accuracy of the reconstruction. The efficiency is measured by the retention rate of tokens, while the accuracy is measured by the fraction of sentences generated by greedily decoding the model that exactly matches the target sentence. The models are also evaluated based on their robustness and ability to adapt to different types of sentences. The user study shows that the models are efficient and accurate, and users can easily adapt to the autocomplete system. The models are also able to handle different types of sentences and achieve high accuracy in reconstructing the keywords. Overall, the models are effective in balancing efficiency and", "answers": ["by training an autocomplete system on 500K randomly sampled sentences from Yelp reviews", "efficiency of a communication scheme $(q_{\\alpha },p_{\\beta })$ by the retention rate of tokens, which is measured as the fraction of tokens that are kept in the keywords, accuracy of a scheme is measured as the fraction of sentences generated by greedily decoding the model that exactly matches the target sentence"], "all_classes": null, "length": 1873}
{"pred": "        Precision, Recall, and F-measure are the most common evaluation metrics for classification tasks. Precision measures the proportion of true positives to all predicted positives, while Recall measures the proportion of true positives to all actual positives. F-measure is a weighted average of Precision and Recall, which provides a single score that combines both metrics. These metrics are used to evaluate the performance of a classification model and help determine whether it is accurate and reliable. (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes", "answers": ["Precision, Recall, F-measure, accuracy", "Precision, Recall and F-measure"], "all_classes": null, "length": 3044}
{"pred": "    The source domain is the existing domain with sufficient labeled data, while the target domain is the new domain with very few or no labeled data. The goal is to transfer knowledge from the source domain to the target domain to alleviate the required labeling effort. (unanswerable)\n\nQuestion: What is the key challenge of domain adaptation?\n\nAnswer: The key challenge of domain adaptation is that data in the source and target domains are drawn from different distributions, which makes adaptation performance decline with an increase in distribution difference. (unanswerable)\n\nQuestion: What are some techniques for addressing the problem of domain shifting?\n\nAnswer: Some techniques for addressing the problem", "answers": ["Book, electronics, beauty, music, IMDB, Yelp, cell phone, baby, DVDs, kitchen", "we use set 1 of the source domain as the only source with sentiment label information during training, and we evaluate the trained model on set 1 of the target domain, Book (BK), Electronics (E), Beauty (BT), and Music (M)"], "all_classes": null, "length": 5061}
{"pred": "    They compare with LSTM and similar recurrent units. They also mention other gating structures and transformations that have been proposed to improve the performance of RNNs. They describe recent work in language modeling and how it relates to their proposed PRU architecture. They also mention that they will study the performance of PRUs on different tasks, including machine translation and question answering. They conclude by acknowledging the support of various organizations and individuals for their research. (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes)", "answers": ["Variational LSTM, CharCNN, Pointer Sentinel-LSTM, RHN, NAS Cell, SRU, QRNN, RAN, 4-layer skip-connection LSTM, AWD-LSTM, Quantized LSTM"], "all_classes": null, "length": 3319}
{"pred": "    The following neural network modules are included in NeuronBlocks: embedding layer, neural network layers (RNN, CNN, QRNN, Transformer, Highway network, Encoder Decoder architecture, attention mechanisms, regularization layers), loss function, metrics.\n\n    The embedding layer supports word/character embedding and extra handcrafted feature embedding such as pos-tagging. The neural network layers include RNN, CNN, QRNN, Transformer, Highway network, Encoder Decoder architecture, attention mechanisms (Linear/Bi-linear Attention, Full Attention, Bidirectional attention flow), and regularization layers (Dropout, Layer Norm, Batch Norm). The loss function includes built-in Py", "answers": ["Embedding Layer, Neural Network Layers, Loss Function, Metrics", "Embedding Layer, Neural Network Layers, Loss Function, Metrics"], "all_classes": null, "length": 1678}
{"pred": "        The datasets they used were the Carnegie Mellon Pronouncing Dictionary and the multilingual pronunciation corpus collected by deri2016grapheme. The corpus consists of spelling-pronunciation pairs extracted from Wiktionary. The corpus is already partitioned into training and test sets. The corpus statistics are presented in Table 10. The raw IPA transcriptions extracted from Wiktionary are cleaned to make them consistent with the phonemic inventories used in Phoible. The cleaning algorithm replaces phonemes that are not in the language's inventory with the phoneme with the most similar articulatory features that is in the language's inventory. The cleaning algorithm", "answers": ["the Carnegie Mellon Pronouncing Dictionary BIBREF12, the multilingual pronunciation corpus collected by deri2016grapheme , ranscriptions extracted from Wiktionary", "multilingual pronunciation corpus collected by deri2016grapheme"], "all_classes": null, "length": 3244}
{"pred": "        The baselines were the results reported by Khandelwal and Sawant (BIBREF12) for BERT on the speculation detection and scope resolution tasks. The results for XLNet and RoBERTa are averaged across 5 runs for statistical significance. (BIBREF12) (BIBREF12) (BIBREF12) (BIBREF12) (BIBREF12) (BIBREF12) (BIBREF12) (BIBREF12) (BIBREF12) (BIBREF12) (BIBREF12) (BIBREF12) (BIB", "answers": ["varied from Maximum Entropy Classifiers (BIBREF4) to Support Vector Machines (BIBREF5,BIBREF6,BIBREF7,BIBREF8), Recursive Neural Networks (BIBREF9,BIBREF10), Convolutional Neural Networks (BIBREF11) and most recently transfer learning-based architectures like Bidirectional Encoder Representation from Transformers (BERT) (BIBREF12)"], "all_classes": null, "length": 2215}
{"pred": "    English, Spanish, Finnish, and other languages. They use these languages to evaluate the performance of cross-lingual models. They also use machine translation systems to translate the test sets into English and back to the target languages. (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable)", "answers": ["English\nFrench\nSpanish\nGerman\nGreek\nBulgarian\nRussian\nTurkish\nArabic\nVietnamese\nThai\nChinese\nHindi\nSwahili\nUrdu\nFinnish", "English, Spanish, Finnish"], "all_classes": null, "length": 4086}
{"pred": "    They test their method on Named Entity Recognition, POS tagging, text classification, and language modeling. They also compare their method to previous work on estimating rare word representations and hashtag prediction for social media. Their method outperforms previous work on both tasks. They also show that their method can be used for other NLP tasks such as Named Entity Recognition, POS tagging, text classification, and language modeling. They also show that their method can be used for other NLP tasks such as Named Entity Recognition, POS tagging, text classification, and language modeling. They also show that their method can be used for other NLP tasks such as Named", "answers": ["None"], "all_classes": null, "length": 2473}
{"pred": "Yes, they use pretrained Glove embeddings for the top 20K words in the vocabulary. They initialize the embeddings of these words with 300-dimensional Glove embeddings. They also tune the hyperparameters of all the models using a validation set. They use Adam with a learning rate of 0.001, 0.0001, and 0.00001. They train the model for a maximum of 20 epochs and use early stopping with the patience set to 5 epochs. They also use a copying mechanism as a post-processing step to deal with the large vocabulary. They identify the time steps at which the decoder produces unknown", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4457}
{"pred": "    Yes, PolyReponse was evaluated against some baseline. The baseline was a traditional task-oriented dialogue system that relies on explicit semantic representations such as dialogue acts or slot-value ontologies. The evaluation showed that PolyReponse outperformed the baseline in terms of task completion rate and user satisfaction. The results suggest that the general approach to search-based dialogue used by PolyReponse is more effective than the traditional approach. However, more research is needed to fully understand the advantages and limitations of this approach. (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (", "answers": ["No", "No"], "all_classes": null, "length": 2738}
{"pred": "        The authors obtain psychological dimensions of people by analyzing the language used in their blogs. They use a tool called LIWC to analyze the language and extract psychological dimensions such as positive emotions, negative emotions, and cognitive processes. They then use these dimensions to create maps that show the distribution of these dimensions across the US. These maps can provide insights into the psychological characteristics of different regions and populations. (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (", "answers": ["using the Meaning Extraction Method", "Unanswerable"], "all_classes": null, "length": 1440}
{"pred": "    The ML methods aim to identify the following argument components: claims, premises, backing, rebuttals, and refutations. These components are used to support or attack the claim. The methods use a variety of linguistic features to identify these components. (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (", "answers": ["claim, premise, backing, rebuttal, and refutation", "claim, premise, backing, rebuttal, refutation"], "all_classes": null, "length": 14472}
{"pred": "    Ngrams of length 3 are aligned using PARENT. The precision and recall of n-grams from the table and reference are computed separately, and then combined to give the final score. PARENT is more effective than using the table as an additional reference, as it rewards correct information missing from the reference and ignores extra incorrect information in the reference. The union and intersection are computed with the help of an entailment model to decide if a text n-gram is entailed by the table. PARENT has higher correlation with human judgments than existing automatic metrics, especially when the references are divergent. It is also comparable to the best", "answers": ["Unanswerable", "Answer with content missing: (Parent subsections) combine precisions for n-gram orders 1-4"], "all_classes": null, "length": 3827}
{"pred": "        1,873 Twitter conversation threads, roughly 14k tweets (approximately 1.5 million comments) (approximately 1.5 million comments) (approximately 1.5 million comments) (approximately 1.5 million comments) (approximately 1.5 million comments) (approximately 1.5 million comments) (approximately 1.5 million comments) (approximately 1.5 million comments) (approximately 1.5 million comments) (approximately 1.5 million comments) (approximately 1.5 million comments) (approximately 1.5 million comments) (approximately 1.5", "answers": ["1,873 Twitter conversation threads, roughly 14k tweets", "1,873 Twitter conversation threads, roughly 14k tweets"], "all_classes": null, "length": 3721}
{"pred": "        12 languages: English, Mandarin Chinese, Yue Chinese, Finnish, Spanish, French, Russian, Polish, Hebrew, Estonian, Welsh, and Kiswahili.        12 languages: English, Mandarin Chinese, Yue Chinese, Finnish, Spanish, French, Russian, Polish, Hebrew, Estonian, Welsh, and Kiswahili.        12 languages: English, Mandarin Chinese, Yue Chinese, Finnish, Spanish, French, Russian, Polish, Hebrew, Estonian, Welsh, and Kiswahili.        12 languages: English, Mandarin Chinese, Yue Chinese, Finnish, Spanish, French, Russian", "answers": ["Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese", "Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese"], "all_classes": null, "length": 14660}
{"pred": "    Wikipedia data and Reddit CMV data. The Wikipedia data is an expanded version of the annotated Wikipedia conversations dataset from BIBREF9, while the Reddit CMV data is constructed from conversations collected via the Reddit API. Both datasets are used to evaluate the performance of the model in forecasting derailment of online conversations. (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no", "answers": [" `Conversations Gone Awry' dataset, subreddit ChangeMyView", "An expanded version of the existing 'Conversations Gone Awry' dataset and the ChangeMyView dataset, a subreddit whose only annotation is whether the conversation required action by the Reddit moderators. "], "all_classes": null, "length": 4718}
{"pred": "        No, the pipeline components were not based on deep learning models. The pipeline components were based on existing NLP tools and techniques, such as part-of-speech tagging, named entity recognition, dependency parsing, and semantic role labeling. These tools and techniques are not based on deep learning models, but rather on statistical and rule-based approaches. The pipeline components were designed to be modular and language-independent, allowing for the potential application to other languages by simply changing the modules or models. The authors also evaluated several existing systems and found that their proposed system outperformed them in terms of full processing pipeline for Portuguese texts. The authors also mentioned that", "answers": ["No", "No"], "all_classes": null, "length": 2276}
{"pred": "    The quality of the data is empirically evaluated by applying various sanity checks to the translations. These checks include computing sentence-level BLEU scores with the NLTK implementation between the human translations and the automatic translations produced by a state-of-the-art system, manually inspecting examples where the source transcript is identical to the translation, measuring the perplexity of the translations using a language model trained on a large amount of clean monolingual data, computing the ratio of English characters in the translations, and calculating similarity scores between transcripts and translations based on LASER cross-lingual sentence embeddings. Samples with low scores are manually inspected and sent back for translation", "answers": ["Validated transcripts were sent to professional translators., various sanity checks to the translations,  sanity check the overlaps of train, development and test sets", "computed sentence-level BLEU, We manually inspected examples where the source transcript was identical to the translation, measured the perplexity of the translations, computed the ratio of English characters in the translations, calculate similarity scores between transcripts and translations"], "all_classes": null, "length": 2435}
{"pred": "    They combine audio and text sequences in their RNN by concatenating the last hidden state of the audio-RNN with the final encoding vector of the text-RNN. The resulting vector is then passed through a fully connected neural network layer to form the final encoding vector. The emotion class is predicted by applying the softmax function to this vector. (EQREF7) (EQREF9) (EQREF10) (EQREF11) (EQREF12) (EQREF13) (EQREF14) (EQREF15) (EQREF16) (EQREF17) (EQREF18) (EQREF19)", "answers": ["combines the information from these sources using a feed-forward neural model", "encodes the information from audio and text sequences using dual RNNs and then combines the information from these sources using a feed-forward neural model"], "all_classes": null, "length": 3201}
{"pred": "        2.11 BLEU, 1.7 FKGL and 1.07 SARI. (The model improved by 2.11 BLEU, 1.7 FKGL and 1.07 SARI.) (The model improved by 2.11 BLEU, 1.7 FKGL and 1.07 SARI.) (The model improved by 2.11 BLEU, 1.7 FKGL and 1.07 SARI.) (The model improved by 2.11 BLEU, 1.7 FKGL and 1.07 SARI.) (The", "answers": ["For the WikiLarge dataset, the improvement over baseline NMT is 2.11 BLEU, 1.7 FKGL and 1.07 SARI.\nFor the WikiSmall dataset, the improvement over baseline NMT is  8.37 BLEU.", "6.37 BLEU"], "all_classes": null, "length": 2271}
{"pred": "        73% of the cases. This shows a strong preference of the annotators for corrected translations over the baseline ones. (Table TABREF30) (Human evaluation) (Results) (Section 5.3) (Human evaluation) (Results) (Table TABREF30) (Human evaluation) (Results) (Section 5.3) (Human evaluation) (Results) (Table TABREF30) (Human evaluation) (Results) (Section 5.3) (Human evaluation) (Results) (Table TABREF30) (Human evaluation) (Results) (Section 5.3) (Human", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3711}
{"pred": "        A tweet goes viral if it is retweeted more than 1000 times. (1) A tweet goes viral if it is retweeted more than 1000 times. (1) A tweet goes viral if it is retweeted more than 1000 times. (1) A tweet goes viral if it is retweeted more than 1000 times. (1) A tweet goes viral if it is retweeted more than 1000 times. (1) A tweet goes viral if it is retweeted more than 1000 times. (1) A tweet goes viral if it is retweeted more than 100", "answers": ["Viral tweets are the ones that are retweeted more than 1000 times", "those that contain a high number of retweets"], "all_classes": null, "length": 3144}
{"pred": "    BERT performs best by itself. It achieves state-of-the-art performance on multiple NLP benchmarks. However, it is not the only architecture that performs well. Other architectures such as CNN and LSTM-CRF also perform well when combined with the right features and ensemble schemes. The choice of architecture depends on the specific task and dataset. In this case, the task is fine-grained propaganda detection and the dataset is the EMNLP19DaSanMartino dataset. The authors found that BERT performs best by itself, but they also found that combining BERT with other architectures and features can improve performance even further. The authors also found", "answers": ["BERT"], "all_classes": null, "length": 1507}
{"pred": "The data was collected using crowdsourcing and an Android application. Each respondent installed the application on their personal device and recorded several phrases in different sessions. The Android application did various checks on each utterance and if it passed all of them, the respondent was directed to the next phrase. For more information about data collection scenario, please refer to BIBREF4. (DeepMine Database: A Large-Scale Speech Corpus for Text-Dependent and Text-Prompted Speaker Verification and Persian Speech Recognition, 2020) (BIBREF4: DeepMine Database: A Large-Scale Speech Corpus for Text-Dependent and Text-P", "answers": ["Android application"], "all_classes": null, "length": 3795}
{"pred": "    Machine learning and deep learning methods such as logistic regression and neural networks are used for RQE. These methods are used to recognize entailment between two questions and retrieve answers to a new question based on the answers to the entailed questions. The methods are evaluated on different datasets and compared to each other. The best performing method is selected for the RQE-based QA system. The system is built and shared with a collection of medical question-answer pairs. The proposed approach outperforms the best results on TREC-2017 LiveQA medical test questions. The approach can be applied and adapted to open-domain and specific-domain QA. Deep learning", "answers": ["Logistic Regression, neural networks"], "all_classes": null, "length": 7257}
{"pred": "    The benchmark dataset is the Social Honeypot dataset, which is a collection of legitimate users and spammers collected by Lee et al. The quality of the dataset is high, as it was collected over a period of 7 months and contains detailed information about the user behavior and user profile features. The dataset has been extensively explored in the paper and has been used to validate the effectiveness of the proposed features. (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (", "answers": ["Social Honeypot dataset (public) and Weibo dataset (self-collected); yes", "Social Honeypot, which is not of high quality"], "all_classes": null, "length": 2242}
{"pred": "    The decoder has an LSTM one component at a time, e.g. the tag PRO;NOM;SG;1 is predicted as a sequence of four components, PRO, NOM, SG, 1. (1) The decoder has an LSTM one component at a time, e.g. the tag PRO;NOM;SG;1 is predicted as a sequence of four components, PRO, NOM, SG, 1. (1) The decoder has an LSTM one component at a time, e.g. the tag PRO;NOM;SG;1 is predicted as a sequence of four components, PRO, NOM,", "answers": ["LSTM", "LSTM"], "all_classes": null, "length": 2289}
{"pred": "    Yes, they report results only on English data. They do not mention any results on non-English data. (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3838}
{"pred": "        BERT with ensemble of Logistic Regression, CNN and BERT is best performing model among author's submissions. It had F1 score of 0.673 on dev (external) set. (SLC) and F1 score of 0.673 on dev (external) set. (FLC) respectively. (SLC and FLC) respectively. (SLC and FLC) respectively. (SLC and FLC) respectively. (SLC and FLC) respectively. (SLC and FLC) respectively. (SLC and FLC) respectively. (SLC and FLC) respectively.", "answers": ["For SLC task, the \"ltuorp\" team  has the best performing  model (0.6323/0.6028/0.6649 for F1/P/R  respectively) and for FLC task the \"newspeak\" team  has the best performing  model (0.2488/0.2863/0.2201 for F1/P/R respectively)."], "all_classes": null, "length": 1541}
{"pred": "    The baseline is the M2M Transformer NMT model (b3) which achieved best results for most of the translation directions and competitive results for the rest. The baseline is the M2M Transformer NMT model (b3) which achieved best results for most of the translation directions and competitive results for the rest. The baseline is the M2M Transformer NMT model (b3) which achieved best results for most of the translation directions and competitive results for the rest. The baseline is the M2M Transformer NMT model (b3) which achieved best results for most of the translation directions and competitive results for the rest", "answers": ["pivot-based translation relying on a helping language BIBREF10, nduction of phrase tables from monolingual data BIBREF14 , attentional RNN-based model (RNMT) BIBREF2, Transformer model BIBREF18, bi-directional model BIBREF11, multi-to-multi (M2M) model BIBREF8, back-translation BIBREF17", "M2M Transformer"], "all_classes": null, "length": 4542}
{"pred": "    0.7033\n\nQuestion: What was their lowest precision score?\n\nAnswer:?\"\n\n\n\n    0.1119\n\nQuestion: What was their highest MRR score?\n\nAnswer:?\"\n\n\n\n    0.2862\n\nQuestion: What was their lowest MRR score?\n\nAnswer:?\"\n\n\n\n    0.0786\n\nQuestion: What was their highest F1 score?\n\nAnswer:?\"\n\n\n\n    0.2862\n\nQuestion: What was their lowest F1 score?\n\nAnswer:?\"\n\n\n\n    0.0786\n\nQuestion: What was their highest macro F1 score?\n\nAnswer:?\"\n\n\n\n    0.2862\n\nQuestion", "answers": ["0.7033", "0.7033"], "all_classes": null, "length": 6810}
{"pred": "    Word embeddings, such as word2vec, are explored in the paper. These techniques learn a representation of a word by word co-occurrence matrix. The basic idea is that the neural network learns a series of weights (the hidden layer within the neural network) that either maximizes the probability of a word given its context, referred to as the continuous bag of words (CBOW) approach, or that maximizes the probability of the context given a word, referred to as the Skip-gram approach. These approaches have been used in numerous recent papers. (Section 2.1) (Section 2.1) (", "answers": ["Skip–gram, CBOW", "integrated vector-res, vector-faith, Skip–gram, CBOW"], "all_classes": null, "length": 4259}
{"pred": "    English to Indian languages (e.g., Bengali, Gujarati, Marathi, Tamil, Malayalam).MixedReality", "answers": ["Unanswerable", "CFILT-preorder system"], "all_classes": null, "length": 2231}
{"pred": "        Yes, the paper explores extraction from electronic health records. The authors note that the outputs of BioIE systems are often used to assist in the creation of databases, or to suggest new paths for research. For example, a ranked list of interacting proteins that are extracted from biomedical literature, but are not present in existing databases, can allow researchers to make informed decisions about which protein/gene to study further. Interactions between drugs are necessary for clinicians who simultaneously administer multiple drugs to their patients. A database of diseases, treatments and tests is beneficial for doctors consulting in complicated medical cases. The main problems in BioIE are similar to those in Information", "answers": ["Yes"], "all_classes": null, "length": 3035}
{"pred": "        The experts used for annotation were legal experts with training in privacy policies. They identified relevant evidence within the privacy policy, as well as provided meta-annotation on the question's relevance, subjectivity, OPP-115 category, and how likely any privacy policy is to contain the answer to the question asked. Their judgements can be considered valid, legally-informed opinions even when their perspectives differ. (Section 4.2.2)\r\r\n\r\r\n        The experts used for annotation were legal experts with training in privacy policies. They identified relevant evidence within the privacy policy, as well as provided meta-annotation on the question's relevance, subjectivity", "answers": ["Individuals with legal training", "Yes"], "all_classes": null, "length": 3846}
{"pred": "    The models used for painting embedding are CNNs, and for language style transfer are seq2seq models. The models are combined using an intermediate representation. The seq2seq model performs better in practice using global attention as compared with local attention. The models are made publicly available. (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (", "answers": ["generating a poem from images we use an existing actor-critic architecture, various types of sequence to sequence models"], "all_classes": null, "length": 1653}
{"pred": "    The RNN layer works better on top of BERT for long documents. The RNN layer is able to capture long-term dependencies and provide a more accurate representation of the document. The transformer layer is better suited for short documents, but it is not as effective for long documents. The RNN layer is able to handle the long-term dependencies and provide a more accurate representation of the document. The transformer layer is better suited for short documents, but it is not as effective for long documents. The RNN layer is able to handle the long-term dependencies and provide a more accurate representation of the document. The transformer layer is better suited for", "answers": ["Transformer over BERT (ToBERT)", "The transformer layer"], "all_classes": null, "length": 2655}
{"pred": "    Yes, the authors hypothesize that humans' robustness to noise is due to their general knowledge. They believe that humans can utilize general knowledge in addition to the knowledge contained in each given passage-question pair, which allows them to achieve good performance on evaluation examples without training examples. The authors also believe that humans can utilize general knowledge such as inter-word semantic connections, which is essential to their reading comprehension ability. They propose a data enrichment method to extract inter-word semantic connections from each passage-question pair and provide them as general knowledge to an MRC model. The authors also propose an end-to-end MRC model named as Knowledge Aided Reader", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4127}
{"pred": "        They addressed three topics of cyberbullying: personal attack, racism, and sexism. (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no)", "answers": ["personal attack, racism, and sexism", "racism, sexism, personal attack, not specifically about any single topic"], "all_classes": null, "length": 3244}
{"pred": "        They propose extended middle context, a new context representation for CNNs for relation classification. The extended middle context uses all parts of the sentence (the relation arguments, left of the relation arguments, between the arguments, right of the arguments) and pays special attention to the middle part. (1) They propose extended middle context, a new context representation for CNNs for relation classification. The extended middle context uses all parts of the sentence (the relation arguments, left of the relation arguments, between the arguments, right of the arguments) and pays special attention to the middle part. (2) They present connectionist bi-directional RNN", "answers": ["They use two independent convolutional and max-pooling layers on (1) a combination of the left context, the left entity and the middle context; and (2) a combination of the middle context, the right entity and the right context. They concatenated the two results after pooling to get the new context representation."], "all_classes": null, "length": 2435}
{"pred": "        4 (PER, LOC, ORG, MISC)       \n\n        4 (PER, LOC, ORG, MISC)       \n\n        4 (PER, LOC, ORG, MISC)       \n\n        4 (PER, LOC, ORG, MISC)       \n\n        4 (PER, LOC, ORG, MISC)       \n\n        4 (PER, LOC, ORG, MISC)       \n\n        4 (PER, LOC, ORG, MISC)       \n\n        4 (PER, LOC, ORG, MISC)       \n\n", "answers": ["OurNepali contains 3 different types of entities, ILPRL contains 4 different types of entities", "three"], "all_classes": null, "length": 2851}
{"pred": "    The resulting annotated data is of higher quality, as the expert annotations are more consistent and accurate than the crowd annotations. The expert annotations also provide more detailed and nuanced information, which can be useful for downstream tasks such as information extraction. The higher quality of the expert annotations can lead to better performance in downstream tasks, such as improved accuracy and precision. The expert annotations can also help to identify and correct errors in the crowd annotations, leading to a more reliable and accurate dataset overall. The higher quality of the expert annotations can also help to reduce the need for additional data collection and annotation, as the expert annotations can be used to supplement and improve", "answers": ["improvement when the difficult subset with expert annotations is mixed with the remaining crowd annotation is 3.5 F1 score, much larger than when a random set of expert annotations are added"], "all_classes": null, "length": 4399}
{"pred": "        65% of the speakers are men, speaking more than 75% of the time. (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (", "answers": ["Women represent 33.16% of the speakers"], "all_classes": null, "length": 4055}
{"pred": "        Multi30K dataset\n\n        Multi30K dataset\n\n    </p>\n\n    <p>\n\n        The approach achieves state of the art results on the Multi30K dataset for English-German translation.\n\n        The approach achieves state of the art results on the Multi30K dataset for English-German translation.\n\n    </p>\n\n    <p>\n\n        The dataset contains 29,000 instances for training, 1,014 for development, and 1,000 for test. The approach uses Flickr30K and human translations into German, French, and Czech to build the dataset.\n\n        The dataset contains 29,000 instances for", "answers": ["the English-German dataset"], "all_classes": null, "length": 1833}
{"pred": "    The strong baselines model is compared to the proposed model. The proposed model uses only unigram features instead of multiple n-gram features in previous work. The proposed model is evaluated on standard benchmark dataset, SIGHAN Bakeoff 2005, which shows not only the proposed model performs segmentation faster than any previous models but also gives new higher or comparable segmentation performance against previous state-of-the-art models. The proposed model is based on attention mechanism only and uses self-attention from the Transformer encoder to take sequence input and bi-affine attention scorer to predict the label of gaps. To improve the ability of capturing the localness and directional", "answers": ["Baseline models are:\n- Chen et al., 2015a\n- Chen et al., 2015b\n- Liu et al., 2016\n- Cai and Zhao, 2016\n- Cai et al., 2017\n- Zhou et al., 2017\n- Ma et al., 2018\n- Wang et al., 2019"], "all_classes": null, "length": 3629}
{"pred": "    Event detection on microblogging platforms such as Twitter aims to detect events preemptively. A main task in event detection is detecting events of predetermined types BIBREF0, such as concerts or controversial events based on microposts matching specific event descriptions. This task has extensive applications ranging from cyber security BIBREF1, BIBREF2 to political elections BIBREF3 or public health BIBREF4, BIBREF5. Due to the high ambiguity and inconsistency of the terms used in microposts, event detection is generally performed though statistical machine learning models, which require a labeled dataset for model training. Data", "answers": ["probabilistic model", "Logistic Regression, Multilayer Perceptron"], "all_classes": null, "length": 4475}
{"pred": "    NLP toolkits typically have the following capabilities: tokenization, part-of-speech (PoS) tagging, chunking, named entity recognition and sentiment analysis. In a study by BIBREF3, it is shown that the well-known NLP toolkits NLTK BIBREF4, Stanford CoreNLP BIBREF5, and TwitterNLP BIBREF6 have tokenization, PoS tagging and NER modules in their pipelines. There are two main approaches for NER: (1) rule-based and (2) statistical or machine learning based. The most ubiquitous algorithms for sequence tagging use Hidden Markov", "answers": ["BIBREF17, BIBREF18, TensiStrength BIBREF13, TwitterNLP BIBREF6, BIBREF19, CogComp-NLP BIBREF20, Stanford NLP NER BIBREF21", "BIBREF23, BIBREF17, BIBREF18, BIBREF19, BIBREF24, BIBREF25, BIBREF26"], "all_classes": null, "length": 1452}
{"pred": "    SQuAD dataset\n\nQuestion: What metrics are used to evaluate the performance of the proposed model?\n\nAnswer: Bf(1), Bf(2), Bf(3), Bf(4), METEOR (MET), and ROUGE-L (R-L) Bf(17), METEOR (MET) Bf(18), and ROUGE-L (R-L) Bf(19) Bf(20)\n\nQuestion: What are the main contributions of the proposed model?\n\nAnswer: The proposed model combines structured answer-relevant relations and unstructured sentences to generate questions. The structured answer-relevant relations help keep the generated questions", "answers": ["SQuAD", "SQuAD"], "all_classes": null, "length": 3757}
{"pred": "    Existing approaches for modelling urban environments and identifying points-of-interest and itineraries have been proposed, but the usefulness of Flickr for characterizing the natural environment is less well-understood. Recent studies have highlighted that Flickr tags capture valuable ecological information, but ecologists have mostly used social media to conduct manual evaluations of image content with little automated exploitation of the associated tags. One recent exception is a bag-of-words representation derived from Flickr tags that gives promising results for predicting a range of different environmental phenomena. However, the ecological information captured by Flickr tags is implicitly represented, and existing methods do not fully exploit this information. The main hypothesis in this", "answers": ["BOW-Tags, BOW-KL(Tags), BOW-All, GloVe"], "all_classes": null, "length": 4658}
{"pred": "    Yes, they use attention to compute the similarity score between passages and questions. The attention function is used to generate a working memory by fusing information from both passages and questions. The attention function is also used to apply self attention to the passage to generate a final memory representation. The attention mechanism is an important component of the model and helps to focus on relevant information in the passage and question. (Yes) (No) (Unanswerable) (Yes) (No) (Unanswerable) (Yes) (No) (Unanswerable) (Yes) (No) (Unanswerable) (Yes) (No", "answers": ["Yes", "Yes"], "all_classes": null, "length": 1687}
{"pred": "        CSAT, 20newsgroups, and Fisher datasets. (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable", "answers": ["CSAT dataset, 20 newsgroups, Fisher Phase 1 corpus", "CSAT dataset , 20 newsgroups, Fisher Phase 1 corpus"], "all_classes": null, "length": 2652}
{"pred": "    IMDb movie review dataset\n\nQuestion: What is the average document length of the IMDb movie review dataset?\n\nAnswer: 231 words\n\nQuestion: What is the best performance on the IMDb development set achieved using a four-layer densely-connected QRNN with 256 units per layer and word vectors initialized using 300-dimensional cased GloVe embeddings?\n\nAnswer: 0.85 accuracy\n\nQuestion: What is the speedup on IMDb train time per epoch compared to the optimized LSTM implementation provided in NVIDIA's cuDNN library?\n\nAnswer: 3.2x\n\nQuestion: What is the speed gain for specific batch sizes and sequence lengths?\n\nAnswer", "answers": ["the IMDb movie review dataset BIBREF17", "IMDb movie review"], "all_classes": null, "length": 3432}
{"pred": "        Yes, previous work has evaluated these tasks in the context of LSTM-based models. The BERT model is based on the Transformer architecture, which relies purely on attention mechanisms, and does not have an explicit notion of word order beyond marking each word with its absolute-position embedding. This reliance on attention may lead one to expect decreased performance on syntax-sensitive tasks compared to RNN (LSTM) models that do model word order directly, and explicitly track states across the sentence. Indeed, previous work finds that transformer-based models perform worse than LSTM models on the agreement prediction dataset. However, recent work has shown that self-attention performs on par with", "answers": ["Yes", "Yes"], "all_classes": null, "length": 1464}
{"pred": "        No, the datasets for sentiment analysis are not balanced. The datasets are often skewed towards positive or negative sentiments, which can lead to biased results. To address this issue, researchers often use techniques such as oversampling and undersampling to balance the datasets. However, these techniques can introduce other issues such as overfitting and underfitting. Therefore, it is important to carefully consider the trade-offs between balancing the datasets and other potential issues when designing sentiment analysis systems. (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (", "answers": ["No"], "all_classes": null, "length": 1441}
{"pred": "    The invertibility condition is that the neural projector is invertible, meaning that the inverse of the projection function exists. This condition allows for tractable exact inference and marginal likelihood computation. (EQREF19) (EQREF20) (EQREF22) (FIGREF24) (FIGREF35) (FIGREF54) (TABREF32) (TABREF39) (TABREF48) (TABREF50) (TABREF51) (TABREF53) (TABREF54) (SECREF5) (FIGREF6) (EQREF17) (EQREF14) (EQREF19) (", "answers": ["The neural projector must be invertible.", "we constrain our neural projector with two requirements: (1) INLINEFORM0 and (2) INLINEFORM1 exists"], "all_classes": null, "length": 4323}
{"pred": "    The proposed qualitative annotation schema looks like a high-level taxonomy of dimensions of interest, including linguistic complexity, required reasoning, and factual correctness. It also includes a metric based on lexical cues to approximate the complexity of the reading comprehension task. The full catalogue of features, their description, detailed annotation guidelines, and illustrating examples can be found in the Appendix. (Figure 10) (Appendix) (Figure 10) (Appendix) (Figure 10) (Appendix) (Figure 10) (Appendix) (Figure 10) (Appendix) (Figure 10) (Appendix) (Figure 10", "answers": ["The resulting taxonomy of the framework is shown in Figure FIGREF10", "FIGREF10"], "all_classes": null, "length": 4958}
{"pred": "        89,042 sentence pairs for WikiSmall and 296,402 sentence pairs for WikiLarge. 11.6M words and 82K vocabulary for simplified data. 2,000 for development and 359 for testing for WikiLarge. 100 pairs for testing for WikiSmall. 2.11 BLEU, 1.7 FKGL and 1.07 SARI for NMT+synthetic on WikiLarge. 6.37 BLEU for NMT+synthetic on WikiSmall. 2.11 BLEU, 1.7 FKGL and 1.07 SARI for N", "answers": ["training set has 89,042 sentence pairs, and the test set has 100 pairs, training set contains 296,402, 2,000 for development and 359 for testing", "WikiSmall  89 142 sentence pair and  WikiLarge 298 761 sentence pairs. "], "all_classes": null, "length": 2266}
{"pred": "    Vanilla ST baseline, Encoder pre-training, Decoder pre-training, Encoder-decoder pre-training, One-to-many setting, Many-to-one setting, Many-to-many setting, Many-to-many+pre-training, Triangle+pre-train\n\nQuestion: What are the experimental results?\n\nAnswer: 노출등록\n\n     노출등록\n\n        Subword-level: 3.6 BLEU, 2.2 BLEU\n\n        Character-level: 4.2 BLEU, 2.8 BLEU\n\n        Average: 3.9 BLEU, 2.5 BLEU\n\n    The many-to-many+pre-training baseline is significantly out", "answers": ["Vanilla ST baseline, encoder pre-training, in which the ST encoder is initialized from an ASR model, decoder pre-training, in which the ST decoder is initialized from an MT model, encoder-decoder pre-training, where both the encoder and decoder are pre-trained, many-to-many multi-task model where the encoders and decoders are derived from pre-trained ASR and MT models, Triangle+pre-train: BIBREF18 DBLP:conf/naacl/AnastasopoulosC18 proposed a triangle multi-task strategy for speech translation", "Vanilla ST baseline, Pre-training baselines, Multi-task baselines, Many-to-many+pre-training, Triangle+pre-train", "Vanilla ST baseline: The vanilla ST BIBREF9 has only a speech encoder and a decoder. It is trained from scratch on the ST-TED corpus.\n\nPre-training baselines: We conduct three pre-training baseline experiments: 1) encoder pre-training, in which the ST encoder is initialized from an ASR model; 2) decoder pre-training, in which the ST decoder is initialized from an MT model; and 3) encoder-decoder pre-training, where both the encoder and decoder are pre-trained. The ASR model has the same architecture with vanilla ST model, trained on the mixture of ST-TED and TED-LIUM2 corpus. The MT model has a text encoder and decoder with the same architecture of which in TCEN. It is first trained on WMT data (out-of-domain) and then fine-tuned on in-domain data.\n\nMulti-task baselines: We also conduct three multi-task baseline experiments including one-to-many setting, many-to-one setting, and many-to-many setting. In the first two settings, we train the model with $\\alpha _{st}=0.75$ while $\\alpha _{asr}=0.25$ or $\\alpha _{mt}=0.25$. For many-to-many setting, we use $\\alpha _{st}=0.6, \\alpha _{asr}=0.2$ and $\\alpha _{mt}=0.2$.. For MT task, we use only in-domain data.\n\nMany-to-many+pre-training: We train a many-to-many multi-task model where the encoders and decoders are derived from pre-trained ASR and MT models. "], "all_classes": null, "length": 4704}
{"pred": "        English, Arabic, and Chinese (Mandarin) are studied in this paper. The focus is on the challenges of imbalanced classification and the ability to generalize on dissimilar data in natural language processing (NLP) tasks such as sentiment analysis, propaganda detection, and event extraction from social media. The paper also explores the use of BERT language models for transfer learning and cost-sensitive classification to address these challenges. (Mandarin) are studied in this paper. The focus is on the challenges of imbalanced classification and the ability to generalize on dissimilar data in natural language processing (NLP) tasks such as sentiment analysis,", "answers": ["Unanswerable", "English"], "all_classes": null, "length": 4513}
{"pred": "    SVM, BiLSTM, CNN\n\n        </div>\n\n        </div>\n\n        </div>\n\n        </div>\n\n        </div>\n\n        </div>\n\n        </div>\n\n        </div>\n\n        </div>\n\n        </div>\n\n        </div>\n\n        </div>\n\n        </div>\n\n        </div>\n\n        </div>\n\n        </div>\n\n        </div>\n\n        </div>\n\n        </div>\n\n        </div>\n\n        </div>\n\n        </div>\n\n        </div>\n\n        </div>\n\n        </div>\n\n        </div>\n\n        </div>\n\n        </div>\n\n        </div>\n\n        </div", "answers": ["linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM trained on word unigrams,  bidirectional Long Short-Term-Memory (BiLSTM),  Convolutional Neural Network (CNN) "], "all_classes": null, "length": 2250}
{"pred": "        Yes, the answered questions measure for the usefulness of the answer. The number of upvotes and downvotes on the answer, as well as the number of comments, can provide insight into how useful the answer is to the community. Additionally, the answer may be accepted by the asker, which indicates that the answer was helpful. The answer may also be marked as the best answer, which indicates that the community found it to be the most helpful. Overall, the answered questions provide a measure of the usefulness of the answer to the community. istringstream is a class in C++ that allows you to read data from a string. It provides", "answers": ["No"], "all_classes": null, "length": 1561}
{"pred": "    GloVe and Edinburgh embeddings were used. The Edinburgh embeddings were obtained by training a skip-gram model on the Edinburgh corpus. The GloVe embeddings were trained on 2 billion tweets. Both embeddings were used to obtain word vectors for each tweet, which were then summed and divided by the number of tokens in the tweet to obtain the final tweet embedding. The use of pretrained word embeddings is common in NLP tasks and can improve performance by capturing semantic relationships between words. The choice of embeddings used in this task was likely informed by the specific characteristics of the Twitter dataset, such as the prevalence of emojis and the need to handle informal and slang", "answers": ["Pretrained word embeddings  were not used", "GloVe, Edinburgh embeddings BIBREF14, Emoji embeddings BIBREF16"], "all_classes": null, "length": 1771}
{"pred": "        The personalized generative models were able to generate plausible, personalized, and coherent recipes that were preferred by human evaluators for consumption. The models also introduced a set of automatic coherence measures for instructional texts as well as personalization metrics to support their claims. The results show that personalization can improve the semantic plausibility of generated recipes. (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable)", "answers": ["average recipe-level coherence scores of 1.78-1.82, human evaluators preferred personalized model outputs to baseline 63% of the time"], "all_classes": null, "length": 2666}
{"pred": "        The combination of rewards for reinforcement learning is a harmonic mean of irony reward and sentiment reward. The irony reward is the difference between the irony score of the input sentence and that of the output sentence, while the sentiment reward is one minus the absolute value of the difference between the standardized sentiment score of the input sentence and that of the generated sentence. The harmonic mean encourages the model to focus on both the irony accuracy and the sentiment preservation. (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes)", "answers": ["irony accuracy, sentiment preservation", " irony accuracy and sentiment preservation"], "all_classes": null, "length": 4592}
{"pred": "        Since we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer as shown in Figure FIGREF12 for \"Starry Night\" with a low average content score. This happens when the style transfer dataset does not have similar words in the training set of sentences. A solution would be to expand the style transfer dataset, for a better representation of the poem data.       \n\n        Since we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer as shown in Figure FIGREF12 for \"Starry Night\" with a low average content score", "answers": ["Since we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer", "we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer as shown in Figure FIGREF12 for \"Starry Night\" with a low average content score"], "all_classes": null, "length": 1651}
{"pred": "    The existing benchmarks they compared to are the Affective Text dataset, the Fairy Tales dataset, and the ISEAR dataset. These datasets are commonly used for the evaluation of emotion detection systems. The Affective Text dataset contains news headlines annotated with emotions and valence, while the Fairy Tales dataset contains sentences from fairy tales annotated with emotions. The ISEAR dataset contains reports from psychology questionnaires answered by people with different cultural backgrounds, annotated with emotions. The authors used these datasets to compare their model's performance to state-of-the-art results. (Section SECREF3) (Section SECREF5) (Section SECREF3) (", "answers": ["Affective Text, Fairy Tales, ISEAR", " Affective Text dataset, Fairy Tales dataset, ISEAR dataset"], "all_classes": null, "length": 3390}
{"pred": "        The sample collected consisted on 1 785 855 tweets published by 848 196 different users. Within our sample, we identified 1327 tweets that went viral (retweeted more than 1000 times by the 8th of November 2016) produced by 643 users. Such small subset of viral tweets were retweeted on 290 841 occasions in the observed time-window. The 1327 `viral' tweets were manually annotated as containing fake news or not. The annotation was carried out by a single person in order to obtain a consistent annotation throughout the dataset. Out of those 1327 tweets,", "answers": ["Distributions of Followers, Friends and URLs are significantly different between the set of tweets containing fake news and those non containing them, but for Favourites, Mentions, Media, Retweets and Hashtags they are not significantly different"], "all_classes": null, "length": 3164}
{"pred": "    The dataset of hashtags is sourced from the Stanford Sentiment Analysis Dataset. The dataset consists of 1,108 unique English hashtags from 1,268 randomly selected tweets along with their crowdsourced segmentations and additional corrections. The dataset is curated by Bansal et al. (2015). The dataset is publicly available and can be accessed at https://github.com/abhishek-bansal/hashtag-segmentation. The dataset is used to evaluate the performance of hashtag segmentation methods. The dataset is also used to train and test the proposed pairwise neural ranking model. The dataset is used to demonstrate the effectiveness of hashtag segmentation in", "answers": ["1,268 randomly selected tweets in the Stanford Sentiment Analysis Dataset BIBREF36, all 12,594 unique English hashtags and their associated tweets from the same Stanford dataset", "Stanford Sentiment Analysis Dataset BIBREF36"], "all_classes": null, "length": 3756}
{"pred": "The corpus contains a variety of accents, including Persian, English, and other languages spoken by the respondents. The respondents were asked to record their speech in their native language, so the accents reflect the diversity of the population. The corpus also includes some accents that are not native to the respondents, such as English accents spoken by Persian speakers. The diversity of accents in the corpus makes it a valuable resource for studying the effects of accent on speech recognition and speaker verification tasks. (unanswerable)\n\nQuestion: what is the main goal of the DeepMine project?\n\nAnswer: to collect speech from at least a few thousand speakers, enabling research and development", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3794}
{"pred": "    A compact, scalable, and meaningful representation of a set of words that belong to the same context. It is generated by applying PCA to the set of word vectors. (Word subspace) (1) (2) (3) (4) (5) (6) (7) (8) (9) (10) (11) (12) (13) (14) (15) (16) (17) (18) (19) (20) (21) (22) (23) (24) (25) (26) (27) (28) (29) (30", "answers": ["Word vectors, usually in the context of others within the same class"], "all_classes": null, "length": 5151}
{"pred": "    The baseline model used is the one proposed by Dunietz and Gillick (BIBREF11). It uses a variety of features that measure salience of an entity in text. The features include positional features, occurrence frequency, and the internal POS structure of the entity and the sentence it occurs in. (BIBREF11) (BIBREF11) (BIBREF11) (BIBREF11) (BIBREF11) (BIBREF11) (BIBREF11) (BIBREF11) (BIBREF11) (BIBREF11) (BIBREF11)", "answers": ["For Article-Entity placement, they consider two baselines: the first one using only salience-based features, and the second baseline checks if the entity appears in the title of the article. \n\nFor Article-Section Placement, they consider two baselines: the first picks the section with the highest lexical similarity to the article, and the second one picks the most frequent section.", "B1. The first baseline uses only the salience-based features by Dunietz and Gillick BIBREF11 ., B2. The second baseline assigns the value relevant to a pair INLINEFORM0 , if and only if INLINEFORM1 appears in the title of INLINEFORM2 .\n\n, S1: Pick the section from template INLINEFORM0 with the highest lexical similarity to INLINEFORM1 : S1 INLINEFORM2, S2: Place the news into the most frequent section in INLINEFORM0"], "all_classes": null, "length": 7891}
{"pred": "    Yes, SemCor3.0 is reflective of English language data in general. It is a large corpus manually annotated with WordNet sense for WSD, and it is used as training corpus in many WSD studies. The dataset is widely used and considered to be representative of English language data. Therefore, the results obtained on SemCor3.0 can be generalized to other English language data. (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 2000}
{"pred": "    Augmented LibriSpeech dataset is 960 hours of speech data. It is a large dataset that can be used for training speech recognition models. The dataset is available for download and can be used for research and development purposes. It is a valuable resource for the speech recognition community. (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable)", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 2410}
{"pred": "        The dataset for fine-grained classification is split in training, development, development_test and test parts. In the rest, we refer to these splits as train, development and test, where train is composed by the training and the development instances. Table TABREF7 presents an overview of the data. As discussed in BIBREF16 and illustrated in the Table, the fine-grained dataset is highly unbalanced and skewed towards the positive sentiment: onlyINLINEFORM0 of the training examples are labeled with one of the negative classes. (The dataset for fine-grained classification is split in training, development, development_test and test parts. In", "answers": [" high-quality datasets  from SemEval-2016 “Sentiment Analysis in Twitter” task", " SemEval-2016 “Sentiment Analysis in Twitter”"], "all_classes": null, "length": 2738}
{"pred": "        Yes, they use the uncased BERT$_\\mathrm {BASE}$ model, which has 12 Transformer blocks, 768 hidden layer, 12 self-attention heads, and 110M parameters. They find that BERT$_\\mathrm {LARGE}$ performs slightly worse than BERT$_\\mathrm {BASE}$ in this task. (Note: The BERT model is a language representation model that uses a multi-layer bidirectional Transformer encoder. It is pre-trained on a large corpus and two novel unsupervised prediction tasks, i.e., masked language model and next sentence prediction tasks. When incorporating BERT into downstream", "answers": ["small BERT", "small BERT"], "all_classes": null, "length": 1999}
{"pred": "    Yes, the datasets are subject to quality control. The authors describe several steps they take to ensure the quality of the datasets, including using expert knowledge sources and carefully constructing the datasets to avoid systematic biases. They also validate the datasets through crowd-sourcing experiments. The authors acknowledge that the datasets are not perfect, but they believe that the quality control measures they take help to mitigate potential issues. (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (", "answers": ["No", "No"], "all_classes": null, "length": 6391}
{"pred": "    Yes, the images are from a specific domain. The ShapeWorld framework is used to generate training and evaluation data for image captioning evaluation. The images are abstract colored shapes, and the captions are generated using the same grammar as the training data. The world model stores information about the underlying microworld used to generate an image and a descriptive caption. The world model gives the actual semantic information contained in an image, which allows evaluation of caption truthfulness. The ShapeWorldICE evaluation suite is used to evaluate image captioning models on the ShapeWorld framework. The GTD evaluation framework focuses on grammaticality, truthfulness, and diversity", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3472}
{"pred": "    The performance of the model on emotion detection was competitive, achieving state-of-the-art results for some of the emotion labels on existing, standard evaluation datasets. The model was trained entirely automatically, and did not rely on any manual annotation or handcrafted resources. The model's performance was compared to existing systems, and it was found to be competitive or even state-of-the-art for some of the emotion labels. The model's performance was also found to be dependent on the choice of Facebook pages used as training data, which provides an intrinsic domain-adaptation method. The model's performance was also found to be dependent on the choice of features used", "answers": ["Answer with content missing: (Table 3) Best author's model B-M average micro f-score is 0.409, 0.459, 0.411 on Affective, Fairy Tales and ISEAR datasets respectively. "], "all_classes": null, "length": 3410}
{"pred": "        {INLINEFORM0, INLINEFORM1} where INLINEFORM0 means the current word is not a pun and INLINEFORM1 means the current word is a pun. If the tag sequence of a sentence contains a INLINEFORM0 tag, then the text contains a pun and the word corresponding to INLINEFORM1 is the pun. The contexts have the characteristic that each context contains a maximum of one pun. To capture this interesting property, a new tagging scheme consisting of three tags, namely {INLINEFORM0, INLINEFORM1, INLINEFORM2} is proposed. The INLINEFORM0 scheme can guarantee the context property that there exists a maximum", "answers": ["A new tagging scheme that tags the words before and after the pun as well as the pun words.", "a new tagging scheme consisting of three tags, namely { INLINEFORM0 }"], "all_classes": null, "length": 2974}
{"pred": "    Yes, Arabic is one of the 11 languages in CoVost. The other 10 languages are French, German, Dutch, Russian, Spanish, Italian, Turkish, Persian, Swedish, and Mongolian. The corpus is diversified with over 11,000 speakers and over 60 accents. It includes a total of 708 hours of speech in 11 languages, with French and German having the largest durations among existing public corpora. The corpus is created at the sentence level and does not require additional alignments or segmentation. The baseline results show that the end-to-end many-to-one multilingual model performs well on low-resource languages", "answers": ["No", "No"], "all_classes": null, "length": 2413}
{"pred": "    A model is considered robust if it is not sensitive to the prior knowledge provided to it. The model should be able to handle the bias in the prior knowledge and still perform well. The model should also be able to handle the situation where the prior knowledge is not available for some classes. The model should be able to make predictions based on the available information and not be misled by the prior knowledge. The model should be able to handle the situation where the prior knowledge is not accurate or complete. The model should be able to handle the situation where the prior knowledge is not available for some classes. The model should be able to handle the situation", "answers": ["ability to accurately classify texts even when the amount of prior knowledge for different classes is unbalanced, and when the class distribution of the dataset is unbalanced", "Low sensitivity to bias in prior knowledge"], "all_classes": null, "length": 3609}
{"pred": "    InferSent, Universal Sentence Encoder, Skip-Thought, and Universal Sentence Encoder are evaluated. The results show that SBERT outperforms these methods on various tasks. (Unanswerable) (Yes) (No) (Unanswerable) (Yes) (No) (Unanswerable) (Yes) (No) (Unanswerable) (Yes) (No) (Unanswerable) (Yes) (No) (Unanswerable) (Yes) (No) (Unanswerable) (Yes) (No) (Unanswerable) (Yes) (No) (Unanswerable) (", "answers": ["GloVe, BERT, Universal Sentence Encoder, TF-IDF, InferSent", "Avg. GloVe embeddings, Avg. fast-text embeddings, Avg. BERT embeddings, BERT CLS-vector, InferSent - GloVe and Universal Sentence Encoder."], "all_classes": null, "length": 3862}
{"pred": "        +0.29 and +0.96 for English datasets, +0.97 and +2.36 for Chinese datasets. (English: CoNLL2003, OntoNotes5.0; Chinese: MSRA, OntoNotes4.0) (F1: +0.29, +0.96, +0.97, +2.36) (Datasets: CoNLL2003, OntoNotes5.0, MSRA, OntoNotes4.0) (Improvements: +0.29, +0.96, +0.97, +2.", "answers": ["English datasets including CoNLL2003 and OntoNotes5.0, our proposed method outperforms BERT-MRCBIBREF38 by +0.29 and +0.96 respectively, Chinese datasets, achieving F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively", "For English datasets including CoNLL2003 and OntoNotes5.0, our proposed method outperforms BERT-MRCBIBREF38 by +0.29 and +0.96 respectively., huge performance boosts on Chinese datasets, achieving F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively"], "all_classes": null, "length": 3586}
{"pred": "        They test their conflict method on two tasks: Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask. They use the same model architecture for both tasks, but the input data and the output labels are different. For Quora Duplicate Question Pair Detection, they use a dataset of question pairs labeled as 1 or 0 depending on whether a pair is duplicate or not. For Ranking questions in Bing's People Also Ask, they use a dataset of user queries and candidate questions labeled as 1 or 0 depending on whether a candidate question is likely to be clicked by a user or not. They train a binary classifier", "answers": ["Task 1: Quora Duplicate Question Pair Detection, Task 2: Ranking questions", "Quora Duplicate Question Pair Detection, Ranking questions in Bing's People Also Ask"], "all_classes": null, "length": 2577}
{"pred": "    Previous syntactic tree-based models and other neural models\n\nConclusion: The SATA Tree-LSTM outperforms previous syntactic tree-based models and other neural models on several sentence-level tasks, demonstrating the effectiveness of linguistic priors in neural models for sentence representations. The model also outperforms latent tree-based models, indicating that modeling with explicit linguistic knowledge can be an attractive option. The authors plan to explore new ways of exploiting dependency trees effectively in future work. (unanswerable)\n\nQuestion: What is the main contribution of this work?\n\nAnswer: The main contribution of this work is the introduction of a novel RvNN architecture that fully", "answers": ["Various tree structured neural networks including variants of Tree-LSTM, Tree-based CNN, RNTN, and non-tree models including variants of LSTMs, CNNs, residual, and self-attention based networks", "Sentence classification baselines: RNTN (Socher et al. 2013), AdaMC-RNTN (Dong et al. 2014), TE-RNTN (Qian et al. 2015), TBCNN (Mou et al. 2015), Tree-LSTM (Tai, Socher, and Manning 2015), AdaHT-LSTM-CM (Liu, Qiu, and Huang 2017), DC-TreeLSTM (Liu, Qiu, and Huang 2017), TE-LSTM (Huang, Qian, and Zhu 2017), BiConTree (Teng and Zhang 2017), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), TreeNet (Cheng et al. 2018), CNN (Kim 2014), AdaSent (Zhao, Lu, and Poupart 2015), LSTM-CNN (Zhou et al. 2016), byte-mLSTM (Radford, Jozefowicz, and Sutskever 2017), BCN + Char + CoVe (McCann et al. 2017), BCN + Char + ELMo (Peters et al. 2018). \nStanford Natural Language Inference baselines: Latent Syntax Tree-LSTM (Yogatama et al. 2017), Tree-based CNN (Mou et al. 2016), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), NSE (Munkhdalai and Yu 2017), Reinforced Self- Attention Network (Shen et al. 2018), Residual stacked encoders: (Nie and Bansal 2017), BiLSTM with generalized pooling (Chen, Ling, and Zhu 2018)."], "all_classes": null, "length": 4781}
{"pred": "    Relation Detection\n\nExplanation: The core component for KBQA is relation detection, which is the process of identifying the relevant relations between entities in the knowledge base. This is a crucial step in KBQA, as it allows the system to retrieve the correct answers from the knowledge base. The relation detection component is responsible for matching the question to the relevant relations in the knowledge base, and for generating the query that will be used to retrieve the answers. The quality of the relation detection component directly impacts the performance of the KBQA system. Therefore, improving the relation detection component is a key focus of this work. The proposed hierarchical matching approach with residual", "answers": ["answer questions by obtaining information from KB tuples ", "hierarchical matching between questions and relations with residual learning"], "all_classes": null, "length": 4527}
{"pred": "    The baseline models are a name-based Nearest-Neighbor model (NN) and an Encoder-Decoder baseline with ingredient attention (Enc-Dec). Both models are used to compare the performance of the personalized models against. The NN model uses the recipe name to find the most similar recipe in the dataset, while the Enc-Dec model uses an attention mechanism to focus on the ingredients mentioned in the input. Both models are used to generate recipes from incomplete input specifications. The personalized models outperform the baseline models in BPE perplexity, but perform worse in BLEU-1/4 and ROUGE-L. However, they generate more diverse and acceptable", "answers": ["name-based Nearest-Neighbor model (NN), Encoder-Decoder baseline with ingredient attention (Enc-Dec)"], "all_classes": null, "length": 2655}
{"pred": "        Manual detection of biases and unwarranted inferences in the Flickr30K dataset, including part-of-speech tagging and coreference graph analysis. davidjl\n\n        Manual detection of biases and unwarranted inferences in the Flickr30K dataset, including part-of-speech tagging and coreference graph analysis. davidjl\n\n        Manual detection of biases and unwarranted inferences in the Flickr30K dataset, including part-of-speech tagging and coreference graph analysis. davidjl\n\n        Manual detection of biases and unwarranted inferences in the Flickr30K dataset, including part-of-speech tagging and coreference graph analysis. davidjl\n\n", "answers": ["spot patterns by just looking at a collection of images, tag all descriptions with part-of-speech information, I applied Louvain clustering", "Looking for adjectives marking the noun \"baby\" and also looking for most-common adjectives related to certain nouns using POS-tagging"], "all_classes": null, "length": 2204}
{"pred": "        They explore Romance languages and Semitic languages. (Romance languages: French, Spanish, Italian, Portuguese; Semitic languages: Arabic, Hebrew) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (", "answers": ["English, French, German ", "French, English, Spanish, Italian, Portuguese, Hebrew, Arabic"], "all_classes": null, "length": 2285}
{"pred": "    They experimented with stacked LSTMs, Cell-aware Stacked LSTMs, and variants of these models. They also experimented with different word embeddings and pooling methods. (1 phrase) (1 phrase) (1 phrase) (1 phrase) (1 phrase) (1 phrase) (1 phrase) (1 phrase) (1 phrase) (1 phrase) (1 phrase) (1 phrase) (1 phrase) (1 phrase) (1 phrase) (1 phrase) (1 phrase) (1 phrase) (1 phrase) (1 phrase) (1 phrase) (1 phrase) (1 phrase) (1", "answers": ["Stacked LSTMs, Cell-aware Stacked LSTMs, Sentence Encoders, Top-layer Classifiers"], "all_classes": null, "length": 3210}
{"pred": "        Yes, they report results only on English data. They use a snapshot of English Wikipedia measuring 8GB in size, with the stop-words filtered out. The algorithm is trained over 20 iterations. The vocabulary size is close to 300,000 while only 16,242 unique words of the vocabulary are present in the concept groups. The proposed method is compared with previous studies that aim to obtain interpretable word vectors. The proposed method is also compared with other methods that aim to improve interpretability while preserving the underlying semantic structure. The results show that the proposed method significantly improves the interpretability of the embeddings while preserving the underlying", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 6169}
{"pred": "    The authors experimented with a few summarization algorithms provided by the Sumy package, including TextRank, LexRank, and SumBasic. They also compared the performance of their ILP-based summarization algorithm with these algorithms. The results showed that the performance of their ILP-based summarization algorithm was comparable with the other algorithms, as the two sample t-test did not show statistically significant difference. Additionally, human evaluators preferred the phrase-based summary generated by their approach to the other sentence-based summaries. (Note: This is a paraphrased answer based on the information in the article, and may not be exactly what the authors wrote in", "answers": ["LSA, TextRank, LexRank and ILP-based summary.", "LSA, TextRank, LexRank"], "all_classes": null, "length": 3045}
{"pred": "    The previous state of the art for this task was a probabilistic graphical model that inferred vocabulary dependent latent post categories to model the thread sequence and infer states that triggered intervention. However, this model required a hyperparameter for the number of latent states, which may not generalize well due to weak evaluation. (BIBREF0) (BIBREF7) (BIBREF0) (BIBREF7) (BIBREF0) (BIBREF7) (BIBREF0) (BIBREF7) (BIBREF0) (BIBREF7) (BIBREF0) (B", "answers": ["hLSTM", "hLSTM"], "all_classes": null, "length": 3725}
{"pred": "    The least impactful component is the renormalization trick. The results show that using sum instead of mean decreases performance everywhere, suggesting that in our application, capturing the distribution of neighbor representations is more important that capturing their structure. The authors hypothesize that this is the case because statistical word co-occurrence networks tend to have similar structural properties, regardless of the topic, polarity, sentiment, etc. of the corresponding documents. The results also show that removing the master node deteriorates performance across all datasets, clearly showing the value of having such a node. The authors hypothesize that since the special document node is connected to all other nodes,", "answers": ["Based on table results provided changing directed to undirected edges had least impact - max abs difference of 0.33 points on all three datasets."], "all_classes": null, "length": 4180}
{"pred": "    The corpus used for the task is DTA18 and DTA19, which are subparts of the DTA corpus. These corpora consist of texts from the 16th to the 20th century, and the corpus version used in the task has the following format: \"year [tab] lemma1 lemma2 lemma3...\". The two corpora correspond to the diachronic corpus pair from BIBREF0: DTA18 and DTA19. They consist of subparts of DTA corpus BIBREF11 which is a freely available lemmatized, POS-tagged and spelling-normalized diach", "answers": ["DTA18, DTA19", "Diachronic Usage Relatedness (DURel) gold standard data set"], "all_classes": null, "length": 1908}
{"pred": "        Kannada, Hindi, Telugu, Malayalam, Bengali, and English\n\n        Kannada, Hindi, Telugu, Malayalam, Bengali, and English\n\n        Kannada, Hindi, Telugu, Malayalam, Bengali, and English\n\n        Kannada, Hindi, Telugu, Malayalam, Bengali, and English\n\n        Kannada, Hindi, Telugu, Malayalam, Bengali, and English\n\n        Kannada, Hindi, Telugu, Malayalam, Bengali, and English\n\n        Kannada, Hindi, Telugu, Malayalam, Bengali, and English\n\n        Kann", "answers": ["Hindi, English, Kannada, Telugu, Assamese, Bengali and Malayalam", "Kannada, Hindi, Telugu, Malayalam, Bengali,  English and Assamese (in table, missing in text)"], "all_classes": null, "length": 2453}
{"pred": "The model performance on target language reading comprehension is reasonable, with EM/F1 scores of 53.8 and 44.1, respectively. However, the performance is degraded when the training data is translated into the target language, indicating that translation degrades the quality of data. The results show that the quality and the size of dataset are much more important than whether the training and testing are in the same language or not. (Unanswerable) (Yes) (No) (Unanswerable) (Yes) (No) (Unanswerable) (Yes) (No) (Unanswerable) (Yes) (No", "answers": ["Table TABREF6, Table TABREF8", "when testing on English, the F1 score of the model training on Chinese (Zh) is 53.8,  F1 score is only 44.1 for the model training on Zh-En"], "all_classes": null, "length": 2492}
{"pred": "        The proposed model outperforms the baselines by a significant margin. The difference in performance is consistent across all five evaluation characters. The proposed model is able to recover the language styles of specific characters more accurately than the baselines. The proposed model also demonstrates stability and robustness in recovering the language styles of characters regardless of their profile and identity, genre of the show, and context of the dialogue. The proposed model is able to recommend tailored responses traceable to specific characters, which is not possible with the baselines. The proposed model is able to effectively use HLAs to improve upon dialogue retrieval performance. The proposed model is able to", "answers": ["Metric difference between Aloha and best baseline score:\nHits@1/20: +0.061 (0.3642 vs 0.3032)\nMRR: +0.0572(0.5114 vs 0.4542)\nF1: -0.0484 (0.3901 vs 0.4385)\nBLEU: +0.0474 (0.2867 vs 0.2393)"], "all_classes": null, "length": 5151}
{"pred": "        1.5-2.0x improvement in forward/reverse perplexity and Self-BLEU on COCO and EMNLP2017 WMT datasets. 2.0x improvement in manual evaluation on WeiboDial dataset. 3.0x improvement in training stability on COCO dataset. 4.0x improvement in training stability on EMNLP2017 WMT dataset. 5.0x improvement in training stability on WeiboDial dataset. 6.0x improvement in training stability on WeiboDial dataset. 7.0x improvement in training stability on Weibo", "answers": ["ARAM has achieved improvement over all baseline methods using reverese perplexity and  slef-BLEU metric.  The maximum reverse perplexity improvement 936,16 is gained for EMNLP2017  WMT  dataset and  48,44 for COCO dataset.", "Compared to the baselines, ARAML does not do better in terms of perplexity on COCO and EMNLP 2017 WMT datasets, but it does by up to 0.27 Self-BLEU points on COCO and 0.35 Self-BLEU on EMNLP 2017 WMT. In terms of Grammaticality and Relevance, it scores better than the baselines on up to 75.5% and 73% of the cases respectively."], "all_classes": null, "length": 3796}
{"pred": "        The authors present some evidence that the model can capture some biases in data annotation and collection. They examine the results and find that the model can detect some biases in the process of collecting or annotating datasets. They also find that the model can detect some biases in the data itself, such as the presence of certain words or phrases that are associated with hate speech. The authors conclude that this ability to detect biases can be a valuable clue in using the pre-trained BERT model to alleviate bias in hate speech datasets in future studies. (unanswerable) (yes) (no) (unanswerable) (yes) (no)", "answers": ["The authors showed few tweets where neither and implicit hatred content exist but the model was able to discriminate"], "all_classes": null, "length": 4119}
{"pred": "        Yes, three baselines were tested to compare with the neural baseline. The first baseline is a simple majority-class baseline, which predicts that all questions are unanswerable. The second baseline is an SVM classifier with three sets of features, including bag-of-words features, length of the question in words, and part-of-speech tags. The third baseline is a CNN neural encoder. The results show that the neural baseline outperforms the other baselines, indicating that it is capable of making some progress towards answering questions in this challenging domain. However, there is still considerable room for improvement to reach human performance. (Unanswerable", "answers": ["SVM, No-Answer Baseline (NA) , Word Count Baseline, Human Performance", "No-Answer Baseline (NA), Word Count Baseline, Human Performance"], "all_classes": null, "length": 3855}
{"pred": "        72782 unique words\n\n        14 million words from books, web-texts and news papers\n\n        6946 sentences and 16225 unique words extracted from POS-tagged Nepali National Corpus\n\n        64%, 16% and 20% of the total dataset into training set, development set and test set respectively\n\n        72782 unique words\n\n        14 million words from books, web-texts and news papers\n\n        6946 sentences and 16225 unique words extracted from POS-tagged Nepali National Corpus\n\n        64%, 16% and 20% of the total dataset into", "answers": ["Dataset contains 3606 total sentences and 79087 total entities.", "ILPRL contains 548 sentences, OurNepali contains 3606 sentences"], "all_classes": null, "length": 2843}
{"pred": "        Using dice loss in replacement of the standard cross-entropy loss, which performs as a soft version of F1 score. The proposed training objective leads to significant performance boost for part-of-speech, named entity recognition, machine reading comprehension and paraphrase identification tasks. (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (", "answers": ["Using DSC loss improves the F1 score by +0.58 for MRPC and +0.73 for QQP", "+0.58"], "all_classes": null, "length": 3566}
{"pred": "        The datasets used are EEG data from BIBREF0 and eye-tracking, self-paced reading time, and ERP data. (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable", "answers": ["Answer with content missing: (Whole Method and Results sections) The primary dataset we use is the ERP data collected and computed by Frank et al. (2015), and we also use behavioral data (eye-tracking data and self-paced reading times) from Frank et al. (2013) which were collected on the same set of 205 sentences.\nSelect:\n- ERP data collected and computed by Frank et al. (2015)\n- behavioral data (eye-tracking data and self-paced reading times) from Frank et al. (2013)", "the ERP data: BIBREF0"], "all_classes": null, "length": 1971}
{"pred": "        The subjects were presented with a series of images depicting different events, such as a person walking, a car driving, and a person eating. The images were designed to elicit event-related responses in the form of brain activity. The subjects were instructed to imagine themselves performing the actions depicted in the images. The researchers then recorded the brain activity of the subjects while they imagined performing the actions. The resulting brain activity data was used to investigate the relationship between brain activity and event-related responses. (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (", "answers": ["7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw)", "KARA ONE BIBREF17 , composed of multimodal data for stimulus-based, imagined and articulated speech state corresponding to 7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw)"], "all_classes": null, "length": 2379}
{"pred": "    Pointer-Gen, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN, Pointer-Gen+ARL-SEN, Test set, Human evaluation, ROUGE, Chi-square test, Fluency, Sensationalism, ARL, MLE, RL, Distant supervision, Pointer-Gen, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN, Pointer-Gen+ARL-SEN, Test set, Human evaluation, ROUGE, Chi-square test, Fluency, Sensationalism, ARL, MLE, RL, Distant supervision, Pointer-Gen", "answers": ["Pointer-Gen, Pointer-Gen+Pos, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN"], "all_classes": null, "length": 4085}
{"pred": "    Traditional machine learning models (Naïve Bayes, Logistic Regression, Support Vector Machine, Random Forests, Gradient Boosted Trees) and neural network models (Convolutional Neural Networks, Recurrent Neural Networks, HybridCNN, RNN with LTC, RNN with attention, etc.) are used on the dataset. The authors also investigate variants of these models and the effect of using context tweets. (Unanswerable) (Yes) (No) (Unanswerable) (Yes) (No) (Unanswerable) (Yes) (No) (Unanswerable) (Yes) (No) (Unanswer", "answers": ["Naïve Bayes (NB), Logistic Regression (LR), Support Vector Machine (SVM), Random Forests (RF), Gradient Boosted Trees (GBT),  Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN)", "Naïve Bayes (NB), Logistic Regression (LR), Support Vector Machine (SVM), Random Forests (RF), Gradient Boosted Trees (GBT), CNN, RNN"], "all_classes": null, "length": 2074}
{"pred": "    The bi-directional language model is used to augment the sequence to sequence encoder, and the uni-directional model is used to augment the decoder. Both models use self-attention and the uni-directional model contains 6 transformer blocks, followed by a word classifier to predict the next word on the right. The bi-directional model solves a cloze-style token prediction task at training time, and the model consists of two towers, the forward tower operating left-to-right and the backward tower operating right-to-left. The forward and backward representations are combined via a self-attention module and the output of this module is used to predict the token", "answers": ["uni-directional model to augment the decoder", "bi-directional language model to augment the sequence to sequence encoder ,  uni-directional model to augment the decoder"], "all_classes": null, "length": 1914}
{"pred": "        The weights are dynamically adjusted by multiplying the soft probability p with a decaying factor (1-p). This decaying factor pushes down the weight of easy examples, making the model less focused on them. The intuition is to push down the weight of easy examples, making the model less focused on them. The weights are dynamically adjusted by multiplying the soft probability p with a decaying factor (1-p). This decaying factor pushes down the weight of easy examples, making the model less focused on them. The intuition is to push down the weight of easy examples, making the model less focused on them. The weights are dynamically adjusted by multiplying", "answers": ["One can think $(1-p_{i1})$ as a weight associated with each example, which changes as training proceeds. The intuition of changing $p_{i1}$ to $(1-p_{i1}) p_{i1}$ is to push down the weight of easy examples. For easy examples whose probability are approaching 0 or 1, $(1-p_{i1}) p_{i1}$ makes the model attach significantly less focus to them. Figure FIGREF23 gives gives an explanation from the perspective in derivative: the derivative of $\\frac{(1-p)p}{1+(1-p)p}$ with respect to $p$ approaches 0 immediately after $p$ approaches 0, which means the model attends less to examples once they are correctly classified.", "associates each training example with a weight in proportion to $(1-p)$, and this weight dynamically changes as training proceeds"], "all_classes": null, "length": 3640}
{"pred": "        The results show that agents utilizing knowledge-graphs in addition to either enhanced exploration method far outperform the baseline A2C and KG-A2C. KG-A2C-chained and KG-A2C-Explore both pass the bottleneck of a score of 40, whereas A2C-Explore gets to the bottleneck but cannot surpass it. There are a couple of key insights that can be drawn from these results. The first is that the knowledge graph appears to be critical; it is theorized to help with partial observability. However the knowledge graph representation isn't sufficient in that the knowledge graph representation without enhanced exploration methods cannot surpass", "answers": ["Reward of 11.8 for the A2C-chained model, 41.8 for the KG-A2C-chained model, 40 for A2C-Explore and 44 for KG-A2C-Explore.", "KG-A2C-chained and KG-A2C-Explore both pass the bottleneck of a score of 40"], "all_classes": null, "length": 2443}
{"pred": "    An individual model consists of individual Bayesian models for each language and crosslingual latent variables to incorporate soft role agreement between aligned constituents. These latent variables capture correlations between roles in different languages and regularize the parameter estimates of the monolingual models. The individual models are coupled together through the crosslingual latent variables, allowing the model to learn from the alignment information in parallel corpora. The individual models are trained separately, but the crosslingual latent variables are shared across all languages. The model is trained using a collapsed Gibbs-sampling based approach, which generates samples for the hidden variables (model parameters) given the visible variables (predicate", "answers": ["Bayesian model of garg2012unsupervised as our base monolingual model"], "all_classes": null, "length": 3701}
{"pred": "        Non-standard pronunciation is identified by annotating the Spanish words interspersed in Mapudungun speech. The annotations label foreign words as such. (Unanswerable) (Yes) (No) (Unanswerable) (Yes) (No) (Unanswerable) (Yes) (No) (Unanswerable) (Yes) (No) (Unanswerable) (Yes) (No) (Unanswerable) (Yes) (No) (Unanswerable) (Yes) (No) (Unanswerable) (Yes) (No) (Unanswerable) (Yes) (No", "answers": ["Unanswerable", "Original transcription was labeled with additional labels in [] brackets with nonstandard pronunciation."], "all_classes": null, "length": 3018}
{"pred": "    A semicharacter architecture is a type of neural network that processes a sentence of words with misspelled characters, predicting the correct words at each step. It is inspired by psycholinguistic studies that demonstrate humans can comprehend text altered by jumbling internal characters, provided that the first and last characters of each word remain unperturbed. The architecture treats the first and the last characters individually, and is agnostic to the ordering of the internal characters. It is trained on a cross-entropy loss function to predict the correct corresponding word at each sequence step. The semicharacter architecture has been shown to achieve strong word recognition", "answers": ["A semi-character based RNN (ScRNN) treats the first and last characters individually, and is agnostic to the ordering of the internal characters", "processes a sentence of words with misspelled characters, predicting the correct words at each step"], "all_classes": null, "length": 4186}
{"pred": "        Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish.        […]\n\n        Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish.        […]        […]\n\n        Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish.        […]        […]        […]        […]        […]        […]        […]        […]        […]        […]", "answers": ["Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish", "Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish , Swedish"], "all_classes": null, "length": 2697}
{"pred": "    The NCEL approach is effective overall, achieving the best performance in most cases with an average gain of 2% on Micro F1 and 3% Macro F1. The proposed method performs consistently well on all datasets, demonstrating good generalization ability. The results of example analysis show that NCEL correctly identifies the correct entity for the mention \"Essex\" and \"England\" with the help of its neighbor mentions. The impact of key modules is also investigated, showing that the attention mechanism and embedding features are effective in improving robustness to noise and generalization ability. Overall, the NCEL approach is a promising solution for collective entity", "answers": ["NCEL consistently outperforms various baselines with a favorable generalization ability"], "all_classes": null, "length": 4113}
{"pred": "    Yes, the data is de-identified with digital zeros and [de-identified] tags. The sentences in the transcript are grounded to the audio with the timestamps of its first and last word. The summaries and MR tags are also grounded using the timestamps of the sentences from the transcript deemed relevant by the expert annotators. The transcript for a typical conversation can be quite long, and not easy for many of the high performing deep learning models to act on. Moreover, the medical information about a concept/condition/entity can change during the conversation after a significant time gap. For example, dosage of a medication can be different when discussing current medication", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4592}
{"pred": "    The baseline used was the performance of the error detection system by Rei2016, trained using the same FCE dataset. The error detection results can be seen in Table 4.INLINEFORM0 as the main evaluation measure, which was established as the preferred measure for error correction and detection by the CoNLL-14 shared task.INLINEFORM1 calculates a weighted harmonic mean of precision and recall, which assigns twice as much importance to precision – this is motivated by practical applications, where accurate predictions from an error detection system are more important compared to coverage. For comparison, we also report the performance of the error detection system by Rei201", "answers": ["error detection system by Rei2016", "error detection system by Rei2016"], "all_classes": null, "length": 2132}
{"pred": "    The annotated clinical notes were obtained from the 2010 i2b2/VA BIBREF0 and 2013 ShARe/CLEF Task 1 BIBREF1 challenges in healthcare. These challenges ask participants to design an algorithm to tag a set of predefined entities of interest in clinical notes. These entity tagging tasks are also known as clinical Named Entity Recognition (NER). For example, the CE task in 2010 i2b2/VA defines three types of entities: \"problem\", \"treatment\", and \"test\". The CE task in 2013 ShARe/CLEF defines various types", "answers": ["clinical notes from the CE task in 2010 i2b2/VA", "clinical notes from the CE task in 2010 i2b2/VA "], "all_classes": null, "length": 3432}
{"pred": "    Masking words in the decoder helps to focus the attention of the decoder on the context of the summary, rather than on the words in the summary itself. This allows the decoder to generate more context-aware and coherent summaries. Moreover, masking words in the decoder helps to reduce exposure bias, which is a common problem in sequence-to-sequence models. Exposure bias occurs when the model generates words based on the previous words it has generated, rather than on the context of the input sequence. By masking words in the decoder, the model is forced to generate words based on the context of the input sequence, rather than on the previous words it has", "answers": ["ecause this process is similar to the cloze task in BERT's pre-train process, therefore by using the ability of the contextual language model the decoder can generate more fluent and natural sequences."], "all_classes": null, "length": 3919}
{"pred": "        Positive and negative emoticons to build a powerful tweet representation. (BIBREF26) (BIBREF2) (BIBREF2) (BIBREF26) (BIBREF2) (BIBREF2) (BIBREF26) (BIBREF2) (BIBREF2) (BIBREF26) (BIBREF2) (BIBREF2) (BIBREF26) (BIBREF2) (BIBREF2) (BIBREF26) (BIBREF2) (BIBREF2) (BIBREF26) (", "answers": ["Unanswerable", " Paraphrase Database (PPDB) ,  book corpus", "Unanswerable"], "all_classes": null, "length": 1902}
{"pred": "    Term Frequency-Inverse Document Frequency (TF-IDF) features are used to extract and corroborate useful keywords from pathology cancer reports. These features are based on the frequency of terms in a document and the inverse document frequency of those terms. The TF-IDF features are used to highlight the important regions of the report and provide useful biomarker information to readers. The TF-IDF features are also used to predict the primary diagnosis from the text content of a given report. The performance of the TF-IDF features is evaluated using different classification models and the results are reported in the article. The TF-IDF features are shown to be effective in", "answers": ["Unanswerable"], "all_classes": null, "length": 2108}
{"pred": "        The dataset is annotated based on a hierarchical model of depression-related symptoms. Each tweet is annotated as no evidence of depression or evidence of depression. If a tweet is annotated evidence of depression, then it is further annotated with one or more depressive symptoms. For example, depressed mood, disturbed sleep, or fatigue or loss of energy. The annotations are binarized as the positive class (e.g., depressed mood=1) or negative class (e.g., not depressed mood=0). The dataset contains 9,473 annotations for 9,300 tweets. (unanswerable) (yes) (no) (unanswerable", "answers": ["no evidence of depression, depressed mood, disturbed sleep, fatigue or loss of energy", "The annotations are based on evidence of depression and further annotated by the depressive symptom if there is evidence of depression"], "all_classes": null, "length": 1947}
{"pred": "    They evaluated on eight biomedical NER tasks: BC5CDR, CheBI, CRAFT, DDI, GENIA, JNLPBA, LLL, and NCBI Disease. They also evaluated on the NCBI Disease task, which is a subset of the BC5CDR task. They used the same setup as the BioBERT paper, with the same random seeds and hyperparameters. They found that GreenBioBERT outperforms general-domain BERT on all tasks, with varying effect sizes. Depending on the points of reference, they cover an average 52% to 60% of the BioBERT – B", "answers": ["BC5CDR-disease, NCBI-disease, BC5CDR-chem, BC4CHEMD, BC2GM, JNLPBA, LINNAEUS, Species-800", "BC5CDR-disease, NCBI-disease, BC5CDR-chem, BC4CHEMD, BC2GM, JNLPBA, LINNAEUS, Species-800"], "all_classes": null, "length": 2800}
{"pred": "        The training data was translated using the machine translation platform Apertium. The English datasets were translated into Spanish, and the new set of \"Spanish\" data was added to the original training set. The machine translation platform Apertium was used for the translation of the datasets.<translation of the datasets><translation of the datasets into Spanish><translation of the datasets into Spanish and addition to the original training set><translation of the datasets into Spanish and addition to the original training set using the machine translation platform Apertium.<translation of the datasets into Spanish and addition to the original training set using the machine translation platform Apertium.<translation of", "answers": ["using the machine translation platform Apertium ", "machine translation platform Apertium BIBREF5"], "all_classes": null, "length": 2423}
{"pred": "        They used a content-based classifier in conjunction with two feature selection methods. davidjl\n\n        They used a content-based classifier in conjunction with two feature selection methods. davidjl\n\n        They used a content-based classifier in conjunction with two feature selection methods. davidjl\n\n        They used a content-based classifier in conjunction with two feature selection methods. davidjl\n\n        They used a content-based classifier in conjunction with two feature selection methods. davidjl\n\n        They used a content-based classifier in conjunction with two feature selection methods. davidjl\n\n        They used a content-based classifier in conjunction with two feature selection methods. davidjl\n\n        They used a content-based classifier in conjunction", "answers": ["AllWords model by counting the frequencies of all the remaining words and training a multinomial Naive Bayes classifier"], "all_classes": null, "length": 4177}
{"pred": "    The baseline for this task was a very simple logistic regression classifier with default parameters, where the input instances were represented by a single feature: the length of the sentence. The performance of this baseline on the SLC task is shown in Tables TABREF33 and TABREF34. (Note: The tables are not included in this answer, but can be found in the original paper.) (Note: The tables are not included in this answer, but can be found in the original paper.) (Note: The tables are not included in this answer, but can be found in the original paper.) (Note: The tables are not included in", "answers": ["The baseline system for the SLC task is a very simple logistic regression classifier with default parameters. The baseline for the FLC task generates spans and selects one of the 18 techniques randomly.", "SLC task is a very simple logistic regression classifier, FLC task generates spans and selects one of the 18 techniques randomly"], "all_classes": null, "length": 3001}
{"pred": "        The baselines they compare with are conditional random fields (CRF) and a pipeline method that uses a perfect pun detection classifier. They also compare their results with prior works that did not employ joint learning. (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (", "answers": ["They compare with the following models: by Pedersen (2017), by Pramanick and Das (2017), by Mikhalkova and Karyakin (2017),  by Vadehra (2017), Indurthi and Oota (2017), by Vechtomova (2017), by (Cai et al., 2018), and CRF."], "all_classes": null, "length": 2991}
{"pred": "    The political bias of different sources is included in the model by assigning a political bias label to different US outlets following the procedure described in BIBREF2. The political bias of different sources is included in the model by assigning a political bias label to different US outlets following the procedure described in BIBREF2. The political bias of different sources is included in the model by assigning a political bias label to different US outlets following the procedure described in BIBREF2. The political bias of different sources is included in the model by assigning a political bias label to different US outlets following the procedure described in BIBREF2. The political bias", "answers": ["By assigning a political bias label to each news article and training only on left-biased or right-biased outlets of both disinformation and mainstream domains", "we also account for political biases inherent to different news sources, referring to the procedure proposed in BIBREF2 to label different outlets. Overall we show that we are able to classify credible vs non-credible diffusion networks (and consequently news articles) with high accuracy (AUROC up to 94%), even when accounting for the political bias of sources (and training only on left-biased or right-biased articles). We observe that the layer of mentions alone conveys useful information for the classification, denoting a different usage of this functionality when sharing news belonging to the two news domains. We also show that most discriminative features, which are relative to the breadth and depth of largest cascades in different layers, are the same across the two countries."], "all_classes": null, "length": 4882}
{"pred": "        The ancient Chinese dataset comes from ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era. They used plain and accurate words to express what happened at that time, and thus ensure the generality of the translated materials. (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (un", "answers": ["ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era", "Ancient Chinese history records in several dynasties and articles written by celebrities during 1000BC-200BC collected from the internet "], "all_classes": null, "length": 3722}
{"pred": "        English (en) overposting overposting\n\n        English (en) overposting overposting\n\n        English (en) overposting overposting\n\n        English (en) overposting overposting\n\n        English (en) overposting overposting\n\n        English (en) overposting overposting\n\n        English (en) overposting overposting\n\n        English (en) overposting overposting\n\n        English (en) overposting overposting\n\n        English (en) overposting overposting\n\n        English (en) overposting overposting\n\n        English (en) overposting overposting\n\n        English (en) overposting overposting\n\n        English (en) overposting overposting\n\n        English (en) overposting overposting\n\n        English (en) overposting overposting\n\n", "answers": ["English", "English ", "English"], "all_classes": null, "length": 2240}
{"pred": "        The Chinese datasets used were the Penn Treebank (PTB) and the Penn Chinese Treebank (PCTB). The PTB contains 10,000 sentences of English, while the PCTB contains 10,000 sentences of Chinese. Both datasets are widely used in natural language processing research. The PTB is a standard benchmark for English grammar induction, while the PCTB is a standard benchmark for Chinese grammar induction. The PTB and PCTB are both available online for download. The PTB is available at https://www.ling.upenn.edu/courses/fall2010/ling571/ptb", "answers": ["Answer with content missing: (Data section) Chinese with version 5.1 of the Chinese Penn Treebank (CTB)"], "all_classes": null, "length": 2545}
{"pred": "        4 layers (user matrix embedding, user vector embedding, topic matrix embedding, topic vector embedding, comment representation, fully connected network) ", "answers": ["eight layers"], "all_classes": null, "length": 4487}
{"pred": "    The dataset used in this paper is Flickr, which is a photo-sharing website that provides tags and georeferenced photos. The tags associated with these photos often describe the location where the photos were taken, and Flickr can thus be regarded as a source of environmental information. The use of Flickr for modelling urban environments has already received considerable attention. For instance, various approaches have been proposed for modelling urban regions, and for identifying points-of-interest and itineraries. However, the usefulness of Flickr for characterizing the natural environment, which is the focus of this paper, is less well-understood. Many recent studies have highlighted that Flickr tags", "answers": [" the same datasets as BIBREF7", "same datasets as BIBREF7"], "all_classes": null, "length": 4661}
{"pred": "    The paper uses two clinical datasets: NUBes-PHI and MEDDOCAN. NUBes-PHI is a corpus of real medical reports written in Spanish and annotated with sensitive information. MEDDOCAN is a synthetic corpus of clinical cases enriched with sensitive information. Both datasets are used to evaluate the performance of the BERT-based model for sensitive information detection and classification in Spanish clinical data. (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable)", "answers": ["MEDDOCAN, NUBes-PHI", "MEDDOCAN, NUBes "], "all_classes": null, "length": 4528}
{"pred": "    Unigrams and Pragmatic features, Stylistic patterns, and patterns related to situational disparity. (BIBREF0, BIBREF1, BIBREF2, BIBREF3) (BIBREF4, BIBREF5) (BIBREF6, BIBREF7) (BIBREF8) (BIBREF9, BIBREF10) (BIBREF11) (BIBREF12) (BIBREF13) (BIBREF14) (BIBREF15) (BIBREF16) (BIBREF17) (BIBREF", "answers": ["Unanswerable"], "all_classes": null, "length": 3543}
{"pred": "    The metrics used to establish that this makes chatbots more knowledgeable and better at learning and conversation are predictive quality and strategy formulation ability. The predictive quality is measured by average MCC and average positive F1 score, while the strategy formulation ability is measured by coverage. The results show that LiLi outperforms the baselines in both metrics. The strategy formulation ability is also evaluated by varying the clue acquisition rate and user interaction. The results show that more clues lead to better performance. The user interaction is also found to be important for better performance. Overall, the results show that LiLi is effective in solving the open-world knowledge base completion problem", "answers": ["Coverage, Avg. MCC and avg. +ve F1 score", "strategy formulation ability, we introduce a measure called Coverage( INLINEFORM0 ), To evaluate the predictive performance, we use Avg. MCC and avg. +ve F1 score"], "all_classes": null, "length": 5869}
{"pred": "        Yes, they employ their indexing-based method to create a sample of a QA Wikipedia dataset. They first index each paragraph in Wikipedia using {1,2,3}-grams, then query each answer sentence from the corpora to Lucene, and retrieve the top-5 ranked paragraphs. The cosine similarity between each sentence in these paragraphs and the answer sentence is measured for 1-grams, and a weighted sum is calculated. Finally, they consider the paragraph containing the sentence with the highest cosine similarity score as the silver-standard answer passage. They use this method to create a dataset for answer retrieval and triggering. (unanswerable) (", "answers": ["Yes", "No"], "all_classes": null, "length": 1910}
{"pred": "        Galatasaray and Fenerbahçe are the targets. (Fenerbahçe is also known as Fenerbahçe Spor Kulübü.) (Fenerbahçe is also known as Fenerbahçe Spor Kulübü.) (Fenerbahçe is also known as Fenerbahçe Spor Kulübü.) (Fenerbahçe is also known as Fenerbahçe Spor Kulübü.) (Fenerbahçe is also known as Fenerbahçe Spor Kulübü.) (Fenerbahçe is also known as Fenerbahçe Spor Kulübü.) (Fenerbahçe is", "answers": ["Galatasaray, Fenerbahçe", "Galatasaray , Fenerbahçe "], "all_classes": null, "length": 2234}
{"pred": "    The experiments conducted are automatic evaluations and human evaluations. Automatic evaluations include sentiment delta, sentiment accuracy, BLEU score, geometric mean, and harmonic mean. Human evaluations include irony accuracy, sentiment preservation, and content preservation. The experiments are conducted to evaluate the performance of the model in generating ironic sentences. The results are compared with other generative models. The conclusions are that the model outperforms other generative models and the rewards are effective. The model is able to generate ironic sentences with high accuracy, sentiment preservation, and content preservation. The model also has some limitations and errors, which are discussed in the paper. The future work includes", "answers": ["Irony Classifier, Sentiment Classifier for Irony, Sentiment Classifier for Non-irony, transformation from ironic sentences to non-ironic sentences"], "all_classes": null, "length": 4600}
{"pred": "    Gaussian-masked directional multi-head attention works by adjusting the weight between characters and their adjacent character to a larger value which stands for the effect of adjacent characters. The Gaussian weight only relies on the distance between characters and ensures that the relationship between two characters with long distances is weaker than adjacent characters. The Gaussian-masked attention combines the Gaussian weight with the self-attention to adjust the weight between characters and their adjacent character. The Gaussian-masked attention uses a triangular matrix mask to let the self-attention focus on different weights for forward and backward encoders. The Gaussian-masked attention is a variant of self-attention that improves the ability of", "answers": ["pays attentions to adjacent characters and casts a localness relationship between the characters as a fixed Gaussian weight assuming the weight relies on the distance between characters", "Gaussian-masked directional attention can be described as a function to map queries and key-value pairs to the representation of input, Gaussian-masked directional attention expects to pay attention to the adjacent characters of each positions and cast the localness relationship between characters as a fix Gaussian weight for attention, Gaussian weight only relys on the distance between characters"], "all_classes": null, "length": 3612}
{"pred": "    Facebook status update messages\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n", "answers": ["Facebook status update messages", "Facebook status update messages"], "all_classes": null, "length": 4005}
{"pred": "    The baseline features are the features extracted from the fully-connected layer of the baseline CNN. These features are used to train the final softmax layer of the baseline CNN, which classifies a sentence as sarcastic or non-sarcastic. The baseline features are the features that the baseline CNN learns to identify sarcastic and non-sarcastic tweets. These features are the inherent semantics of the sarcastic corpus that the baseline CNN learns to extract. The baseline features are the features that the baseline CNN learns to identify sarcastic and non-sarcastic tweets. These features are the inherent semantics of the sarcastic corpus that the baseline CNN", "answers": [" The features extracted from CNN."], "all_classes": null, "length": 4855}
{"pred": "        The number of clusters and the type of word vectors were varied in the experiments on the four tasks. The number of clusters was varied between 250 and 1000, and the type of word vectors was varied between skipgram, cbow, and GloVe. The in-domain and out-of-domain word vectors were also compared. The hyperparameters were varied to determine the optimal settings for each task. The results showed that the optimal settings varied across tasks and that the use of cluster membership features improved performance in all tasks. (unanswerable) (yes) (no) (unanswerable) (yes) (no) (", "answers": ["number of clusters, seed value in clustering, selection of word vectors, window size and dimension of embedding", "different number of clusters, different embeddings"], "all_classes": null, "length": 2753}
{"pred": "        Second (EI-Reg), second (EI-Oc), fourth (V-Reg) and fifth (V-Oc) on the SemEval AIT-2018 leaderboard. (EI-Reg: Emotion Intensity Regression, EI-Oc: Emotion Intensity Ordinal Classification, V-Reg: Valence Regression, V-Oc: Valence Ordinal Classification) (Unanswerable) (Yes) (No) (Unanswerable) (Yes) (No) (Unanswerable) (Yes) (No) (Unanswerable) (Yes) (No) (Unanswerable) (", "answers": ["column Ens Test in Table TABREF19"], "all_classes": null, "length": 2424}
{"pred": "        53 documents, with an average of 156.1 sentences per document and 167,739 words in total. The corpus consists of 8,275 sentences and 167,739 words in total. The number of annotated entities is summarized in Table TABREF24. The most frequently annotated type of entity is findings, followed by conditions, factors, findings, and modifiers. The average length of entities ranges from one token for all types to 5 tokens for cases, 9 tokens for conditions, 16 tokens for factors, 25 tokens for findings, and 18 tokens for modifiers. The corpus also includes relations between entities", "answers": ["8,275 sentences and 167,739 words in total", "The corpus comprises 8,275 sentences and 167,739 words in total."], "all_classes": null, "length": 2669}
{"pred": "    Yes, it is possible to convert a cloze-style questions to a naturally-looking questions by using a heuristic to generate natural questions. However, none of the existing approaches show a significant improvement in the semi-supervised setting. Recent papers use unlabeled data for QA by training large language models and extracting contextual word vectors, but the applicability of this method in the low-resource setting is unclear. Instead, we show that it is possible to automatically construct the source dataset from the same domain as the target, which turns out to be more beneficial in terms of performance as well. Several cloze datasets have been proposed in the literature which use he", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 2764}
{"pred": "    They consider text categorization and sentiment classification. They also mention that prior knowledge can be used to guide the learning process in both NLP and machine learning communities. (BIBREF0, BIBREF1, BIBREF2, BIBREF3, BIBREF4, BIBREF5, BIBREF6, BIBREF7, BIBREF8) (BIBREF0, BIBREF1, BIBREF2, BIBREF3, BIBREF4, BIBREF5, BIBREF6, BIBREF7, BIBREF8) (BIB", "answers": ["text classification for themes including sentiment, web-page, science, medical and healthcare"], "all_classes": null, "length": 3591}
{"pred": "    Previous methods include rule-based and machine learning approaches, as well as term frequency models. Their model is compared to these previous methods in terms of performance and accuracy. (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes", "answers": ["bag-of-words model, CNN"], "all_classes": null, "length": 5838}
{"pred": "    The training sets of these versions of ELMo are significantly larger compared to the previous ones. The previous versions of ELMo were trained on a one billion word large English corpus, while these versions are trained on much larger corpora, ranging from 20 million to 280 million tokens. The larger training sets allow the models to capture more semantic and contextual information, resulting in better quality embeddings. (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswer", "answers": ["By 14 times.", "up to 1.95 times larger"], "all_classes": null, "length": 3294}
{"pred": "        6946 sentences\n\n        16225 unique words\n\n        3 major classes (Person, Location, Organization)\n\n        299 post-positions\n\n        10% F1 score improvement after lemmatizing post-positions\n\n        95.14% accuracy of POS-tagged BiLSTM model\n\n        64%, 16%, 20% of total dataset into training, development, and test sets\n\n        10 epochs early stopping\n\n        0.5 dropout rate\n\n        0.001 learning rate\n\n        0.0001 weight decay\n\n        128 batch size\n\n        512", "answers": ["3606", "6946"], "all_classes": null, "length": 2835}
{"pred": "        Eusboost and MWMOTE techniques for imbalanced data (around 3 % absolute improvement in INLINEFORM1 value for s2sL compared to MWMOTE, when INLINEFORM2 of the training data is considered). In particular, at lower amounts of training data, s2sL outperforms all the other methods, illustrating its effectiveness even for low resourced data imbalance problems. s2sL method shows an absolute improvement of 6% (INLINEFORM3) in INLINEFORM4 value over the second best (INLINEFORM5 for MWMOTE), when only INLINEFORM6 of the training data is used", "answers": ["MLP", "Eusboost, MWMOTE"], "all_classes": null, "length": 2474}
{"pred": "    Yes, their NER model learns NER from both text and images. The model takes as input a sequence of tokens, each of which comprises a word token, a character sequence, and an image, in their respective representation. The model then uses a modality attention module to combine representations from each modality and produce an entity label for each token. The model is able to learn NER from both text and images because it can use visual information to disambiguate polysemous entities and provide additional context for understanding short and noisy social media posts. The modality attention module also helps the model better handle missing tokens and noisy", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3784}
{"pred": "        Yes, they evaluate only on English datasets. They use the Wall Street Journal (WSJ) portion of the Penn Treebank for both POS tagging and dependency parsing. The skip-gram word embeddings are trained on the one billion word language modeling benchmark dataset in addition to the WSJ corpus. The experimental setup is described in detail in the \"Experiments\" section. The authors do not mention evaluating on any other languages or datasets. Therefore, it is not clear whether their approach is applicable to other languages or not. However, the fact that they use pre-trained word embeddings suggests that their approach may be language-agnostic and could potentially be", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 4320}
{"pred": "    32% νεφοκάλυψης\n\n     νεφοκάλυψης\n\n    32% νεφοκάλυψης\n\n    32% νεφοκάλυψης\n\n    32% νεφοκάλυψης\n\n    32% νεφοκάλυψης\n\n    32% νεφοκάλυψης\n\n    32% νεφοκάλυψης\n\n    32% νεφοκάλυψης\n\n    32% νεφοκάλυψης\n\n    32% νεφοκάλυψης\n\n    32% νεφοκάλυψης\n\n    32% νεφοκάλυψης\n\n    32% νεφοκάλυψης\n\n    32% νεφοκάλυψης\n\n    32% νεφοκάλυψης\n\n    32% νεφοκάλυψης\n\n    32% νεφοκάλυψης\n\n    32% νεφοκάλυψης\n\n    32% νεφοκάλυψης\n\n    32% νεφοκάλυψης\n\n    32% νεφοκάλυψης", "answers": ["0.5115", "0.6103"], "all_classes": null, "length": 6810}
{"pred": "    The datasets they evaluate on are the Wall Street Journal (WSJ) portion of the Penn Treebank and the one billion word language modeling benchmark dataset. They use skip-gram word embeddings trained on the WSJ corpus and the one billion word language modeling benchmark dataset to create the observed data embeddings. The dimensionality of the embeddings is set to 100, and the training context window size is set to 1 to encode more syntactic information. The skip-gram embeddings are trained on the one billion word language modeling benchmark dataset in addition to the WSJ corpus. The general experimental setup is described in the article. (100 words)", "answers": [" Wall Street Journal (WSJ) portion of the Penn Treebank", "Unanswerable"], "all_classes": null, "length": 4327}
{"pred": "    The authors cite several studies that show that engineers often face challenges when applying DNN models to specific NLP tasks. These challenges include the need to choose from multiple frameworks, models and optimization techniques, which can be time-consuming and error-prone. The authors also provide examples of these challenges in the article, such as the need to choose between different embedding layers, loss functions and metrics for different tasks. These examples demonstrate the need for a toolkit that can help engineers choose the right components for their specific tasks. The authors also cite a survey among engineers that identified a spectrum of three typical personas, which further supports the claim that many engineers face", "answers": ["By conducting a survey among engineers", "Unanswerable"], "all_classes": null, "length": 1692}
{"pred": "    On SimpleQuestions and WebQSP. They achieve the state of the art on both benchmarks. Their proposed model, HR-BiLSTM, outperforms previous methods on both tasks and allows their KBQA system to achieve state-of-the-art results. They also show that their model contributes to the good performance in multiple ways, including improving the KBQA end task by 2-3% compared to a baseline relation detector. Finally, they show that their model can be integrated into end-to-end systems and that they will investigate new emerging datasets to handle more characteristics of general QA. (Unanswerable) (Unanswerable)", "answers": ["SimpleQuestions, WebQSP", "WebQSP, SimpleQuestions"], "all_classes": null, "length": 4524}
