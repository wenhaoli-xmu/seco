{"pred": "    The ground truth for fake news is established by manually inspecting the text field within the tweets to label them as containing fake news, or not containing them (according to the characterization presented before). This annotated dataset is publicly available and can be freely reused. {\r\r\n});\n\n    Unanswerable;\r\r\n});\n\n    No;\r\r\n});\n\n    Unanswerable;\r\r\n});\n\n    Yes;\r\r\n});\n\n    No;\r\r\n});\n\n    Unanswerable;\r\r\n});\n\n    Yes;\r\r\n});\n\n    No;\r\r\n});\n\n    Unanswerable;\r\r\n});\n\n    Yes;\r\r\n});\n\n    No;\r\r\n});\n\n    Unanswerable;\r\r\n});\n\n    Yes;\r\r\n});\n\n    No;\r\r\n});\n\n    Unanswerable;\r\r\n});\n\n    Yes;\r\r\n", "answers": ["Ground truth is not established in the paper"], "all_classes": null, "length": 3141}
{"pred": "    The GhostVLAD approach is a pooling strategy that was originally proposed for face recognition. It adds ghost clusters along with the NetVLAD clusters to map any noisy or irrelevant content into ghost clusters. The ghost clusters are not included during the feature aggregation stage, which helps to reduce the contribution of noisy and unwanted features to normal VLAD clusters. The GhostVLAD approach has been shown to improve language identification performance for Indian languages. davidjl\n\nQuestion: What is the best pooling strategy for language identification?\n\nAnswer: davidjl\n\n     davidjl\n\n    The best pooling strategy for language identification is GhostVLAD pooling. It outperforms all other pooling", "answers": ["extension of the NetVLAD, adds Ghost clusters along with the NetVLAD clusters", "An extension of NetVLAD which replaces hard assignment-based clustering with soft assignment-based clustering with the additon o fusing Ghost clusters to deal with noisy content."], "all_classes": null, "length": 2454}
{"pred": "    The model outperforms the state of the art results by 68.8% to 71.8%. {\r\r\n});\n\n    });\n\n    });\n\n    });\n\n    });\n\n    });\n\n    });\n\n    });\n\n    });\n\n    });\n\n    });\n\n    });\n\n    });\n\n    });\n\n    });\n\n    });\n\n    });\n\n    });\n\n    });\n\n    });\n\n    });\n\n    });\n\n    });\n\n    });\n\n    });\n\n    });\n\n    });\n\n    });\n\n    });\n\n    });\n\n    });\n\n    });\n\n    });\n\n    });\n\n    });\n\n    });\n\n    });\n\n    });\n\n    });\n\n    });\n\n    });\n\n    });\n\n    });\n\n    });\n\n    });\n\n    });\n\n    });\n\n    });\n\n    });\n\n    });\n\n    });\n\n    });\n\n", "answers": ["the attention model, MDREA, also outperforms the best existing research results (WAP 0.690 to 0.688)"], "all_classes": null, "length": 3207}
{"pred": "    The additional features and context proposed are context tweets, which are the tweets that the abusive tweet has replied to or quoted. The context tweets are used as an additional feature of neural network models to improve the accuracy of classification. The context tweets are directly used as an additional feature of neural network models by converting them to vectors and concatenating them with the outputs of their corresponding labeled tweets. The context tweets are used to improve the accuracy of classification by providing additional contextual information that can help understand the abusive tweet better. The context tweets are used to improve the accuracy of classification by providing additional information that can help identify the target of the malice and", "answers": ["using tweets that one has replied or quoted to as contextual information", "text sequences of context tweets"], "all_classes": null, "length": 2060}
{"pred": "    The Facebook pages they looked at were FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney.�单词情感检测器\n\nQuestion: What are the emotions they used in their analysis?\n\nAnswer: davidjl\n\n     davidjl\n\n    The emotions they used in their analysis were anger, joy, sadness, surprise.单词情感检测器\n\nQuestion: How did they select the Facebook pages to use as training data?\n\nAnswer: davidjl\n\n     davidjl\n\n    They selected the", "answers": ["FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney", "FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney."], "all_classes": null, "length": 3411}
{"pred": "    Yes, the hashtag and SemEval datasets contain only English data. The hashtag dataset consists of 12,594 unique English hashtags and their associated tweets, while the SemEval 2017 dataset consists of 49,669 tweets and 12,284 hashtags, all of which are in English. The hashtag segmentation model is trained on the 2,518 manually segmented hashtags in the training set of the hashtag dataset, and the sentiment analysis model is trained on the 40,000 tweets in the training set of the SemEval 2017 dataset. Both models are evaluated on the test sets of the respective datasets. The hashtag segmentation model", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3735}
{"pred": "    The proposed evaluation is based on the consensus of multiple workers, represented in the average score of their importance estimates. The average score is used as an estimate for the importance of each proposition. The evaluation is designed to be more reliable and less subjective than traditional approaches. 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ", "answers": ["Answer with content missing: (Evaluation Metrics section) Precision, Recall, F1-scores, Strict match, METEOR, ROUGE-2"], "all_classes": null, "length": 4263}
{"pred": "    The datasets used for evaluation are CNN/DailyMail, New York Times Annotated Corpus (NYT), and XSum. These datasets represent different summary styles ranging from highlights to very brief one sentence summaries. The summaries also vary with respect to the type of rewriting operations they exemplify (e.g., some showcase more cut and paste operations while others are genuinely abstractive). The proportion of novel bi-grams in gold summaries is also reported as a measure of their abstractiveness. CNN/DailyMail and NYT are somewhat abstractive, while XSum is highly abstractive. 】\n\n    The datasets used for evaluation are CNN", "answers": ["CNN/DailyMail news highlights, New York Times Annotated Corpus, XSum", "the CNN/DailyMail news highlights dataset BIBREF24, the New York Times Annotated Corpus (NYT; BIBREF25), XSum BIBREF22"], "all_classes": null, "length": 4369}
{"pred": "    The proposed approach outperforms existing approaches in terms of both word similarity and entailment tasks. It captures the best of both worlds, catering to both word similarity and entailment. The use of KL divergence as an energy function enables capturing asymmetry in entailment datasets, while also promoting diversity in learning senses of a word. The approximate KL divergence function used for computational benefits provides a stricter bound on KL between Gaussian mixtures, leading to better performance on benchmark datasets. Overall, the proposed approach demonstrates effectiveness in learning multi-sense word embedding distributions. {\r\r\n});\n\n    Unanswerable. {\r\r\n});\n\n    Yes. {\r\r\n});\n\n    No. {\r\r\n", "answers": ["GM$\\_$KL achieves better correlation than existing approaches for various metrics on SCWS dataset."], "all_classes": null, "length": 2189}
{"pred": "    The ensemble method works by simply averaging the predictions from the constituent single models. These single models are selected using a greedy algorithm that starts with the best performing model according to validation performance and then adds the best performing model that has not been previously tried. The algorithm keeps the model in the ensemble if it improves the validation performance and discards it otherwise. This process is repeated until all models have been tried once. The resulting model is called a greedy ensemble. The algorithm is offered 10 models and selects 5 of them for the final ensemble. {\r\r\n？」\n\n    Unanswerable. {\r\r\n？」\n\n    Yes.？」\n\n    No.？」\n\n    Un", "answers": ["simply averaging the predictions from the constituent single models"], "all_classes": null, "length": 4212}
{"pred": "    The sources of the datasets are Friends TV sitcom and Facebook messenger chats. The Friends dataset is composed of dialogues from the scripts of the Friends TV sitcom, while the EmotionPush dataset is made up of Facebook messenger chats. Both datasets are annotated by five annotators on a crowd-sourcing platform (Amazon Mechanical Turk), and the labeling work is only based on the textual content. The Friends dataset includes 1,000 English dialogues, while the EmotionPush dataset also includes 1,000 English dialogues. Each dialogue can be further divided into a few consecutive utterances. The Friends dataset is speech-based, while the Em", "answers": ["Friends TV sitcom, Facebook messenger chats"], "all_classes": null, "length": 3185}
{"pred": "    English. The paper focuses on simplifying English texts.？」\n\n    English. The paper focuses on simplifying English texts.？」\n\n    English. The paper focuses on simplifying English texts.？」\n\n    English. The paper focuses on simplifying English texts.？」\n\n    English. The paper focuses on simplifying English texts.？」\n\n    English. The paper focuses on simplifying English texts.？」\n\n    English. The paper focuses on simplifying English texts.？」\n\n    English. The paper focuses on simplifying English texts.？」\n\n    English. The paper focuses on simplifying English texts.？」\n\n    English. The paper focuses on simplifying English texts", "answers": ["English", "Simple English"], "all_classes": null, "length": 2243}
{"pred": "    The IMDb dataset of movie reviews is used for sentiment analysis. {\r\r\n？」\n\n    The IMDb dataset of movie reviews is used for sentiment analysis.？」\n\n    Yes？」\n\n    No？」\n\n    Unanswerable？」\n\n    The IMDb dataset of movie reviews is used for sentiment analysis.？」\n\n    Yes？」\n\n    No？」\n\n    Unanswerable？」\n\n    The IMDb dataset of movie reviews is used for sentiment analysis.？」\n\n    Yes？」\n\n    No？」\n\n    Unanswerable？」\n\n    The IMDb dataset of movie reviews is used for sentiment analysis.？」\n\n    Yes？」\n\n    No？」\n\n    Unanswerable？」\n\n    The IMDb dataset of movie reviews is used for sentiment analysis.", "answers": ["IMDb dataset of movie reviews", "IMDb"], "all_classes": null, "length": 2327}
{"pred": "    0.6033 on the development set and 0.612 on the test set.", "answers": ["F1 scores of 85.99 on the DL-PS data,  75.15 on the EC-MT data and 71.53 on the EC-UQ data ", "F1 of 85.99 on the DL-PS dataset (dialog domain); 75.15 on EC-MT and 71.53 on EC-UQ (e-commerce domain)"], "all_classes": null, "length": 5310}
{"pred": "    Yes, they experimented with this new dataset. They recorded data from 19 participants and discarded the data of one of them due to technical difficulties with the eye-tracking calibration. Hence, they share the data of 18 participants. All participants are healthy adults (mean age = 34 (SD=8.3), 10 females). Their native language is English, originating from Australia, Canada, UK, USA or South Africa. Two participants are left-handed and three participants wear glasses for reading. The participants read 739 sentences that were selected from the Wikipedia corpus provided by culotta2006integrating. The sentences were chosen in the", "answers": ["No"], "all_classes": null, "length": 3445}
{"pred": "        The datasets used are: 1) a set of 124 questions that the users asked, 2) a set of 184,001 Twitter posts and 62,949 news articles related to finance, and 3) a set of 63,270,124 word occurrences. The word vectors are created from these datasets. The training set is created from the 124 questions and consists of 37 classes of intents and 415 samples. The test set is created from the 63 classes of intents and 659 samples. The test set is used to evaluate the accuracy of the classifiers. The test framework is used to validate", "answers": ["Custom dataset with user questions; set of documents, twitter posts and news articles, all related to finance.", "a self-collected financial intents dataset in Portuguese"], "all_classes": null, "length": 13401}
{"pred": "    The best performance was achieved by the Energy sector.", "answers": ["Energy with accuracy of 0.538", "Energy"], "all_classes": null, "length": 10349}
{"pred": "    They compared the performance of the SMT and various NMT models on their built dataset. The models to be tested and their configurations are as follows: SMT: Moses toolkit, KenLM, GIZA++ RNN-based NMT: Basic RNN-based NMT model, target language reversal, residual connection, word2vec Transformer-NMT: Transformer model with the training configuration shown in Table 32. The hyper-parameters are set based on the settings in the paper and the sizes of their training sets. They also tested the basic NMT model with several techniques, such as target language reversal, residual connection, and word", "answers": ["RNN-based NMT model, Transformer-NMT"], "all_classes": null, "length": 3708}
{"pred": "    1. A regularization term associated with neutral features. 2. The maximum entropy of class distribution regularization term. 3. The KL divergence between reference and predicted class distribution. 4. Unanswerable. 5. Yes. 6. No. 7. Unanswerable. 8. Yes. 9. No. 10. Unanswerable. 11. Yes. 12. No. 13. Unanswerable. 14. Yes. 15. No. 16. Unanswerable. 17. Yes. 18. No. 19. Unanswerable", "answers": ["a regularization term associated with neutral features, the maximum entropy of class distribution regularization term, the KL divergence between reference and predicted class distribution", "a regularization term associated with neutral features,  the maximum entropy of class distribution, KL divergence between reference and predicted class distribution"], "all_classes": null, "length": 3604}
{"pred": "    1) SVM with unigram, bigram, and trigram features, which is a standard yet rather strong classifier for text features; 2) SVM with average word embedding, where a document is represented as a continuous representation by averaging the embeddings of the composite words; 3) SVM with average transformed word embeddings (the INLINEFORM0 in equation EQREF6), where a document is represented as a continuous representation by averaging the transformed embeddings of the composite words; 4) two mature deep learning models on text classification, CNN BIBREF3 and Recurrent Convolutional Neural Networks (RCNN) BIBREF0", "answers": ["SVM with unigram, bigram, and trigram features, SVM with average word embedding, SVM with average transformed word embeddings, CNN, ecurrent Convolutional Neural Networks, SVM and deep learning models with comment information", "SVM with unigram, bigram, trigram features, with average word embedding, with average transformed word embeddings, CNN and RCNN, SVM, CNN, RCNN with comment information"], "all_classes": null, "length": 4512}
{"pred": "        5.2% improvement in the macro-averaged Mean Absolute Error (MAE) score. 2.3% improvement in the micro-averaged MAE score. 5.2% improvement in the macro-averaged MAE score. 2.3% improvement in the micro-averaged MAE score. 5.2% improvement in the macro-averaged MAE score. 2.3% improvement in the micro-averaged MAE score. 5.2% improvement in the macro-averaged MAE score. 2.3% improvement in the micro-", "answers": ["They decrease MAE in 0.34"], "all_classes": null, "length": 2735}
{"pred": "    The model improves interpretability by allowing different attention heads to learn different sparsity patterns, which can lead to more specialized and confident behavior. Additionally, the model can learn to combine sparse and dense attention, which can help the model better handle ambiguous inputs. The model also allows for the automatic learning of the shape and sparsity of the attention mapping, which can help the model adapt to different tasks and inputs. Overall, the model provides a more flexible and interpretable attention mechanism that can lead to improved performance and interpretability.", "answers": ["the attention heads in the proposed adaptively sparse Transformer can specialize more and with higher confidence", "We introduce sparse attention into the Transformer architecture"], "all_classes": null, "length": 4902}
{"pred": "    The baseline was a context-agnostic machine translation system. {\r\r\n？」\n\n    The baseline was a context-agnostic machine translation system.？」\n\n    Unanswerable？」\n\n    Yes？」\n\n    No？」\n\n    Unanswerable？」\n\n    The baseline was a context-agnostic machine translation system.？」\n\n    The baseline was a context-agnostic machine translation system.？」\n\n    The baseline was a context-agnostic machine translation system.？」\n\n    The baseline was a context-agnostic machine translation system.？」\n\n    The baseline was a context-agnostic machine translation system.？」\n\n    The baseline was a context-agnostic machine translation system.？」\n\n    The baseline was a context-", "answers": [" MT system on the data released by BIBREF11", "Transformer base, two-pass CADec model"], "all_classes": null, "length": 3716}
{"pred": "    The metrics used for evaluation are Labeled Attachment Scores (LAS) for zero-shot dependency parsing and accuracy for cross-lingual natural language inference (XNLI). The LAS score measures the percentage of correctly labeled dependencies in a treebank, while the accuracy score measures the percentage of correctly classified sentences in the XNLI dataset. Both metrics are used to evaluate the performance of the bilingual language models (RAMEN) in zero-shot cross-lingual transfer tasks. The higher the LAS score and accuracy, the better the performance of the model. 】\n\nQuestion: How does the approach compare to mBERT in terms of zero-shot performance", "answers": ["translation probabilities, Labeled Attachment Scores (LAS)", "accuracy, Labeled Attachment Scores (LAS)"], "all_classes": null, "length": 3402}
{"pred": "    The attention module is pretrained on the MT task.", "answers": ["the model is pre-trained on CTC-based ASR task and MT task in the pre-training stage."], "all_classes": null, "length": 4656}
{"pred": "    The stylistic features obtained are emoticons, laughter expressions such as “lol” etc. These features are used to detect sarcasm in text.", "answers": ["Unanswerable"], "all_classes": null, "length": 3543}
{"pred": "    The encoder has an LSTM layer.", "answers": ["LSTM", "LSTM"], "all_classes": null, "length": 2289}
{"pred": "    Yes, WordNet is useful for taxonomic reasoning for this task. The WordNet dataset contains a set of triples that represent semantic relations between concepts, including hypernymy, hyponymy, synonymy, and definitions. These relations can be used to generate questions that require taxonomic reasoning, such as identifying the hypernym or hyponym of a given concept. The WordNet dataset also contains example sentences and glosses for each concept, which can be used to contextualize the questions and make them more natural. Overall, the WordNet dataset provides a rich source of knowledge that can be used to probe the ability of models", "answers": ["Unanswerable", "Yes"], "all_classes": null, "length": 6391}
{"pred": "    The baselines were the conventional automatic speech recognition (ASR) systems, which typically consist of several independently learned components: an acoustic model to predict context-dependent sub-phoneme states (senones) from audio, a graph structure to map senones to phonemes, and a pronunciation model to map phonemes to words. Hybrid systems combine hidden Markov models to model state dependencies with neural networks to predict states. Newer approaches such as end-to-end (E2E) systems reduce the overall complexity of the final system. The baselines were also the prior work that has explored using time-delay neural networks (TDNN), other", "answers": ["Unanswerable", "LF-MMI Attention\nSeq2Seq \nRNN-T \nChar E2E LF-MMI \nPhone E2E LF-MMI \nCTC + Gram-CTC"], "all_classes": null, "length": 1856}
{"pred": "    22,880 users.", "answers": ["22,880 users", "20,000"], "all_classes": null, "length": 4160}
{"pred": "    The metrics used for evaluation are BPE perplexity, BLEU-1/4, ROUGE-L, Distinct-1/2, and recipe-level coherence. The personalized models outperform the baseline in BPE perplexity, and achieve higher recipe-level coherence scores. The personalized models also perform better in terms of local coherence, as measured by step entailment scores. Human evaluation shows that the personalized models generate recipes that are more coherent and preferable to those generated by the baseline. The personalized models also achieve higher user matching accuracy and mean reciprocal rank, indicating that they personalize generated recipes to the given user profiles. Overall, the personalized", "answers": ["Byte-Pair Encoding perplexity  (BPE PPL),\nBLEU-1,\nBLEU-4,\nROUGE-L,\npercentage of distinct unigram (D-1),\npercentage of distinct bigrams(D-2),\nuser matching accuracy(UMA),\nMean Reciprocal Rank(MRR)\nPairwise preference over baseline(PP)", "BLEU-1/4 and ROUGE-L, likelihood of generated recipes using identical input specifications but conditioned on ten different user profiles, user matching accuracy (UMA), Mean Reciprocal Rank (MRR), neural scoring model from BIBREF33 to measure recipe-level coherence", " Distinct-1/2, UMA = User Matching Accuracy, MRR\n= Mean Reciprocal Rank, PP = Pairwise preference over baseline (evaluated for 310 recipe pairs per model)"], "all_classes": null, "length": 2673}
{"pred": "    The labels they create on their dataset are: 1) Open-ended Inquiry, 2) Detailed Inquiry, 3) Multi-Intent Inquiry, 4) Reconfirmation Inquiry, 5) Inquiry with Transitional Clauses, 6) Yes/No Response, 7) Detailed Response, 8) Response with Revision, 9) Response with Topic Drift, 10) Response with Transitional Clauses. 1) Open-ended Inquiry: Inquiries about general well-being or a particular symptom; e.g., “How are you feeling?” and “Do you cough?” 2) Detailed Inquiry: Inquiries with", "answers": ["(1) the time the patient has been experiencing the symptom, (2) activities that trigger the symptom (to occur or worsen), (3) the extent of seriousness, (4) the frequency occurrence of the symptom, and (5) the location of symptom, No Answer", "the time the patient has been experiencing the symptom, activities that trigger the symptom, the extent of seriousness, the frequency occurrence of the symptom, the location of symptom, 9 symptoms"], "all_classes": null, "length": 3424}
{"pred": "    1000 sentences. 2 sentences. 1000 sentences. 2 sentences. 1000 sentences. 2 sentences. 1000 sentences. 2 sentences. 1000 sentences. 2 sentences. 1000 sentences. 2 sentences. 1000 sentences. 2 sentences. 1000 sentences. 2 sentences. 1000 sentences. 2 sentences. 1000 sentences. 2 sentences. 1000 sentences. 2 sentences. 1000 sentences. 2 sentences. 1000 sentences. 2 sentences. 1000 sentences. 2 sentences. ", "answers": ["57,505 sentences", "57,505 sentences"], "all_classes": null, "length": 4371}
{"pred": "    The tasks used for evaluation are Neural Machine Translation, and analyzing the behavior of the learned attention heads.rbrakk\n\nQuestion: What are the main contributions of the paper?\n\nAnswer: 노출등록\n\n    The main contributions of the paper are a novel strategy for adaptively sparse attention, and an analysis of Transformers with sparse attention mappings. The paper shows potential in both translation accuracy as well as in model interpretability.rbrakk\n\nQuestion: What are the main findings of the analysis?\n\nAnswer: 노출등록\n\n    The analysis finds that the attention heads in the proposed adaptively sparse Transformer can specialize more and with higher confidence. The adaptivity strategy relies only on", "answers": ["four machine translation tasks: German -> English, Japanese -> English, Romanian -> English, English -> German", " four machine translation tasks, IWSLT 2017 German $\\rightarrow $ English BIBREF27, KFTT Japanese $\\rightarrow $ English BIBREF28, WMT 2016 Romanian $\\rightarrow $ English BIBREF29, WMT 2014 English $\\rightarrow $ German BIBREF30"], "all_classes": null, "length": 4898}
{"pred": "    The improvement in performance for Estonian in the NER task is 0.05.\r\r\n\r\r\n    The improvement in performance for Estonian in the NER task is 0.05.\r\r\n\r\r\n    The improvement in performance for Estonian in the NER task is 0.05.\r\r\n\r\r\n    The improvement in performance for Estonian in the NER task is 0.05.\r\r\n\r\r\n    The improvement in performance for Estonian in the NER task is 0.05.\r\r\n\r\r\n    The improvement in performance for Estonian in the NER task is 0.05.\r\r\n\r\r\n    The improvement in performance for Estonian", "answers": ["5 percent points.", "0.05 F1"], "all_classes": null, "length": 3290}
{"pred": "    The authors have backgrounds in computational linguistics, political science, and digital humanities. They have experience in analyzing text as social and cultural data. They have worked on projects related to hate speech, rumors, and political traits. They have also worked on projects that involve mixed deductive-inductive strategies for developing annotation schemes. They have experience in using various computational approaches such as dictionaries, supervised models, and topic modeling. They have experience in validating and analyzing the results of their analyses. They have experience in combining computational text analysis with qualitative approaches. They have experience in working across disciplines and negotiating about appropriate approaches to analysis. They have experience in making", "answers": ["Unanswerable"], "all_classes": null, "length": 8506}
{"pred": "    Unanswerable. The paper does not explicitly state whether the proposed method is unsupervised or supervised. The paper mentions that the LDA model is used to extract topic-based features, but it does not specify how these features are used in the spam detection process. The paper also mentions that the proposed method is evaluated using a 10-fold cross-validation approach, which is typically used in supervised learning. Therefore, it is unclear whether the proposed method is unsupervised or supervised. The paper does not provide enough information to answer this question. It is possible that the proposed method is supervised, but the paper does not explicitly state this.", "answers": ["No", "No"], "all_classes": null, "length": 2239}
{"pred": "    The Nguni languages are similar to each other and harder to distinguish. The same is true of the Sotho languages. {\r\r\n？」\n\n    The Nguni languages are similar to each other and harder to distinguish. The same is true of the Sotho languages.？」\n\n    The Nguni languages are similar to each other and harder to distinguish. The same is true of the Sotho languages.？」\n\n    The Nguni languages are similar to each other and harder to distinguish. The same is true of the Sotho languages.？」\n\n    The Nguni languages are similar to each other and harder to distinguish", "answers": ["Nguni languages (zul, xho, nbl, ssw), Sotho languages (nso, sot, tsn)", "The Nguni languages are similar to each other, The same is true of the Sotho languages"], "all_classes": null, "length": 1877}
{"pred": "    They compared the performance of 6-layers and 9-layers sMBR models. The 3 additional layers of layer-wise training brings relative 12.6% decreasing of CER. The averaged CER of sMBR models with different layers decreases absolute 0.73% approximately compared with CE models. The 2-layers distilled model of Shenma voice search has shown a impressive performance on Shenma Test, and they call it Shenma model. The Shenma model presents the worst performance among three methods, since it does not trained for Amap scenario. The 2-layers Shenma model further trained", "answers": ["Unidirectional LSTM networks with 2, 6, 7, 8, and  9 layers."], "all_classes": null, "length": 4286}
{"pred": "    The Wikipedia dataset consists of articles from English Wikipedia, with quality class labels assigned by the Wikipedia community. The quality class of a Wikipedia article is assigned by Wikipedia reviewers or any registered user, who can discuss through the article's talk page to reach consensus. The Wikipedia dataset contains around 5K Featured Articles, 28K Good Articles, 212K B-class Articles, 533K C-class Articles, 2.6M Start Articles, and 3.2M Stub Articles. The arXiv dataset consists of three subsets of academic articles under the arXiv repository of Computer Science (cs), from the three subject areas", "answers": ["a sample of  29,794 wikipedia articles and 2,794 arXiv papers "], "all_classes": null, "length": 4187}
{"pred": "    The human evaluation was conducted by a group of 50 native speakers of both English and Tamil who were well-versed in both languages. A collection of 100 sentences was selected from the test set results and presented to the annotators for evaluation. The annotators were asked to rate the adequacy and fluency of the translations on a 5-point scale. The ranking of the translations was also determined by the annotators. The intra-annotator agreement was calculated using the kappa coefficient. The results of the human evaluation are presented in Table 32. The kappa coefficient for the ranking of the RNNMorph model was 0", "answers": ["50 human annotators ranked a random sample of 100 translations by Adequacy, Fluency and overall ranking on a 5-point scale.", "adequacy, precision and ranking values"], "all_classes": null, "length": 5344}
{"pred": "    Yes, they test their framework performance on commonly used language pairs, such as English-to-German. They report that their approach achieves considerable improvements in BLEU scores compared to the baseline NMT system. They also show that incorporating additional parallel data from other language pairs can further improve the performance of their framework. Overall, their approach demonstrates the potential of multilingual NMT for improving translation quality.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4472}
{"pred": "    The models are evaluated based on their efficiency and accuracy in reconstructing the target sentence from the keywords provided by the user. The efficiency is measured by the retention rate of tokens, while the accuracy is measured by the fraction of sentences generated by greedily decoding the model that exactly matches the target sentence. The models are also evaluated based on their stability and robustness. The stability is measured by the sensitivity of the tradeoff between efficiency and accuracy to the tradeoff parameter, while the robustness is measured by the ability of the models to generalize to unseen data. Finally, the models are evaluated based on their interpretability, which is measured by", "answers": ["by training an autocomplete system on 500K randomly sampled sentences from Yelp reviews", "efficiency of a communication scheme $(q_{\\alpha },p_{\\beta })$ by the retention rate of tokens, which is measured as the fraction of tokens that are kept in the keywords, accuracy of a scheme is measured as the fraction of sentences generated by greedily decoding the model that exactly matches the target sentence"], "all_classes": null, "length": 1873}
{"pred": "    Precision, recall, and F-measure are the evaluation metrics that are looked at for classification tasks. Precision measures the proportion of true positives among all the predicted positives, recall measures the proportion of true positives among all the actual positives, and F-measure is a weighted average of precision and recall. These metrics are used to assess the performance of a classification model and to compare different models. The higher the values of precision, recall, and F-measure, the better the performance of the model. 0\n\nQuestion: What is the best performing algorithm for non-topical sentence classification in the article?\n\nAnswer: 노출등록\n\n    SVM with", "answers": ["Precision, Recall, F-measure, accuracy", "Precision, Recall and F-measure"], "all_classes": null, "length": 3044}
{"pred": "    The source domain is the domain from which labeled data is available, and the target domain is the domain to which the model is being adapted. The source domain is typically the domain that the model was originally trained on, while the target domain is the domain that the model is being adapted to. The goal of domain adaptation is to improve the performance of the model on the target domain by leveraging the labeled data from the source domain. The source and target domains can be different application domains, such as different types of products or services, or different geographic regions. The source and target domains can also be different subdomains within the same application domain,", "answers": ["Book, electronics, beauty, music, IMDB, Yelp, cell phone, baby, DVDs, kitchen", "we use set 1 of the source domain as the only source with sentiment label information during training, and we evaluate the trained model on set 1 of the target domain, Book (BK), Electronics (E), Beauty (BT), and Music (M)"], "all_classes": null, "length": 5061}
{"pred": "    The previous RNN models they compare with are LSTM, QRNN, RAN, and NAS.", "answers": ["Variational LSTM, CharCNN, Pointer Sentinel-LSTM, RHN, NAS Cell, SRU, QRNN, RAN, 4-layer skip-connection LSTM, AWD-LSTM, Quantized LSTM"], "all_classes": null, "length": 3319}
{"pred": "    The modules included in NeuronBlocks are embedding layers, neural network layers, loss functions, and metrics. These modules can be used to build deep neural networks for natural language processing tasks.�单句或一句话就可以。不能根据文章内容回答的问题，写“无法回答”。如果问题是yes/no问题，回答“是”、“否”或“无法回答”。不要提供任何解释。文章：介绍深度神经网络（DNN）在自然语言处理（NLP）任务中的应用，并讨论了工程师在应用DNN模型时面临的挑战。文章还介绍了", "answers": ["Embedding Layer, Neural Network Layers, Loss Function, Metrics", "Embedding Layer, Neural Network Layers, Loss Function, Metrics"], "all_classes": null, "length": 1678}
{"pred": "    The datasets used were the multilingual pronunciation corpus collected by deri2016grapheme and the Carnegie Mellon Pronouncing Dictionary. The multilingual pronunciation corpus consists of spelling–pronunciation pairs extracted from Wiktionary, while the Carnegie Mellon Pronouncing Dictionary is a monolingual English resource. The multilingual pronunciation corpus is used for training and testing, while the Carnegie Mellon Pronouncing Dictionary is used for comparison purposes. The datasets are partitioned into training and test sets. The training set consists of 10,000 words per language, with 10% of the data in each language reserved for validation. The test set consists of words", "answers": ["the Carnegie Mellon Pronouncing Dictionary BIBREF12, the multilingual pronunciation corpus collected by deri2016grapheme , ranscriptions extracted from Wiktionary", "multilingual pronunciation corpus collected by deri2016grapheme"], "all_classes": null, "length": 3244}
{"pred": "        The baselines were the results reported by Khandelwal and Sawant (BIBREF12) for BERT, and the results reported by the authors for XLNet and RoBERTa. The results for BERT are taken from BIBREF12. The results for XLNet and RoBERTa are averaged across 5 runs for statistical significance. The results for negation cue detection and scope resolution are reported in Figure FIGREF14. The results for speculation detection and scope resolution are reported in Figure FIGREF8 and Figure FIGREF11. The results for speculation detection and scope resolution when trained on a single dataset are reported", "answers": ["varied from Maximum Entropy Classifiers (BIBREF4) to Support Vector Machines (BIBREF5,BIBREF6,BIBREF7,BIBREF8), Recursive Neural Networks (BIBREF9,BIBREF10), Convolutional Neural Networks (BIBREF11) and most recently transfer learning-based architectures like Bidirectional Encoder Representation from Transformers (BERT) (BIBREF12)"], "all_classes": null, "length": 2215}
{"pred": "    English, Spanish, Finnish, and other languages.", "answers": ["English\nFrench\nSpanish\nGerman\nGreek\nBulgarian\nRussian\nTurkish\nArabic\nVietnamese\nThai\nChinese\nHindi\nSwahili\nUrdu\nFinnish", "English, Spanish, Finnish"], "all_classes": null, "length": 4086}
{"pred": "    They test their method on Named Entity Recognition, POS tagging, text classification, and language modeling. {\r\r\n        […]\n\n    Unanswerable. {\r\r\n        […]\n\n    Yes. {\r\r\n        […]\n\n    No. {\r\r\n        […]\n\n    Unanswerable. {\r\r\n        […]\n\n    Yes. {\r\r\n        […]\n\n    No. {\r\r\n        […]\n\n    Unanswerable. {\r\r\n        […]\n\n    Yes. {\r\r\n        […]\n\n    No. {\r\r\n        […]\n\n    Unanswerable. {\r\r\n        […]\n\n    Yes. {\r\r\n        […]\n\n    No. {\r\r\n        […]\n\n    Unanswerable.・━・━        […]\n\n    Yes.・━・━        […]\n\n    No.・━・━        […]\n\n    Unanswerable.", "answers": ["None"], "all_classes": null, "length": 2473}
{"pred": "    Yes, they use pretrained GloVe embeddings.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4457}
{"pred": "    Yes, PolyReponse was evaluated against a baseline system that uses explicit semantic representations. The results showed that PolyReponse outperformed the baseline system in terms of accuracy and naturalness of the responses.", "answers": ["No", "No"], "all_classes": null, "length": 2738}
{"pred": "        The researchers obtain psychological dimensions of people by analyzing the language used in their blog posts. They use a tool called Linguistic Inquiry and Word Count (LIWC) to analyze the language and extract information about the psychological dimensions of the bloggers. The LIWC tool categorizes words into different categories based on their psycholinguistic and semantic properties, and the researchers use these categories to generate maps that reflect the psychological dimensions of the bloggers. For example, they can generate maps that show the distribution of positive feelings or money-related words in different regions of the country. The researchers also use other tools and resources to extract information about the psychological dimensions of", "answers": ["using the Meaning Extraction Method", "Unanswerable"], "all_classes": null, "length": 1440}
{"pred": "    The ML methods aim to identify the following argument components: backing, claim, premise, rebuttal, refutation. {\r\r\n？」\n\n    The ML methods aim to identify the following argument components: backing, claim, premise, rebuttal, refutation.？」\n\n    The ML methods aim to identify the following argument components: backing, claim, premise, rebuttal, refutation.？」\n\n    The ML methods aim to identify the following argument components: backing, claim, premise, rebuttal, refutation.？」\n\n    The ML methods aim to identify the following argument components: backing, claim, premise, rebuttal, refutation.？」\n\n    The", "answers": ["claim, premise, backing, rebuttal, and refutation", "claim, premise, backing, rebuttal, refutation"], "all_classes": null, "length": 14472}
{"pred": "    3-grams are aligned using PARENT.", "answers": ["Unanswerable", "Answer with content missing: (Parent subsections) combine precisions for n-gram orders 1-4"], "all_classes": null, "length": 3827}
{"pred": "    1,873 conversation threads, roughly 14k tweets.ETwitter dataset is smaller than the OSG dataset.ETwitter dataset contains less therapeutic conversations in comparison to specialized on-line support forums.ETwitter dataset is broadly applicable to public health research.ETwitter dataset contains less therapeutic conversations in comparison to specialized on-line support forums.ETwitter dataset is broadly applicable to public health research.ETwitter dataset contains less therapeutic conversations in comparison to specialized on-line support forums.ETwitter dataset is broadly applicable to public health research.ETwitter dataset contains less therapeutic conversations in comparison to specialized on-line support forums.ETwitter dataset is broadly applicable to public health research.ETwitter dataset contains less", "answers": ["1,873 Twitter conversation threads, roughly 14k tweets", "1,873 Twitter conversation threads, roughly 14k tweets"], "all_classes": null, "length": 3721}
{"pred": "        12 languages are covered: English, Mandarin Chinese, French, Spanish, Finnish, Russian, Polish, Hebrew, Estonian, Welsh, Kiswahili, and Yue Chinese. The languages are chosen to be typologically diverse and include under-resourced languages. The resource covers an unprecedented amount of 1,888 word pairs, carefully balanced according to their similarity score, frequency, concreteness, part-of-speech class, and lexical field. The core Multi-SimLex we release with this paper already enables researchers to carry out novel linguistic analysis as well as establishes a benchmark for evaluating representation learning models. The results indicate that", "answers": ["Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese", "Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese"], "all_classes": null, "length": 14660}
{"pred": "    The two datasets model is applied to are the Wikipedia and CMV datasets. The Wikipedia dataset is an expanded version of the annotated Wikipedia conversations dataset from BIBREF9, while the CMV dataset is constructed from conversations collected via the Reddit API. Both datasets are used to evaluate the performance of the model in forecasting conversational derailment. {\r\r\n？」\n\n    The two datasets model is applied to are the Wikipedia and CMV datasets. The Wikipedia dataset is an expanded version of the annotated Wikipedia conversations dataset from BIBREF9, while the CMV dataset is constructed from conversations collected via the Reddit API. Both datasets are used to evaluate the performance", "answers": [" `Conversations Gone Awry' dataset, subreddit ChangeMyView", "An expanded version of the existing 'Conversations Gone Awry' dataset and the ChangeMyView dataset, a subreddit whose only annotation is whether the conversation required action by the Reddit moderators. "], "all_classes": null, "length": 4718}
{"pred": "    No, the pipeline components were not based on deep learning models. The pipeline components were based on existing tools and libraries for natural language processing, such as Freeling and System-T. The pipeline components were designed to be modular and independent of specific language, allowing for potential application to other languages by changing the modules or models. The pipeline components were evaluated against existing systems and found to perform better than other systems. The pipeline components were developed within the context of the Agatha project, which aimed to create a knowledge base for criminal law events. The pipeline components were designed to extract subject-verb-object triples and perform ontology matching procedures. The pipeline", "answers": ["No", "No"], "all_classes": null, "length": 2276}
{"pred": "    The quality of the data is empirically evaluated by performing sanity checks on the translations. These checks include computing sentence-level BLEU scores with the NLTK implementation, manually inspecting examples where the source transcript is identical to the translation, measuring the perplexity of the translations using a language model, computing the ratio of English characters in the translations, and calculating similarity scores between transcripts and translations based on LASER cross-lingual sentence embeddings. Samples with low scores are manually inspected and sent back for translation when needed. The overlaps of train, development, and test sets are also checked to ensure they are totally disjoint. Additionally, the overlaps between", "answers": ["Validated transcripts were sent to professional translators., various sanity checks to the translations,  sanity check the overlaps of train, development and test sets", "computed sentence-level BLEU, We manually inspected examples where the source transcript was identical to the translation, measured the perplexity of the translations, computed the ratio of English characters in the translations, calculate similarity scores between transcripts and translations"], "all_classes": null, "length": 2435}
{"pred": "    The proposed model encodes the information from audio and text sequences using dual RNNs and then combines the information from these sources using a feed-forward neural model to predict the emotion class.", "answers": ["combines the information from these sources using a feed-forward neural model", "encodes the information from audio and text sequences using dual RNNs and then combines the information from these sources using a feed-forward neural model"], "all_classes": null, "length": 3201}
{"pred": "    2.11 BLEU, 1.7 FKGL and 1.07 SARI. {\r\r\n）\n\n    2.11 BLEU, 1.7 FKGL and 1.07 SARI. {\r\r\n）\n\n    2.11 BLEU, 1.7 FKGL and 1.07 SARI. {\r\r\n）\n\n    2.11 BLEU, 1.7 FKGL and 1.07 SARI. {\r\r\n）\n\n    2.11 BLEU, 1.7 FKGL and 1.07 SARI. {\r\r\n）\n\n    2.11 BLEU,", "answers": ["For the WikiLarge dataset, the improvement over baseline NMT is 2.11 BLEU, 1.7 FKGL and 1.07 SARI.\nFor the WikiSmall dataset, the improvement over baseline NMT is  8.37 BLEU.", "6.37 BLEU"], "all_classes": null, "length": 2271}
{"pred": "    3 humans evaluated the results. {\r\r\n？」\n\n    3 humans evaluated the results.？」\n\n    3 humans evaluated the results.？」\n\n    3 humans evaluated the results.？」\n\n    3 humans evaluated the results.？」\n\n    3 humans evaluated the results.？」\n\n    3 humans evaluated the results.？」\n\n    3 humans evaluated the results.？」\n\n    3 humans evaluated the results.？」\n\n    3 humans evaluated the results.？」\n\n    3 humans evaluated the results.？」\n\n    3 humans evaluated the results.？」\n\n    3 humans evaluated the results.？」\n\n    3 humans evaluated the results.？」\n\n   ", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3711}
{"pred": "    Tweets going viral are those that are retweeted more than 1000 times. {\r\r\n});\n\nQuestion: What is their definition of fake news?\n\nAnswer: 노출등록\n\n    Tweets containing fake news are those that fall into one of the following categories: serious fabrication, large-scale hoaxes, jokes taken at face value, slanted reporting of real facts, and stories where the truth is contentious. {\r\r\n});\n\nQuestion: What are their research hypotheses?\n\nAnswer: 노출등록\n\n    1. Exposure: Viral tweets containing fake news will have a shorter lifespan than viral tweets not containing fake news. 2. Characterization: Accounts spreading fake news will", "answers": ["Viral tweets are the ones that are retweeted more than 1000 times", "those that contain a high number of retweets"], "all_classes": null, "length": 3144}
{"pred": "    BERT performs best by itself. {\r\r\n    \n\n    Unanswerable. {\r\r\n    \n\n    Yes. {\r\r\n    \n\n    No. {\r\r\n    \n\n    Unanswerable. {\r\r\n    \n\n    Yes. {\r\r\n    \n\n    No. {\r\r\n    \n\n    Unanswerable. {\r\r\n    \n\n    Yes. {\r\r\n    \n\n    No. {\r\r\n    \n\n    Unanswerable. {\r\r\n    \n\n    Yes. {\r\r\n    \n\n    No. {\r\r\n    \n\n    Unanswerable. {\r\r\n    \n\n    Yes. {\r\r\n    \n\n    No. {\r\r\n    \n\n    Unanswerable. {\r\r\n    \n\n    Yes. {\r\r\n    \n\n    No. {\r\r\n    \n\n    Unanswerable. {\r\r\n    \n\n    Yes. {\r\r\n    \n\n    No. {\r\r\n", "answers": ["BERT"], "all_classes": null, "length": 1507}
{"pred": "    The data was collected using crowdsourcing and an Android application. Each respondent installed the application on their personal device and recorded several phrases in different sessions. The Android application did various checks on each utterance and if it passed all of them, the respondent was directed to the next phrase. For more information about data collection scenario, please refer to BIBREF4. {\r\r\n？」\n\n    The data collection project was mainly supported by Sharif DeepMine company. The work on the paper was supported by Czech National Science Foundation (GACR) project \"NEUREM3\" No. 19-26934X and the National Programme of Sustainability", "answers": ["Android application"], "all_classes": null, "length": 3795}
{"pred": "    The machine learning methods used for RQE are logistic regression and deep learning models. The deep learning models used are based on recurrent neural networks and convolutional neural networks. The deep learning models are trained on datasets such as SNLI and multiNLI. The logistic regression model is trained on datasets such as clinical-RQE. The deep learning models achieve better results on datasets such as SNLI and multiNLI, while the logistic regression model achieves better results on datasets such as clinical-RQE. The deep learning models are also used for question similarity detection, and the hybrid method of combining the logistic regression model and the IR baseline provides the best", "answers": ["Logistic Regression, neural networks"], "all_classes": null, "length": 7257}
{"pred": "    The benchmark dataset is the Social Honeypot dataset, which is a collection of legitimate users and spammers on Twitter. The quality of the dataset is high, as it was created by deploying 60 seed social accounts on Twitter to attract spammers and collecting their interactions. The dataset has been extensively explored in the paper and has been used to validate the effectiveness of the proposed features. The dataset is publicly available and can be accessed by researchers. The quality of the dataset is high, as it was created by deploying 60 seed social accounts on Twitter to attract spammers and collecting their interactions. The dataset has been extensively explored in the", "answers": ["Social Honeypot dataset (public) and Weibo dataset (self-collected); yes", "Social Honeypot, which is not of high quality"], "all_classes": null, "length": 2242}
{"pred": "    The decoder has an LSTM one component at a time, e.g. the tag PRO;NOM;SG;1 is predicted as a sequence of four components, PRO, NOM, SG, 1. {\r\r\n？」\n\n    The decoder has an LSTM one component at a time, e.g. the tag PRO;NOM;SG;1 is predicted as a sequence of four components, PRO, NOM, SG, 1.？」\n\n    Yes？」\n\n    The decoder has an LSTM one component at a time, e.g. the tag PRO;NOM;SG;1 is predicted as a sequence of four components, PRO,", "answers": ["LSTM", "LSTM"], "all_classes": null, "length": 2289}
{"pred": "    Yes, the experiments are conducted on English data. The authors use three datasets, namely FSD, Twitter, and Google, which are all English datasets. The authors also use a named entity tagger and a Twitter Part-of-Speech (POS) tagger to extract named entities and POS tags from the tweets, and the Stanford Named Entity Recognizer to identify named entities in the news articles. The authors also remove common stopwords and only keep the recognized named entities and the tokens which are verbs, nouns or adjectives. Therefore, the results are only reported on English data. {\r\r\n\n\n    Unanswerable. The article does not provide", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3838}
{"pred": "    The best performing model among author's submissions is the ensemble of Logistic Regression, CNN and BERT with relax-voting ensemble scheme. It had a F1 score of 0.673 on the dev (external) set. The second best performing model is the ensemble of Multi-grain, Multi-task LSTM-CRF with BERT, which had a F1 score of 0.665 on the dev (external) set. The third best performing model is the LSTM-CRF with word embeddings and character embeddings, which had a F1 score of 0.658 on the dev (external) set. The fourth best performing model", "answers": ["For SLC task, the \"ltuorp\" team  has the best performing  model (0.6323/0.6028/0.6649 for F1/P/R  respectively) and for FLC task the \"newspeak\" team  has the best performing  model (0.2488/0.2863/0.2201 for F1/P/R respectively)."], "all_classes": null, "length": 1541}
{"pred": "    The baseline was a uni-directional NMT model trained on the corresponding parallel corpus. (a1) and (b1) in Table TABREF27. (c1) in Table TABREF31. (a1) and (b1) in Table TABREF27. (c1) in Table TABREF31. (a1) and (b1) in Table TABREF27. (c1) in Table TABREF31. (a1) and (b1) in Table TABREF27. (c1) in Table TABREF31. (a1) and (b1) in", "answers": ["pivot-based translation relying on a helping language BIBREF10, nduction of phrase tables from monolingual data BIBREF14 , attentional RNN-based model (RNMT) BIBREF2, Transformer model BIBREF18, bi-directional model BIBREF11, multi-to-multi (M2M) model BIBREF8, back-translation BIBREF17", "M2M Transformer"], "all_classes": null, "length": 4542}
{"pred": "    0.7033\n\n    0.7033\n\n    0.7033\n\n    0.7033\n\n    0.7033\n\n    0.7033\n\n    0.7033\n\n    0.7033\n\n    0.7033\n\n    0.7033\n\n    0.7033\n\n    0.7033\n\n    0.7033\n\n    0.7033\n\n    0.7033\n\n    0.7033\n\n    0.7033\n\n    0.7033\n\n    ", "answers": ["0.7033", "0.7033"], "all_classes": null, "length": 6810}
{"pred": "    The paper explores word embedding techniques such as word2vec and neural networks. It also explores second-order co-occurrence vectors and their integration with semantic similarity measures. Finally, it compares the proposed method to other distributional approaches.", "answers": ["Skip–gram, CBOW", "integrated vector-res, vector-faith, Skip–gram, CBOW"], "all_classes": null, "length": 4259}
{"pred": "    They match words before reordering them by translating each word in the source language into English using a bilingual dictionary.�单词匹配后再进行重新排序。使用双语词典将源语言中的每个单词翻译成英语。）\n\nQuestion: What is the name of the system used for reordering English sentences?\n\nAnswer:", "answers": ["Unanswerable", "CFILT-preorder system"], "all_classes": null, "length": 2231}
{"pred": "    No, the paper does not explore extraction from electronic health records. The paper focuses on extraction from scientific articles. The paper mentions that electronic health records are important for clinicians and researchers, but does not discuss extraction from this type of data. The paper also mentions that extraction from scientific articles can assist in the creation of databases and suggest new paths for research. The paper does not mention any specific databases or research paths that are suggested by extraction from scientific articles. The paper does not discuss any specific challenges or difficulties associated with extraction from electronic health records. The paper does not mention any specific methods or techniques that have been used to extract information from electronic", "answers": ["Yes"], "all_classes": null, "length": 3035}
{"pred": "    The experts used for annotation were seven legal experts with training in privacy law. They were tasked with identifying relevant evidence within the privacy policy, as well as providing meta-annotation on the question's relevance, subjectivity, OPP-115 category, and how likely any privacy policy is to contain the answer to the question asked. {\r\r\n？」\n\nQuestion: What is the distribution of questions in the corpus across OPP-115 categories?\n\nAnswer: 노출등록\n\n     노출등록\n\n    First party and third party related questions are the largest categories, forming nearly 66.4% of all questions asked to the privacy assistant. {\r\r\n？」\n\nQuestion: What are the sources", "answers": ["Individuals with legal training", "Yes"], "all_classes": null, "length": 3846}
{"pred": "    The models used for painting embedding are a CNN-RNN generative model and a sequence-to-sequence model. The models used for language style transfer are a sequence-to-sequence model with global attention and a sequence-to-sequence model with pointer networks. {\r\r\n\n\n    The models used for painting embedding are a CNN-RNN generative model and a sequence-to-sequence model. The models used for language style transfer are a sequence-to-sequence model with global attention and a sequence-to-sequence model with pointer networks. {\r\r\n\n\n    The models used for painting embedding are a CNN-RNN generative model and a sequence-to-sequence model", "answers": ["generating a poem from images we use an existing actor-critic architecture, various types of sequence to sequence models"], "all_classes": null, "length": 1653}
{"pred": "    The RNN layer works better on top of BERT for long documents. The transformer layer works better for short documents. The RNN layer is able to capture long-term dependencies in the input sequence, while the transformer layer is limited to short-term dependencies. The RNN layer also has a lower computational complexity than the transformer layer, making it more suitable for long documents. However, the transformer layer is able to capture long-term dependencies in a more efficient way, making it more suitable for short documents. Therefore, the choice of the layer on top of BERT depends on the length of the input sequence. For long documents, the R", "answers": ["Transformer over BERT (ToBERT)", "The transformer layer"], "all_classes": null, "length": 2655}
{"pred": "    Yes, the authors hypothesize that humans' robustness to noise is due to their general knowledge. They believe that humans can utilize general knowledge in addition to the knowledge contained in each given passage-question pair, which allows them to achieve good performance on evaluation examples without training examples. The authors also believe that humans can utilize general knowledge to overcome the problems of hunger for data and robustness to noise in MRC models. Therefore, the authors propose a data enrichment method to extract inter-word semantic connections as general knowledge from each given passage-question pair, and an end-to-end MRC model named as Knowledge Aided Reader (KAR) that", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4127}
{"pred": "    The authors addressed three topics of cyberbullying: personal attack, racism, and sexism. They used three datasets to address these topics: Formspring, Twitter, and Wikipedia. The Formspring dataset contains examples of cyberbullying that are not specifically about any single topic. The Twitter dataset contains examples of racism and sexism. The Wikipedia dataset contains examples of personal attack. The authors used deep learning models to detect cyberbullying in these datasets. They also used transfer learning to improve the performance of their models. The authors found that the deep learning models coupled with transfer learning beat the best-known results for all three datasets. The authors concluded that", "answers": ["personal attack, racism, and sexism", "racism, sexism, personal attack, not specifically about any single topic"], "all_classes": null, "length": 3244}
{"pred": "    The new context representation is obtained by splitting the context into three disjoint regions based on the two relation arguments: the left context, the middle context and the right context. The middle context is then repeated in the new representation to force the network to pay special attention to it. The two contexts are processed by two independent convolutional and max-pooling layers, and the results are concatenated to form the sentence representation. This new representation is called the extended middle context. {\r\r\n？」\n\n    The new context representation is obtained by splitting the context into three disjoint regions based on the two relation arguments: the left context, the middle context and the right context", "answers": ["They use two independent convolutional and max-pooling layers on (1) a combination of the left context, the left entity and the middle context; and (2) a combination of the middle context, the right entity and the right context. They concatenated the two results after pooling to get the new context representation."], "all_classes": null, "length": 2435}
{"pred": "    4 (PER, LOC, ORG, MISC) {\r\r\n？」\n\n？」\n\n？」\n\n？」\n\n？」\n\n？」\n\n？」\n\n？」\n\n？」\n\n？」\n\n？」\n\n？」\n\n？」\n\n？」\n\n？」\n\n？」\n\n？」\n\n？」\n\n？」\n\n？」\n\n？」\n\n？」\n\n？」\n\n？」\n\n？」\n\n？」\n\n？」\n\n？」\n\n？」\n\n？」\n\n？」\n\n？」\n\n？」\n\n？」\n\n？」\n\n？」\n\n？」\n\n？」\n\n？」\n\n？」\n\n？」\n\n？」\n\n？」\n\n？」\n\n？」\n\n？」\n\n？」\n\n？」\n\n？」\n\n？」\n\n？」\n\n？」\n\n？」\n\n？」\n\n？」\n\n？」\n\n？」\n\n？」\n\n？」\n\n？」\n\n？」\n\n？」\n\n？」\n\n？」\n\n？」\n\n？」\n\n？」\n\n？」\n\n？」\n\n？」\n\n？」\n\n？」\n\n？」\n\n？」\n\n？」\n\n？」\n\n？」\n\n？」\n\n？」\n\n？」\n\n？」\n\n？」\n\n？」\n\n？」\n\n？」\n\n？」\n\n？」\n\n？」\n\n？」\n\n？」\n\n？」\n\n？」\n\n？」\n\n？」\n\n？」\n\n？」\n\n？」\n\n？」\n\n？」\n\n？」\n\n？」\n\n？」\n\n？」\n\n？」\n\n？」\n\n？」\n\n？」\n\n？」\n\n？」\n\n？」\n\n？」\n\n？」\n\n？」\n\n", "answers": ["OurNepali contains 3 different types of entities, ILPRL contains 4 different types of entities", "three"], "all_classes": null, "length": 2851}
{"pred": "    The resulting annotated data is of higher quality. The expert annotations are more consistent and accurate than the crowd annotations. The expert annotations also have higher precision and recall than the crowd annotations. The expert annotations are also more reliable and trustworthy than the crowd annotations. The expert annotations are also more comprehensive and complete than the crowd annotations. The expert annotations are also more consistent and reliable than the crowd annotations. The expert annotations are also more accurate and precise than the crowd annotations. The expert annotations are also more consistent and reliable than the crowd annotations. The expert annotations are also more accurate and precise than the crowd annotations. The expert annotations are also more consistent", "answers": ["improvement when the difficult subset with expert annotations is mixed with the remaining crowd annotation is 3.5 F1 score, much larger than when a random set of expert annotations are added"], "all_classes": null, "length": 4399}
{"pred": "        65% of the speakers are men, speaking more than 75% of the time. {\r\r\n        29.47% of the speakers within the Anchor category, even less than among the Punctual speakers. {\r\r\n        49.04% for the women and 38.56% for the men with a p-value smaller than 10^-6. {\r\r\n        61.29% for women and 46.51% for men with p-value smaller than 10^-14. {\r\r\n        24% for women compared to men. {\r\r\n        27.2% for Punctual speakers and 31", "answers": ["Women represent 33.16% of the speakers"], "all_classes": null, "length": 4055}
{"pred": "    The dataset is Multi30K. The approach achieves state of the art results on the English-German dataset. {\r\r\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n", "answers": ["the English-German dataset"], "all_classes": null, "length": 1833}
{"pred": "    The strong baselines model is compared to the models proposed by BIBREF20 and BIBREF18. The models proposed by BIBREF20 and BIBREF18 are both neural models that use unsupervised segmentation features to improve the performance of CWS. The models proposed by BIBREF20 and BIBREF18 are considered strong baselines because they achieve state-of-the-art performance on the SIGHAN Bakeoff 2005 dataset. The models proposed by BIBREF20 and BIBREF18 are also compared to the models proposed by BIBREF17 and BIBREF11, which are also neural", "answers": ["Baseline models are:\n- Chen et al., 2015a\n- Chen et al., 2015b\n- Liu et al., 2016\n- Cai and Zhao, 2016\n- Cai et al., 2017\n- Zhou et al., 2017\n- Ma et al., 2018\n- Wang et al., 2019"], "all_classes": null, "length": 3629}
{"pred": "    The classifiers used are logistic regression and multilayer perceptron.", "answers": ["probabilistic model", "Logistic Regression, Multilayer Perceptron"], "all_classes": null, "length": 4475}
{"pred": "    The toolkits used are NLTK, Stanford CoreNLP, and TwitterNLP. These toolkits have capabilities such as tokenization, part-of-speech tagging, chunking, named entity recognition, and sentiment analysis. The most common algorithms used for named entity recognition are Hidden Markov Models, Maximum Entropy Markov Models, and Conditional Random Fields. Recent works have used recurrent neural networks with attention modules for named entity recognition. The sentiment detection tools used are SentiStrength and TensiStrength, which are rule-based tools that rely on dictionaries of emoticons, slangs, idioms, and ironic phrases, and set of", "answers": ["BIBREF17, BIBREF18, TensiStrength BIBREF13, TwitterNLP BIBREF6, BIBREF19, CogComp-NLP BIBREF20, Stanford NLP NER BIBREF21", "BIBREF23, BIBREF17, BIBREF18, BIBREF19, BIBREF24, BIBREF25, BIBREF26"], "all_classes": null, "length": 1452}
{"pred": "    The experiments are performed on the SQuAD dataset.", "answers": ["SQuAD", "SQuAD"], "all_classes": null, "length": 3757}
{"pred": "    Existing approaches include using social media to conduct manual evaluations of image content with little automated exploitation of the associated tags, and using bag-of-words representations derived from Flickr tags to predict a range of different environmental phenomena. However, these approaches have not fully exploited the potential of Flickr tags for characterizing the natural environment. The main hypothesis in this paper is that vector space embeddings can be used to integrate textual information with structured information in a more effective way. The proposed model combines Flickr tags and structured scientific information to represent a set of locations. The model has three components: tag-based location embedding, structured environmental data, and categorical features. The tag", "answers": ["BOW-Tags, BOW-KL(Tags), BOW-All, GloVe"], "all_classes": null, "length": 4658}
{"pred": "    Yes, they use attention to compute the similarity score between passages and questions. The attention function is used to compute the similarity score between passages and questions as: A question-aware passage representation is computed as. After that, they use the method of self attention to the passage. Finally, the final memory is passed through a BiLSTM to form the final memory. The attention mechanism is used to focus on the most relevant parts of the passage and question, which helps the model to better understand the context and generate more accurate answers. The use of attention in the model is an important feature that helps the model to better understand the context and generate", "answers": ["Yes", "Yes"], "all_classes": null, "length": 1687}
{"pred": "    CSAT dataset for CSAT prediction, consisting of spoken transcripts (automatic via ASR). 20 newsgroups for topic identification task, consisting of written text; Fisher Phase 1 corpus for topic identification task, consisting of spoken transcripts (manual).", "answers": ["CSAT dataset, 20 newsgroups, Fisher Phase 1 corpus", "CSAT dataset , 20 newsgroups, Fisher Phase 1 corpus"], "all_classes": null, "length": 2652}
{"pred": "    IMDb movie review dataset. {\r\r\n        25,000 positive and 25,000 negative reviews, divided into equal-size train and test sets, with an average document length of 231 words. {\r\r\n        25,000 positive and 25,000 negative reviews, divided into equal-size train and test sets, with an average document length of 231 words. {\r\r\n        25,000 positive and 25,000 negative reviews, divided into equal-size train and test sets, with an average document length of 231 words. {\r\r\n        25,000 positive and 25,000 negative reviews, divided into equal-size train", "answers": ["the IMDb movie review dataset BIBREF17", "IMDb movie review"], "all_classes": null, "length": 3432}
{"pred": "        Yes, previous work has evaluated these tasks. The tasks were evaluated in the works of BIBREF1, BIBREF2, and BIBREF3. The evaluation protocol and stimuli were adapted to the bidirectional setting required by BERT, and the pre-trained BERT models (both the Large and the Base models) were evaluated. The out-of-the-box models performed very well on all the syntactic tasks. The results are not directly comparable to previous work due to differences in training data, bidirectionality, and evaluation data. However, the high performance numbers indicate that the purely attention-based BERT models are likely capable", "answers": ["Yes", "Yes"], "all_classes": null, "length": 1464}
{"pred": "    The dataset of 1,000 randomly selected tweets contains more than twice as many tweets about Trump than about the other candidates. In the named-entity recognition experiment, the average CCR of crowdworkers was 98.6%, while the CCR of the automated systems ranged from 77.2% to 96.7%. For four of the automated systems, detecting the entity Trump was more difficult than the other entities (e.g., spaCy 72.7% for the entity Trump vs. above 91% for the other entities). An example of incorrect NER is shown in Figure 1 top. The difficulties the", "answers": ["No"], "all_classes": null, "length": 1441}
{"pred": "    The invertibility condition is that the neural projector is invertible, meaning that the projection from the observed embedding space to the latent embedding space is invertible. This condition ensures that the latent embeddings can be recovered from the observed embeddings, and allows for tractable exact inference and marginal likelihood computation. The invertibility condition is satisfied by constraining the neural projector to be volume-preserving, meaning that the Jacobian determinant of the projection is equal to 1. This constraint ensures that the volume of the latent embedding space is preserved, and prevents information loss during the projection. The invertibility condition is crucial for the optimization of the model, as", "answers": ["The neural projector must be invertible.", "we constrain our neural projector with two requirements: (1) INLINEFORM0 and (2) INLINEFORM1 exists"], "all_classes": null, "length": 4323}
{"pred": "    The proposed qualitative annotation schema looks like a taxonomy of dimensions of interest, including factual correctness, required reasoning, knowledge, and linguistic complexity. The schema also includes a complexity metric based on lexical overlap between question and context sentences. The full catalogue of features, their description, detailed annotation guideline as well as illustrating examples can be found in Appendix. 1. Linguistic Features: Redundancy, Synonyms, Paraphrases, Abbreviations, Dative, Genitive, Nominalisation, Voice, Restrictivity, Factivity, Coreference, Listing, Prepositions, Coordination Scope, Relative clauses/Adverbial phrases", "answers": ["The resulting taxonomy of the framework is shown in Figure FIGREF10", "FIGREF10"], "all_classes": null, "length": 4958}
{"pred": "    The sizes of both datasets are as follows: \n\n    WikiSmall: 89,042 sentence pairs (training set) and 100 pairs (test set)\n\n    WikiLarge: 296,402 sentence pairs (training set) and 2,000/359 pairs (development/test set)\n\n    The sizes of the vocabulary are also as follows: \n\n    WikiSmall: 82K\n\n    WikiLarge: 82K\n\n    The sizes of the simplified corpora are also as follows: \n\n    WikiSmall: 600K sentences (11.6M words)\n\n    WikiLarge: 600K sentences (11.", "answers": ["training set has 89,042 sentence pairs, and the test set has 100 pairs, training set contains 296,402, 2,000 for development and 359 for testing", "WikiSmall  89 142 sentence pair and  WikiLarge 298 761 sentence pairs. "], "all_classes": null, "length": 2266}
{"pred": "    1. Vanilla ST baseline: The vanilla ST has only a speech encoder and a decoder. It is trained from scratch on the ST-TED corpus. 2. Pre-training baselines: We conduct three pre-training baseline experiments: 1) encoder pre-training, in which the ST encoder is initialized from an ASR model; 2) decoder pre-training, in which the ST decoder is initialized from an MT model; and 3) encoder-decoder pre-training, where both the encoder and decoder are pre-trained. 3. Multi-task baselines: We also conduct three multi-task baseline experiments including one-to-many setting", "answers": ["Vanilla ST baseline, encoder pre-training, in which the ST encoder is initialized from an ASR model, decoder pre-training, in which the ST decoder is initialized from an MT model, encoder-decoder pre-training, where both the encoder and decoder are pre-trained, many-to-many multi-task model where the encoders and decoders are derived from pre-trained ASR and MT models, Triangle+pre-train: BIBREF18 DBLP:conf/naacl/AnastasopoulosC18 proposed a triangle multi-task strategy for speech translation", "Vanilla ST baseline, Pre-training baselines, Multi-task baselines, Many-to-many+pre-training, Triangle+pre-train", "Vanilla ST baseline: The vanilla ST BIBREF9 has only a speech encoder and a decoder. It is trained from scratch on the ST-TED corpus.\n\nPre-training baselines: We conduct three pre-training baseline experiments: 1) encoder pre-training, in which the ST encoder is initialized from an ASR model; 2) decoder pre-training, in which the ST decoder is initialized from an MT model; and 3) encoder-decoder pre-training, where both the encoder and decoder are pre-trained. The ASR model has the same architecture with vanilla ST model, trained on the mixture of ST-TED and TED-LIUM2 corpus. The MT model has a text encoder and decoder with the same architecture of which in TCEN. It is first trained on WMT data (out-of-domain) and then fine-tuned on in-domain data.\n\nMulti-task baselines: We also conduct three multi-task baseline experiments including one-to-many setting, many-to-one setting, and many-to-many setting. In the first two settings, we train the model with $\\alpha _{st}=0.75$ while $\\alpha _{asr}=0.25$ or $\\alpha _{mt}=0.25$. For many-to-many setting, we use $\\alpha _{st}=0.6, \\alpha _{asr}=0.2$ and $\\alpha _{mt}=0.2$.. For MT task, we use only in-domain data.\n\nMany-to-many+pre-training: We train a many-to-many multi-task model where the encoders and decoders are derived from pre-trained ASR and MT models. "], "all_classes": null, "length": 4704}
{"pred": "    The paper studies English and German.", "answers": ["Unanswerable", "English"], "all_classes": null, "length": 4513}
{"pred": "    The models used in the experiment are: 1) Linear SVM trained on word unigrams, 2) Bidirectional Long Short-Term-Memory (BiLSTM) model, and 3) Convolutional Neural Network (CNN) model. 4) The CNN model is based on the architecture of BIBREF15. 5) The models are trained on the training data and evaluated by predicting the labels for the held-out test set. 6) The distribution of the training and test data is described in Table TABREF15. 7) The models are compared against baselines of predicting all labels as the", "answers": ["linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM trained on word unigrams,  bidirectional Long Short-Term-Memory (BiLSTM),  Convolutional Neural Network (CNN) "], "all_classes": null, "length": 2250}
{"pred": "        No, the answered questions measure for the usefulness of the question. The usefulness of the answer is measured by the number of upvotes it receives. The usefulness of the question is measured by the number of answers it receives. The usefulness of the answer is not directly related to the usefulness of the question. The usefulness of the answer is determined by the quality of the answer, which is determined by the number of upvotes it receives. The usefulness of the question is determined by the number of answers it receives, which is not directly related to the quality of the answers. The usefulness of the question is determined by the number of people who find", "answers": ["No"], "all_classes": null, "length": 1561}
{"pred": "    GloVe and Edinburgh embeddings. {\r\r\n？」\n\n    GloVe and Edinburgh embeddings. {\r\r\n？」\n\n    The official submission comprised of the top-performing model for each emotion category. This system ranked 3 for the entire test dataset and 2 for the subset of the test data formed by taking every instance with a gold emotion intensity score greater than or equal to 0.5. Post competition, experiments were performed on ensembling diverse models for improving the accuracy. An ensemble obtained by averaging the results of the top 2 performing models outperforms all the individual models.？」\n\n    The official submission comprised of the top-performing model for each emotion", "answers": ["Pretrained word embeddings  were not used", "GloVe, Edinburgh embeddings BIBREF14, Emoji embeddings BIBREF16"], "all_classes": null, "length": 1771}
{"pred": "    The results on the new dataset show that the personalized models outperform the baseline in BPE perplexity, and generate more diverse and acceptable recipes. The personalized models also perform better in user matching accuracy and mean reciprocal rank, indicating that they personalize generated recipes to the given user profiles. The Prior Name model achieves the best user matching accuracy and mean reciprocal rank, revealing that prior recipe names are strong signals for personalization. Moreover, the addition of attention mechanisms to capture these signals improves language modeling performance over a strong non-personalized baseline. The personalized models also achieve higher recipe-level coherence scores and better step entailment scores, indicating that they", "answers": ["average recipe-level coherence scores of 1.78-1.82, human evaluators preferred personalized model outputs to baseline 63% of the time"], "all_classes": null, "length": 2666}
{"pred": "    The combination of rewards for reinforcement learning is a combination of the irony reward and the sentiment reward. The irony reward is the difference between the irony score of the input sentence and that of the output sentence, while the sentiment reward is one minus the absolute value of the difference between the standardized sentiment score of the input sentence and that of the output sentence. The harmonic mean of the irony reward and the sentiment reward is used to encourage the model to focus on both the irony accuracy and the sentiment preservation.", "answers": ["irony accuracy, sentiment preservation", " irony accuracy and sentiment preservation"], "all_classes": null, "length": 4592}
{"pred": "    The authors demonstrate that their model has limitations in generating Shakespearean prose for paintings that do not have similar words in the training set of sentences. This happens when the style transfer dataset does not have similar words in the training set of sentences. A solution would be to expand the style transfer dataset, for a better representation of the poem data. {\r\r\n？」\n\n    The authors demonstrate that their model has limitations in generating Shakespearean prose for paintings that do not have similar words in the training set of sentences. This happens when the style transfer dataset does not have similar words in the training set of sentences. A solution would be to expand the style transfer dataset", "answers": ["Since we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer", "we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer as shown in Figure FIGREF12 for \"Starry Night\" with a low average content score"], "all_classes": null, "length": 1651}
{"pred": "    The existing benchmarks they compared to were the Affective Text dataset, the Fairy Tales dataset, and the ISEAR dataset. These datasets are commonly used for the evaluation of emotion detection systems. The Affective Text dataset contains news headlines annotated with emotions and valence, while the Fairy Tales dataset contains sentences from fairy tales annotated with emotions. The ISEAR dataset contains reports of situations in which people experienced emotions, and is the largest dataset out of the three. The authors mapped the emotions in these datasets to a subset of emotions that they used in their experiments: anger, joy, sadness, and surprise. They also provided an overview of", "answers": ["Affective Text, Fairy Tales, ISEAR", " Affective Text dataset, Fairy Tales dataset, ISEAR dataset"], "all_classes": null, "length": 3390}
{"pred": "    The distribution results showed that tweets containing fake news were created more recently, had fewer favourites, and used more hashtags than tweets not containing fake news. Additionally, accounts spreading fake news had a higher proportion of friends/followers and more URLs than accounts not spreading fake news. Finally, the content of viral fake news was highly polarized. {\r\r\n？」\n\n    The distribution results showed that tweets containing fake news were created more recently, had fewer favourites, and used more hashtags than tweets not containing fake news. Additionally, accounts spreading fake news had a higher proportion of friends/followers and more URLs than accounts not spreading fake news. Finally, the", "answers": ["Distributions of Followers, Friends and URLs are significantly different between the set of tweets containing fake news and those non containing them, but for Favourites, Mentions, Media, Retweets and Hashtags they are not significantly different"], "all_classes": null, "length": 3164}
{"pred": "    The dataset of hashtags is sourced from the Stanford Sentiment Analysis Dataset. The dataset consists of 1,108 unique English hashtags from 1,268 randomly selected tweets. The hashtags are manually segmented by crowdsourced workers. The dataset also includes additional corrections made by the authors. The dataset is available for download at https://github.com/StanfordNLP/stanford-sentiment-analysis-dataset. 0\n\n    Unanswerable 1\n\n    No 2\n\n    Unanswerable 3\n\n    Yes 4\n\n    No 5\n\n    Unanswerable 6\n\n    Yes 7\n\n    No ", "answers": ["1,268 randomly selected tweets in the Stanford Sentiment Analysis Dataset BIBREF36, all 12,594 unique English hashtags and their associated tweets from the same Stanford dataset", "Stanford Sentiment Analysis Dataset BIBREF36"], "all_classes": null, "length": 3756}
{"pred": "    The DeepMine database contains speech from a variety of accents, including Persian, English, and other languages. The database was collected using crowdsourcing, so the accents present in the corpus reflect the diversity of the respondents. The database is designed to be a standard benchmark for text-dependent speaker verification and speech recognition, so it is important to have a diverse set of accents to ensure that the results are generalizable. The database also includes phone-level transcriptions, which can be used to train deep neural network models for speech recognition in different languages. Overall, the DeepMine database is a valuable resource for researchers and developers working on speech-related tasks,", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3794}
{"pred": "    Word subspace can represent sets of word vectors as low-dimensional linear subspaces in a word vector space with high dimensionality. The word subspace is modeled by applying PCA without data centering to the set of word vectors of the class. The word subspace can effectively and compactly represent the context of the corresponding text. The word subspace of each class is modeled by applying PCA without data centering to the set of word vectors of the class. The word subspace can effectively and compactly represent the context of the corresponding text. The word subspace of each class is modeled by applying PCA without data centering to the set", "answers": ["Word vectors, usually in the context of others within the same class"], "all_classes": null, "length": 5151}
{"pred": "    The baseline model used is B1, which uses only the salience-based features by Dunietz and Gillick. Bf1 Bf1 Bf1 Bf1 Bf1 Bf1 Bf1 Bf1 Bf1 Bf1 Bf1 Bf1 Bf1 Bf1 Bf1 Bf1 Bf1 Bf1 Bf1 Bf1 Bf1 Bf1 Bf1 Bf1 Bf1 Bf1 Bf1 Bf1 Bf1 Bf1 Bf1 Bf1 Bf1 Bf1 Bf1 Bf1 Bf1 Bf1 Bf1 Bf1 Bf1 Bf1 Bf1 Bf1 Bf1 Bf1 Bf1 Bf1 Bf1 Bf1 Bf1 Bf", "answers": ["For Article-Entity placement, they consider two baselines: the first one using only salience-based features, and the second baseline checks if the entity appears in the title of the article. \n\nFor Article-Section Placement, they consider two baselines: the first picks the section with the highest lexical similarity to the article, and the second one picks the most frequent section.", "B1. The first baseline uses only the salience-based features by Dunietz and Gillick BIBREF11 ., B2. The second baseline assigns the value relevant to a pair INLINEFORM0 , if and only if INLINEFORM1 appears in the title of INLINEFORM2 .\n\n, S1: Pick the section from template INLINEFORM0 with the highest lexical similarity to INLINEFORM1 : S1 INLINEFORM2, S2: Place the news into the most frequent section in INLINEFORM0"], "all_classes": null, "length": 7891}
{"pred": "    Yes, SemCor3.0 is reflective of English language data in general. It is a large corpus of manually annotated English text with WordNet sense, and it is widely used as a training corpus for WSD. The data in SemCor3.0 is representative of the English language and can be used to train models that perform well on other English language datasets. Therefore, the results obtained on SemCor3.0 can be generalized to other English language datasets. Moreover, the results obtained on SemCor3.0 can be used to evaluate the performance of WSD models on other English language datasets. The results obtained on Sem", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 2000}
{"pred": "    38 hours of Spanish-English speech.  udál", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 2410}
{"pred": "    The dataset for fine-grained classification is split in training, development, development_test and test parts. In the rest, we refer to these splits as train, development and test, where train is composed by the training and the development instances. Table 7 presents an overview of the data. As discussed in BIBREF16 and illustrated in the Table, the fine-grained dataset is highly unbalanced and skewed towards the positive sentiment: only 20% of the training examples are labeled with one of the negative classes. 20% of the training examples are labeled with one of the negative classes. 20% of the training examples", "answers": [" high-quality datasets  from SemEval-2016 “Sentiment Analysis in Twitter” task", " SemEval-2016 “Sentiment Analysis in Twitter”"], "all_classes": null, "length": 2738}
{"pred": "    BERT$_\\mathrm {BASE}$ model for fine-tuning, because we find that BERT$_\\mathrm {LARGE}$ model performs slightly worse than BERT$_\\mathrm {BASE}$ in this task. (1) 2. The number of Transformer blocks is 12, the number of the hidden layer is 768, the number of self-attention heads is 12, and the total number of parameters of the pre-trained model is 110M. (2) 3. When fine-tuning, we use the development set (SE07) to find the optimal settings for our experiments. We keep", "answers": ["small BERT", "small BERT"], "all_classes": null, "length": 1999}
{"pred": "    Yes, the datasets are subject to quality control. The authors use crowd-sourcing experiments to validate samples of the data and find high agreement across probes. Human scores correlate with the model trends across the probe categories. The authors also note that the expert knowledge they employ is hand-curated by human experts, which is assumed to be generally correct, but is fallible and error-prone. The authors emphasize that using synthetic versus naturalistic QA data comes with important trade-offs, and that the positive results of their study should be taken with a grain of salt. The authors leave further work to develop better ways to evaluate semantic clusters and model robust", "answers": ["No", "No"], "all_classes": null, "length": 6391}
{"pred": "    Yes, the images are from a specific domain. The images are generated using the ShapeWorld framework, which consists of abstract colored shapes. The images are used to evaluate image captioning models. The ShapeWorld framework is designed to generate training and evaluation data for image captioning models. The images are used to evaluate the grammaticality, truthfulness, and diversity of the generated captions. The images are also used to evaluate the performance of the image captioning models on different types of tasks, such as existential descriptions, spatial descriptions, and quantification descriptions. The images are used to evaluate the performance of the image captioning models on different", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3472}
{"pred": "    The model achieved competitive results on standard benchmarks, with an average f-score of 0.368. The best model (B-M) achieved an f-score of 0.432 on the development set. The model was trained on a subset of Facebook pages and used a combination of features including Google embeddings, character n-grams, and negation words. The model did not rely on any handcrafted resources. The model's performance was compared to existing systems, and the results were competitive. The model's performance was also compared to the results of the best model on the development set, and the results were similar. The model's performance", "answers": ["Answer with content missing: (Table 3) Best author's model B-M average micro f-score is 0.409, 0.459, 0.411 on Affective, Fairy Tales and ISEAR datasets respectively. "], "all_classes": null, "length": 3410}
{"pred": "    The tagging scheme employed is {INLINEFORM0, INLINEFORM1, INLINEFORM2, INLINEFORM3, INLINEFORM4, INLINEFORM5, INLINEFORM6, INLINEFORM7, INLINEFORM8}. {INLINEFORM0} tag indicates that the current word appears before the pun in the given context. {INLINE？」\n\n    {INLINEFORM1} tag highlights the current word is a pun. {INLINE？」\n\n    {INLINEFORM2} tag indicates that the current word appears after the pun. {INLINE？」\n\n    {INLINEFORM3} tag indicates that the current word is not a pun. {INLINE？」\n\n    {INLINEFORM", "answers": ["A new tagging scheme that tags the words before and after the pun as well as the pun words.", "a new tagging scheme consisting of three tags, namely { INLINEFORM0 }"], "all_classes": null, "length": 2974}
{"pred": "    Yes, Arabic is one of the 11 languages in CoVost. The other languages are French, German, Dutch, Russian, Spanish, Italian, Turkish, Persian, Swedish, Mongolian, and Chinese. The corpus is designed to be multilingual, with translations from each of these languages into English. The corpus is also diversified, with over 11,000 speakers and over 60 accents. The corpus is free to use with a CC0 license, and the additional Tatoeba evaluation samples are also CC-licensed. The corpus is designed to be used for research and development in the field of speech-to-text translation. The", "answers": ["No", "No"], "all_classes": null, "length": 2413}
{"pred": "    Robustness of a model is defined as its ability to perform well in the presence of noise or uncertainty in the input data. A robust model is one that can handle unexpected inputs and produce accurate outputs. The robustness of a model is important in real-world applications where the input data may be noisy or uncertain. A robust model can handle such uncertainty and produce accurate outputs. The robustness of a model is also important in the context of machine learning, where the model may be exposed to new data that it has not seen before. A robust model can handle such new data and produce accurate outputs. The robustness of a model is", "answers": ["ability to accurately classify texts even when the amount of prior knowledge for different classes is unbalanced, and when the class distribution of the dataset is unbalanced", "Low sensitivity to bias in prior knowledge"], "all_classes": null, "length": 3609}
{"pred": "    InferSent and Universal Sentence Encoder.", "answers": ["GloVe, BERT, Universal Sentence Encoder, TF-IDF, InferSent", "Avg. GloVe embeddings, Avg. fast-text embeddings, Avg. BERT embeddings, BERT CLS-vector, InferSent - GloVe and Universal Sentence Encoder."], "all_classes": null, "length": 3862}
{"pred": "        +0.29 and +0.96 for English datasets, +0.97 and +2.36 for Chinese datasets. {\r\r\n        +0.29 and +0.96 for English datasets, +0.97 and +2.36 for Chinese datasets. {\r\r\n        +0.29 and +0.96 for English datasets, +0.97 and +2.36 for Chinese datasets.・━・━        +0.29 and +0.96 for English datasets, +0.97 and +2.36 for Chinese datasets.・━・━        +0.29 and +0.96 for English datasets, +0", "answers": ["English datasets including CoNLL2003 and OntoNotes5.0, our proposed method outperforms BERT-MRCBIBREF38 by +0.29 and +0.96 respectively, Chinese datasets, achieving F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively", "For English datasets including CoNLL2003 and OntoNotes5.0, our proposed method outperforms BERT-MRCBIBREF38 by +0.29 and +0.96 respectively., huge performance boosts on Chinese datasets, achieving F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively"], "all_classes": null, "length": 3586}
{"pred": "    They test their conflict method on two tasks: Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask. {\r\r\n        […]\n\n        […]        […]\n\n        On which tasks do they test their conflict method?        […]\n\n        Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask.        […]\n\n        […]        […]\n\n        How does their conflict method work?        […]\n\n        Their conflict method works by computing how much two sequences repel each other. It uses element wise difference between two vectors followed by a linear transformation to produce a scalar weight. The remaining of the process acts similar to how attention works.        […]\n\n", "answers": ["Task 1: Quora Duplicate Question Pair Detection, Task 2: Ranking questions", "Quora Duplicate Question Pair Detection, Ranking questions in Bing's People Also Ask"], "all_classes": null, "length": 2577}
{"pred": "    They compared against previous syntactic tree-based models as well as other neural models.", "answers": ["Various tree structured neural networks including variants of Tree-LSTM, Tree-based CNN, RNTN, and non-tree models including variants of LSTMs, CNNs, residual, and self-attention based networks", "Sentence classification baselines: RNTN (Socher et al. 2013), AdaMC-RNTN (Dong et al. 2014), TE-RNTN (Qian et al. 2015), TBCNN (Mou et al. 2015), Tree-LSTM (Tai, Socher, and Manning 2015), AdaHT-LSTM-CM (Liu, Qiu, and Huang 2017), DC-TreeLSTM (Liu, Qiu, and Huang 2017), TE-LSTM (Huang, Qian, and Zhu 2017), BiConTree (Teng and Zhang 2017), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), TreeNet (Cheng et al. 2018), CNN (Kim 2014), AdaSent (Zhao, Lu, and Poupart 2015), LSTM-CNN (Zhou et al. 2016), byte-mLSTM (Radford, Jozefowicz, and Sutskever 2017), BCN + Char + CoVe (McCann et al. 2017), BCN + Char + ELMo (Peters et al. 2018). \nStanford Natural Language Inference baselines: Latent Syntax Tree-LSTM (Yogatama et al. 2017), Tree-based CNN (Mou et al. 2016), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), NSE (Munkhdalai and Yu 2017), Reinforced Self- Attention Network (Shen et al. 2018), Residual stacked encoders: (Nie and Bansal 2017), BiLSTM with generalized pooling (Chen, Ling, and Zhu 2018)."], "all_classes": null, "length": 4781}
{"pred": "    The core component for KBQA is relation detection. Relation detection is the process of identifying the relations between entities in a knowledge base (KB) that are relevant to a given question. The relation detection component is responsible for generating the KB query that is used to retrieve the answers from the KB. The quality of the relation detection component directly impacts the accuracy of the KBQA system. Therefore, improving the relation detection component is crucial for improving the overall performance of the KBQA system. The proposed hierarchical matching with residual learning approach for relation detection is designed to address the challenges of KB relation detection and improve the accuracy of the KBQA system. The", "answers": ["answer questions by obtaining information from KB tuples ", "hierarchical matching between questions and relations with residual learning"], "all_classes": null, "length": 4527}
{"pred": "    The baseline models are a name-based Nearest-Neighbor model (NN) and an Encoder-Decoder baseline with ingredient attention (Enc-Dec). {\r\r\n）\n\nQuestion: How do the personalized models perform compared to the baseline models?\n\nAnswer: 노출등록\n\n     노출등록\n\n    The personalized models outperform the baseline models in BPE perplexity, Distinct-1/2, and recipe-level coherence. They also perform better in terms of personalization metrics like UMA and MRR. {\r\r\n）\n\nQuestion: How do the personalized models perform in terms of recipe-level coherence?\n\nAnswer: 노출등록\n\n     노출등록\n\n    The personalized models achieve average recipe-level", "answers": ["name-based Nearest-Neighbor model (NN), Encoder-Decoder baseline with ingredient attention (Enc-Dec)"], "all_classes": null, "length": 2655}
{"pred": "    1. Browser-based annotation tool 2. Part-of-speech information 3. Flickr30K Entities 4. Louvain clustering 5. Coreference graph 6. Clustering of referring expressions 7. Manual inspection of the data 8. Statistical analysis of the data 9. Analysis of the structure of the data 10. Analysis of the distribution of terms used to describe entities 11. Analysis of the use of adjectives 12. Analysis of the use of negations 13. Analysis of the use of stereotypes 14. Analysis of the use of unwarranted inferences 15.", "answers": ["spot patterns by just looking at a collection of images, tag all descriptions with part-of-speech information, I applied Louvain clustering", "Looking for adjectives marking the noun \"baby\" and also looking for most-common adjectives related to certain nouns using POS-tagging"], "all_classes": null, "length": 2204}
{"pred": "        Romance languages and Semitic languages. {\r\r\n        The Romance languages include French, Spanish, Italian, and Portuguese, while the Semitic languages include Arabic and Hebrew. {\r\r\n        In these languages, the third person plural pronoun \"they\" has a gender distinction, with masculine and feminine forms. To correctly translate a sentence containing \"they\" into one of these languages, it is necessary to determine whether the referent is a group of females. If it is, then the translation must use the feminine pronoun; otherwise, it must use the masculine pronoun. {\r\r\n        The Winograd Schema Challenge is a challenge for AI programs to identify", "answers": ["English, French, German ", "French, English, Spanish, Italian, Portuguese, Hebrew, Arabic"], "all_classes": null, "length": 2285}
{"pred": "    They experimented with stacked LSTMs, Cell-aware Stacked LSTMs (CAS-LSTMs), Tree-LSTMs, Multidimensional LSTMs (MDLSTMs), and Grid LSTMs.", "answers": ["Stacked LSTMs, Cell-aware Stacked LSTMs, Sentence Encoders, Top-layer Classifiers"], "all_classes": null, "length": 3210}
{"pred": "        Yes, they report results only on English data.", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 6169}
{"pred": "    The authors experimented with a few summarization algorithms provided by the Sumy package, including TextRank, LexRank, and Luhn. They also implemented their own Integer Linear Programming (ILP)-based summarization algorithm. The ILP-based algorithm selects a subset of important text fragments or phrases from the peer feedback comments and generates a summary based on these phrases. The authors compared the performance of their ILP-based algorithm with the other algorithms using the ROUGE unigram score. They found that the performance of their algorithm was comparable with the other algorithms, as the two-sample t-test did not show statistically significant difference. The authors also noted", "answers": ["LSA, TextRank, LexRank and ILP-based summary.", "LSA, TextRank, LexRank"], "all_classes": null, "length": 3045}
{"pred": "    The previous state of the art for this task was a probabilistic graphical model that used latent post categories to model the thread sequence and infer states that triggered intervention. However, this model required a hyperparameter for the number of latent states, which may not generalize well. (BIBREF0) (BIBREF7) (BIBREF1) (BIBREF8) (BIBREF2) (BIBREF3) (BIBREF4) (BIBREF5) (BIBREF6) (BIBREF7) (BIBREF9) (BIBREF10) (B", "answers": ["hLSTM", "hLSTM"], "all_classes": null, "length": 3725}
{"pred": "    The master node skip connection. The master node vector bypasses the attention mechanism, which is equivalent to a skip or shortcut connection. This is done to directly inject global information about the document into its final representation. The master node is expected to learn a high-level summary about the document, such as its size, vocabulary, etc. Therefore, by making the master node bypass the attention layer, we directly inject global information about the document into its final representation. The other components, such as the message passing iterations, the renormalization trick, and the GRU-based updates, are all important for the overall performance of the model. However", "answers": ["Based on table results provided changing directed to undirected edges had least impact - max abs difference of 0.33 points on all three datasets."], "all_classes": null, "length": 4180}
{"pred": "    The corpus used for the task is DTA18 and DTA19, which are subparts of the DTA corpus. These corpora consist of texts from the 16th to the 20th century, with DTA18 containing texts published between 1750-1799 and DTA19 containing texts published between 1850-1899. The corpus version used in the task has the following format: \"year [tab] lemma1 lemma2 lemma3...\". {\r\r\n？」\n\n    The corpus used for the task is DTA18 and DTA19, which are subparts of the DTA corpus. These corpor", "answers": ["DTA18, DTA19", "Diachronic Usage Relatedness (DURel) gold standard data set"], "all_classes": null, "length": 1908}
{"pred": "        Kannada, Hindi, Telugu, Malayalam, Bengali, and English. {\r\r\n        Kannada, Hindi, Telugu, Malayalam, Bengali, and English.・━・━        Kannada, Hindi, Telugu, Malayalam, Bengali, and English.・━・━        Kannada, Hindi, Telugu, Malayalam, Bengali, and English.・━・━        Kannada, Hindi, Telugu, Malayalam, Bengali, and English.・━・━        Kannada, Hindi, Telugu, Malayalam, Bengali, and English.・━・━        Kannada, Hindi, Telugu, Malayalam, Bengali", "answers": ["Hindi, English, Kannada, Telugu, Assamese, Bengali and Malayalam", "Kannada, Hindi, Telugu, Malayalam, Bengali,  English and Assamese (in table, missing in text)"], "all_classes": null, "length": 2453}
{"pred": "    The model performance on target language reading comprehension is 53.8 F1 score when training on Chinese and testing on English, and 44.1 F1 score when training on Chinese and testing on English translated into Chinese. The performance drops when training on English translated into Chinese and testing on English. The performance improves when training on English translated into Chinese and testing on Korean. The performance improves when training on English translated into Chinese, Japanese, and Korean and testing on Korean. The performance improves when training on English translated into Chinese, Japanese, and Korean and testing on English. The performance improves when training on English translated into Chinese, Japanese", "answers": ["Table TABREF6, Table TABREF8", "when testing on English, the F1 score of the model training on Chinese (Zh) is 53.8,  F1 score is only 44.1 for the model training on Zh-En"], "all_classes": null, "length": 2492}
{"pred": "        The proposed model outperforms the baselines by a significant margin. The difference in performance is substantial, indicating that the proposed model is effective in recovering the language styles of specific characters. The proposed model is able to retrieve the correct response of five diverse characters with different identities, demonstrating its robustness and stability. The proposed model is able to recover the dialogue styles of fictional characters regardless of the character's profile and identity, genre of the show, and context of the dialogue. The proposed model is able to recommend tailored responses traceable to specific characters, showing promise for learning character or personality styles. The proposed model is able to effectively recover", "answers": ["Metric difference between Aloha and best baseline score:\nHits@1/20: +0.061 (0.3642 vs 0.3032)\nMRR: +0.0572(0.5114 vs 0.4542)\nF1: -0.0484 (0.3901 vs 0.4385)\nBLEU: +0.0474 (0.2867 vs 0.2393)"], "all_classes": null, "length": 5151}
{"pred": "    ARAML performs better than other baselines in all the cases. It has significantly better performance in both grammaticality and relevance. The improvement is significant. {\r\r\n？」\n\n    Yes, ARAML performs significantly better than other baselines in all the cases.？」\n\n    No, ARAML performs worse than other baselines in some cases.？」\n\n    Unanswerable.？」\n\n    Yes, ARAML performs significantly better than other baselines in all the cases.？」\n\n    No, ARAML performs worse than other baselines in some cases.？」\n\n    Unanswerable.？」\n\n    Yes, ARAML performs significantly better than other baselines in all", "answers": ["ARAM has achieved improvement over all baseline methods using reverese perplexity and  slef-BLEU metric.  The maximum reverse perplexity improvement 936,16 is gained for EMNLP2017  WMT  dataset and  48,44 for COCO dataset.", "Compared to the baselines, ARAML does not do better in terms of perplexity on COCO and EMNLP 2017 WMT datasets, but it does by up to 0.27 Self-BLEU points on COCO and 0.35 Self-BLEU on EMNLP 2017 WMT. In terms of Grammaticality and Relevance, it scores better than the baselines on up to 75.5% and 73% of the cases respectively."], "all_classes": null, "length": 3796}
{"pred": "        The authors present evidence that the model can capture some biases in data annotation and collection by examining the results of the model's error analysis. They find that many errors are due to biases from data collection and rules of annotation, and not the classifier itself. They also find that the model can differentiate hate and offensive samples accurately by leveraging knowledge-aware language understanding that it has. These findings suggest that the model can detect some biases in data annotation and collection, and may be able to alleviate bias in hate speech datasets in future studies. {\r\r\n        Unanswerable. {\r\r\n        Yes. {\r\r\n        No. {\r\r\n        Unanswerable. {\r\r\n        No", "answers": ["The authors showed few tweets where neither and implicit hatred content exist but the model was able to discriminate"], "all_classes": null, "length": 4119}
{"pred": "    Yes, three baselines were tested: SVM, CNN, and BERT. The SVM baseline used bag-of-words features, length of the question, and part-of-speech tags. The CNN baseline used GloVe word embeddings and a filter size of 5 with 64 filters. The BERT baseline was fine-tuned on the binary answerability identification task. The best-performing baseline was BERT + Unanswerable, which achieved an F1 of 39.8. The No-answer baseline performed at 28 F1, providing a lower bound on performance. The human performance baseline was 50.5 F1", "answers": ["SVM, No-Answer Baseline (NA) , Word Count Baseline, Human Performance", "No-Answer Baseline (NA), Word Count Baseline, Human Performance"], "all_classes": null, "length": 3855}
{"pred": "    6946 sentences and 16225 unique words. {\r\r\n？」\n\n    6946 sentences and 16225 unique words.？」\n\n    6946 sentences and 16225 unique words.？」\n\n    6946 sentences and 16225 unique words.？」\n\n    6946 sentences and 16225 unique words.？」\n\n    6946 sentences and 16225 unique words.？」\n\n    6946 sentences and 16225 unique words.？」\n\n    6946 sentences and 16225 unique words.？」\n\n    6946 sentences and 16225 unique words.？」\n\n    6946 sentences and 16225 unique", "answers": ["Dataset contains 3606 total sentences and 79087 total entities.", "ILPRL contains 548 sentences, OurNepali contains 3606 sentences"], "all_classes": null, "length": 2843}
{"pred": "    The proposed method improves F1 for paraphrase identification by +0.58 for MRPC and +0.73 for QQP. {\r\r\n？」\n\n    The proposed method improves F1 for paraphrase identification by +0.58 for MRPC and +0.73 for QQP.？」\n\n    Yes？」\n\n    The proposed method improves F1 for paraphrase identification by +0.58 for MRPC and +0.73 for QQP.？」\n\n    No？」\n\n    The proposed method improves F1 for paraphrase identification by +0.58 for MRPC and +0.73 for QQP.？」\n\n    Unanswerable？」\n\n", "answers": ["Using DSC loss improves the F1 score by +0.58 for MRPC and +0.73 for QQP", "+0.58"], "all_classes": null, "length": 3566}
{"pred": "        The datasets used are EEG data from BIBREF0 and eye-tracking and self-paced reading time data from BIBREF10.�单词\n\n        The datasets used are EEG data from BIBREF0 and eye-tracking and self-paced reading time data from BIBREF10.单词\n\n        The datasets used are EEG data from BIBREF0 and eye-tracking and self-paced reading time data from BIBREF10.单词\n\n        The datasets used are EEG data from BIBREF0 and eye-tracking and self-paced reading time data from BIBREF10.单词\n\n        The datasets used are EEG data from", "answers": ["Answer with content missing: (Whole Method and Results sections) The primary dataset we use is the ERP data collected and computed by Frank et al. (2015), and we also use behavioral data (eye-tracking data and self-paced reading times) from Frank et al. (2013) which were collected on the same set of 205 sentences.\nSelect:\n- ERP data collected and computed by Frank et al. (2015)\n- behavioral data (eye-tracking data and self-paced reading times) from Frank et al. (2013)", "the ERP data: BIBREF0"], "all_classes": null, "length": 1971}
{"pred": "        The subjects were presented with a series of images depicting different objects, such as a car, a house, and a tree. The images were presented in a random order, and the subjects were instructed to imagine the object in the image as they viewed it. The subjects were also instructed to imagine the object in the image as they viewed it. The subjects were also instructed to imagine the object in the image as they viewed it. The subjects were also instructed to imagine the object in the image as they viewed it. The subjects were also instructed to imagine the object in the image as they viewed it. The subjects were also instructed to imagine the", "answers": ["7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw)", "KARA ONE BIBREF17 , composed of multimodal data for stimulus-based, imagined and articulated speech state corresponding to 7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw)"], "all_classes": null, "length": 2379}
{"pred": "    Pointer-Gen, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN, Pointer-Gen+ARL-SEN, Test set. {\r\r\n‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍", "answers": ["Pointer-Gen, Pointer-Gen+Pos, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN"], "all_classes": null, "length": 4085}
{"pred": "    The learning models used on the dataset are traditional machine learning classifiers and neural network based models. Traditional machine learning classifiers include Naïve Bayes, Logistic Regression, Support Vector Machine, Random Forests, and Gradient Boosted Trees. Neural network based models include Convolutional Neural Networks, Recurrent Neural Networks, and their variants. The neural network based models use pre-trained GloVe word representations and apply attention mechanisms to improve performance. Additionally, the models use context tweets to provide additional information for classification. The traditional machine learning classifiers are implemented with Bag Of Words (BOW) representations and Term Frequency-Inverse Document Frequency (TF-IDF", "answers": ["Naïve Bayes (NB), Logistic Regression (LR), Support Vector Machine (SVM), Random Forests (RF), Gradient Boosted Trees (GBT),  Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN)", "Naïve Bayes (NB), Logistic Regression (LR), Support Vector Machine (SVM), Random Forests (RF), Gradient Boosted Trees (GBT), CNN, RNN"], "all_classes": null, "length": 2074}
{"pred": "    The language model architectures used are a bi-directional language model to augment the sequence to sequence encoder and a uni-directional model to augment the decoder. Both use self-attention and the uni-directional model contains 6 transformer blocks, followed by a word classifier to predict the next word on the right. The bi-directional model solves a cloze-style token prediction task at training time. The model consists of two towers, the forward tower operates left-to-right and the tower operating right-to-left as backward tower; each tower contains 6 transformer blocks. The forward and backward representations are combined via a self-attention module and the output", "answers": ["uni-directional model to augment the decoder", "bi-directional language model to augment the sequence to sequence encoder ,  uni-directional model to augment the decoder"], "all_classes": null, "length": 1914}
{"pred": "    The weights are dynamically adjusted by multiplying the soft probability p with a decaying factor (1-p). This decaying factor pushes down the weight of easy examples as training proceeds. The intuition is to push down the weight of easy examples and make the model attend less to them. (1-p) p is thought as a weight associated with each example, which changes as training proceeds. (1-p) p is a weight that changes as training proceeds and pushes down the weight of easy examples. (1-p) p is a weight that changes as training proceeds and pushes down the weight of easy examples. (1-p) p is a", "answers": ["One can think $(1-p_{i1})$ as a weight associated with each example, which changes as training proceeds. The intuition of changing $p_{i1}$ to $(1-p_{i1}) p_{i1}$ is to push down the weight of easy examples. For easy examples whose probability are approaching 0 or 1, $(1-p_{i1}) p_{i1}$ makes the model attach significantly less focus to them. Figure FIGREF23 gives gives an explanation from the perspective in derivative: the derivative of $\\frac{(1-p)p}{1+(1-p)p}$ with respect to $p$ approaches 0 immediately after $p$ approaches 0, which means the model attends less to examples once they are correctly classified.", "associates each training example with a weight in proportion to $(1-p)$, and this weight dynamically changes as training proceeds"], "all_classes": null, "length": 3640}
{"pred": "        The results show that agents utilizing knowledge-graphs in addition to either enhanced exploration method far outperform the baseline A2C and KG-A2C. KG-A2C-chained and KG-A2C-Explore both pass the bottleneck of a score of 40, whereas A2C-Explore gets to the bottleneck but cannot surpass it. There are a couple of key insights that can be drawn from these results. The first is that the knowledge graph appears to be critical; it is theorized to help with partial observability. However the knowledge graph representation isn't sufficient in that the knowledge graph representation without enhanced exploration methods cannot surpass", "answers": ["Reward of 11.8 for the A2C-chained model, 41.8 for the KG-A2C-chained model, 40 for A2C-Explore and 44 for KG-A2C-Explore.", "KG-A2C-chained and KG-A2C-Explore both pass the bottleneck of a score of 40"], "all_classes": null, "length": 2443}
{"pred": "    An individual model consists of a Bayesian model for each language. The models are coupled by crosslingual latent variables that capture role alignments in parallel corpora. The crosslingual latent variables regularize the parameter estimates of the monolingual models. {\r\r\n});\n\nQuestion: What is the generative process of the multilingual model?\n\nAnswer: 노출등록\n\n    The generative process of the multilingual model is as follows:\n\n    1. Generate the predicate-tuple and its voice.\n    2. Generate the syntactic features of all the identified arguments.\n    3. Generate the role labels for each language using the monolingual", "answers": ["Bayesian model of garg2012unsupervised as our base monolingual model"], "all_classes": null, "length": 3701}
{"pred": "    The non-standard pronunciation is identified by the annotations of the Spanish words interspersed in Mapudungun speech. The annotations label foreign words, in this case Spanish words, as such. The annotations also label non-verbal articulations and pauses. The non-standard pronunciation is identified by the annotations of the Spanish words interspersed in Mapudungun speech. The annotations label foreign words, in this case Spanish words, as such. The annotations also label non-verbal articulations and pauses. The non-standard pronunciation is identified by the annotations of the Spanish words interspersed in Mapudungun speech. The annotations label foreign", "answers": ["Unanswerable", "Original transcription was labeled with additional labels in [] brackets with nonstandard pronunciation."], "all_classes": null, "length": 3018}
{"pred": "    A semicharacter architecture is a type of neural network that processes a sentence of words with misspelled characters, predicting the correct words at each step. The architecture treats the first and the last characters individually, and is agnostic to the ordering of the internal characters. The training target is the correct corresponding word, and the model is optimized with cross-entropy loss. The architecture is inspired by psycholinguistic studies that demonstrate that humans can comprehend text altered by jumbling internal characters, provided that the first and last characters of each word remain unperturbed. The architecture is used to build robust word recognition models that can handle", "answers": ["A semi-character based RNN (ScRNN) treats the first and last characters individually, and is agnostic to the ordering of the internal characters", "processes a sentence of words with misspelled characters, predicting the correct words at each step"], "all_classes": null, "length": 4186}
{"pred": "        The languages explored are Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish, and Swedish.        […]\n\n        The languages explored are Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish, and Swedish.        […]        […]\n\n        The languages explored are Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish, and Swedish.        […]        […]\n\n       ", "answers": ["Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish", "Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish , Swedish"], "all_classes": null, "length": 2697}
{"pred": "    The NCEL approach is effective overall, achieving the best performance in most cases with an average gain of 2% on Micro F1 and 3% Macro F1. The baseline methods also achieve competitive results on some datasets but fail to adapt to the others. The proposed method performs consistently well on all datasets, demonstrating its good generalization ability. The impact of key modules is also investigated, showing that the prior probability performs well in the \"easy\" case but poorly in the \"hard\" case, and the global module is more helpful in the \"hard\" case. The attention and embedding features are effective in improving the robustness", "answers": ["NCEL consistently outperforms various baselines with a favorable generalization ability"], "all_classes": null, "length": 4113}
{"pred": "    Yes, the data is de-identified with digital zeros and [de-identified] tags. The sentences in the transcript are grounded to the audio with the timestamps of its first and last word. The transcript for a typical conversation can be quite long, and not easy for many of the high performing deep learning models to act on. Moreover, the medical information about a concept/condition/entity can change during the conversation after a significant time gap. Hence, the summaries and MR tags are also local and are grounded using the timestamps of the sentences from the transcript deemed relevant by the expert annotators. The summaries are medically relevant and local. The", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4592}
{"pred": "    The baseline used was the error detection system by Rei2016, trained using the same FCE dataset. {\r\r\n        […]\n\n    The baseline used was the error detection system by Rei2016, trained using the same FCE dataset. {\r\r\n        […]\n\n    The baseline used was the error detection system by Rei2016, trained using the same FCE dataset. {\r\r\n        […]\n\n    The baseline used was the error detection system by Rei2016, trained using the same FCE dataset. {\r\r\n        […]\n\n    The baseline used was the error detection system by Rei2016, trained using the same FCE dataset. {\r\r\n        […]\n\n    The baseline", "answers": ["error detection system by Rei2016", "error detection system by Rei2016"], "all_classes": null, "length": 2132}
{"pred": "    The annotated clinical notes were obtained from the 2010 i2b2/VA BIBREF0 and 2013 ShARe/CLEF Task 1 BIBREF1 challenges. These challenges ask participants to design an algorithm to tag a set of predefined entities of interest in clinical notes. These entity tagging tasks are also known as clinical Named Entity Recognition (NER). For example, the CE task in 2010 i2b2/VA defines three types of entities: “problem”, “treatment”, and “test”. The CE task in 2013 ShARe/CLEF defines various types of disorder", "answers": ["clinical notes from the CE task in 2010 i2b2/VA", "clinical notes from the CE task in 2010 i2b2/VA "], "all_classes": null, "length": 3432}
{"pred": "    Masking words in the decoder helps the model learn to generate more fluent and natural sequences by concentrating on one word at a time, based on the source document as well as other words. This process is similar to the cloze task in BERT's pre-train process, and by using the ability of the contextual language model the decoder can generate more fluent and natural sequences. The refine decoder is designed to enhance the decoder using BERT's contextualized representations, and the parameters are shared between the draft decoder and refine decoder to help the model learn to generate refined summaries. The discrete objective and reinforcement learning objective are introduced to balance between optimizing", "answers": ["ecause this process is similar to the cloze task in BERT's pre-train process, therefore by using the ability of the contextual language model the decoder can generate more fluent and natural sequences."], "all_classes": null, "length": 3919}
{"pred": "    The dataset used is the Twitter dataset.ETwitter dataset.ETwitter dataset.ETwitter dataset.ETwitter dataset.ETwitter dataset.ETwitter dataset.ETwitter dataset.ETwitter dataset.ETwitter dataset.ETwitter dataset.ETwitter dataset.ETwitter dataset.ETwitter dataset.ETwitter dataset.ETwitter dataset.ETwitter dataset.ETwitter dataset.ETwitter dataset.ETwitter dataset.ETwitter dataset.ETwitter dataset.ETwitter dataset.ETwitter dataset.ETwitter dataset.ETwitter dataset.ETwitter dataset.ETwitter dataset.ETwitter dataset.ETwitter dataset.ETwitter dataset.ETwitter dataset.ETwitter dataset.ETwitter dataset.ETwitter dataset.ETwitter dataset.ETwitter dataset.ETwitter dataset.ETwitter dataset.ETwitter dataset.ETwitter dataset", "answers": ["Unanswerable", " Paraphrase Database (PPDB) ,  book corpus", "Unanswerable"], "all_classes": null, "length": 1902}
{"pred": "    The features used are TF-IDF weights. These weights are calculated based on the frequency of a term in a document and the inverse document frequency. The higher the TF-IDF weight, the more important the term is considered to be in the document. The features are extracted from the pre-processed reports. The reports are pre-processed by setting their text content to lowercase, removing non-alphanumeric characters, removing stopping words, joining bigrams with a hyphen, and removing words that occur less than 2% or more than 90% in each of the diagnostic categories. The features are stored in a separate text file for each", "answers": ["Unanswerable"], "all_classes": null, "length": 2108}
{"pred": "    The dataset is annotated with 9,473 annotations for 9,300 tweets. Each tweet is annotated as no evidence of depression (e.g., “Citizens fear an economic depression\") or evidence of depression (e.g., “depressed over disappointment\"). If a tweet is annotated evidence of depression, then it is further annotated with one or more depressive symptoms, for example, depressed mood (e.g., “feeling down in the dumps\"), disturbed sleep (e.g., “another restless night\"), or fatigue or loss of energy (e.g., “the fatigue is unbearable\") BIBREF10. For each class, every", "answers": ["no evidence of depression, depressed mood, disturbed sleep, fatigue or loss of energy", "The annotations are based on evidence of depression and further annotated by the depressive symptom if there is evidence of depression"], "all_classes": null, "length": 1947}
{"pred": "    The eight NER tasks they evaluated on were BC4CHEMD, BC5CDR, CHEMDNER, CHEBI, CL, DDI, MIMIC-III, and SIDER. 1. BC4CHEMD: This task involves identifying chemical entities in biomedical text. 2. BC5CDR: This task involves identifying chemical and disease entities in biomedical text. 3. CHEMDNER: This task involves identifying chemical and drug entities in biomedical text. 4. CHEBI: This task involves identifying chemical entities in biomedical text. 5. CL: This task involves identifying clinical entities in biomedical", "answers": ["BC5CDR-disease, NCBI-disease, BC5CDR-chem, BC4CHEMD, BC2GM, JNLPBA, LINNAEUS, Species-800", "BC5CDR-disease, NCBI-disease, BC5CDR-chem, BC4CHEMD, BC2GM, JNLPBA, LINNAEUS, Species-800"], "all_classes": null, "length": 2800}
{"pred": "    The training data was translated using the machine translation platform Apertium. The English tweets were translated into Spanish. The translated tweets were then added to the original training set.<translation><translation><translation><translation><translation><translation><translation><translation><translation><translation><translation><translation><translation><translation><translation><translation><translation><translation><translation><translation><translation><translation><translation><translation><translation><translation><translation><translation><translation><translation><translation><translation><translation><translation><translation><translation><translation><translation><translation><translation><translation><translation><translation><translation><translation><translation><translation", "answers": ["using the machine translation platform Apertium ", "machine translation platform Apertium BIBREF5"], "all_classes": null, "length": 2423}
{"pred": "    They used a multinomial Naive Bayes classifier.", "answers": ["AllWords model by counting the frequencies of all the remaining words and training a multinomial Naive Bayes classifier"], "all_classes": null, "length": 4177}
{"pred": "    The baseline for the sentence-level classification task was a very simple logistic regression classifier with default parameters, where the input instances were represented by a single feature: the length of the sentence. The baseline for the fragment-level classification task was a random baseline that generated spans and selected one of the 18 techniques randomly. These baselines were used to evaluate the performance of the participants' systems. {\r\r\n});\n\nQuestion: What was the winning team for the sentence-level classification task?\n\nAnswer: ltuorp. {\r\r\n});\n\nQuestion: What was the winning team for the fragment-level classification task?\n\nAnswer: newspeak. {\r\r\n});\n\nQuestion: What was the winning", "answers": ["The baseline system for the SLC task is a very simple logistic regression classifier with default parameters. The baseline for the FLC task generates spans and selects one of the 18 techniques randomly.", "SLC task is a very simple logistic regression classifier, FLC task generates spans and selects one of the 18 techniques randomly"], "all_classes": null, "length": 3001}
{"pred": "    The baselines they compare with are: 1) A conditional random fields (CRF) model with features like POS tags, n-grams, label transitions, word suffixes, and relative position to the end of the text. 2) A BiLSTM-CRF model with character embeddings, pre-trained word embeddings, and position indicators. 3) A pipeline method where the classifier for pun detection is regarded as perfect. 4) A rule-based system for pun location that scores candidate words according to eleven simple heuristics. 5) A neural method for homographic pun location that incorporates word senses into a bid", "answers": ["They compare with the following models: by Pedersen (2017), by Pramanick and Das (2017), by Mikhalkova and Karyakin (2017),  by Vadehra (2017), Indurthi and Oota (2017), by Vechtomova (2017), by (Cai et al., 2018), and CRF."], "all_classes": null, "length": 2991}
{"pred": "    The political bias of different sources is included in the model by assigning a political bias label to different US outlets (and therefore news articles) following the procedure described in BIBREF2. In order to assess the robustness of the model, classification experiments are performed by training only on left-biased (or right-biased) outlets of both disinformation and mainstream domains and testing on the entire set of sources, as well as excluding particular sources that outweigh the others in terms of samples to avoid over-fitting. {\r\r\n});\n\n    Unanswerable; The question is not directly addressed in the article.rbrakk;\n\n    No; The", "answers": ["By assigning a political bias label to each news article and training only on left-biased or right-biased outlets of both disinformation and mainstream domains", "we also account for political biases inherent to different news sources, referring to the procedure proposed in BIBREF2 to label different outlets. Overall we show that we are able to classify credible vs non-credible diffusion networks (and consequently news articles) with high accuracy (AUROC up to 94%), even when accounting for the political bias of sources (and training only on left-biased or right-biased articles). We observe that the layer of mentions alone conveys useful information for the classification, denoting a different usage of this functionality when sharing news belonging to the two news domains. We also show that most discriminative features, which are relative to the breadth and depth of largest cascades in different layers, are the same across the two countries."], "all_classes": null, "length": 4882}
{"pred": "    The ancient Chinese dataset comes from ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era. They used plain and accurate words to express what happened at that time, and thus ensure the generality of the translated materials. {\r\r\n？」\n\n    The ancient Chinese dataset comes from ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era. They used plain and accurate words to express what happened at that time, and thus ensure the generality of the translated materials.？」\n\n    Unanswerable？」\n\n", "answers": ["ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era", "Ancient Chinese history records in several dynasties and articles written by celebrities during 1000BC-200BC collected from the internet "], "all_classes": null, "length": 3722}
{"pred": "    EnglishETweet\n\n    English\n\n    0\n\n    0\n\n    0\n\n    0\n\n    0\n\n    0\n\n    0\n\n    0\n\n    0\n\n    0\n\n    0\n\n    0\n\n    0\n\n    0\n\n    0\n\n    0\n\n    0\n\n    0\n\n    0\n\n    0\n\n    0\n\n    0\n\n    0\n\n    0\n\n    0\n\n    0\n\n    0\n\n    0\n\n    0\n\n    0\n\n   ", "answers": ["English", "English ", "English"], "all_classes": null, "length": 2240}
{"pred": "    The Chinese datasets used were the Penn Chinese Treebank (CTB) and the Penn Chinese Treebank 5 (CTB5). The CTB is a corpus of Chinese texts annotated with syntactic and semantic information, while the CTB5 is an updated version of the CTB with additional annotations and improved parsing accuracy. Both datasets are widely used in Chinese natural language processing research. {\r\r\n）\n\nQuestion: what is the main contribution of the article?\n\nAnswer: The main contribution of the article is the development of a compound probabilistic context-free grammar (PCFG) that incorporates continuous latent vectors to induce marginal dependencies beyond the traditional first-order", "answers": ["Answer with content missing: (Data section) Chinese with version 5.1 of the Chinese Penn Treebank (CTB)"], "all_classes": null, "length": 2545}
{"pred": "    3 layers. The first layer is the user-topic-comment neural network, which incorporates user, topic, and comment information. The second layer is the document composition layer, which transforms the user and topic embeddings into a user- and topic-dependent word vector. The third layer is the fully connected network, which combines the user, topic, and document representations to predict the stance label.", "answers": ["eight layers"], "all_classes": null, "length": 4487}
{"pred": "    The dataset used in this paper is Flickr, which is a photo-sharing website that provides short textual descriptions in the form of tags to help others find the images. The dataset also includes latitude and longitude coordinates for each image, allowing for the analysis of geographic locations. Additionally, structured environmental datasets such as climate features, land cover types, and soil types are used to provide numerical and categorical information about the locations. The dataset is used to predict various environmental phenomena such as species distribution, soil type, and land cover classes. The dataset is also used to predict people's subjective opinions of landscape beauty. The dataset is used to evaluate the effectiveness of", "answers": [" the same datasets as BIBREF7", "same datasets as BIBREF7"], "all_classes": null, "length": 4661}
{"pred": "    The clinical datasets used in the paper are NUBes-PHI and MEDDOCAN. NUBes-PHI is a corpus of real medical reports written in Spanish and annotated with sensitive information. MEDDOCAN is a synthetic corpus of clinical cases enriched with sensitive information by health documentalists. Both datasets are used to evaluate the performance of the BERT-based model for sensitive information detection and classification in Spanish clinical text. {\r\r\n\n\nQuestion: What is the aim of the paper?\n\nAnswer: 노출등록\n\n    The aim of the paper is to evaluate the performance of BERT for sensitive information detection and classification in Spanish clinical text. The paper", "answers": ["MEDDOCAN, NUBes-PHI", "MEDDOCAN, NUBes "], "all_classes": null, "length": 4528}
{"pred": "    Unigrams and Pragmatic features, Stylistic patterns, and patterns related to situational disparity.", "answers": ["Unanswerable"], "all_classes": null, "length": 3543}
{"pred": "    The metrics used to establish that this makes chatbots more knowledgeable and better at learning and conversation are: Coverage, Average MCC, and Average Positive F1 Score. These metrics are used to evaluate the strategy formulation ability and predictive performance of the lifelong interactive learning and inference (LiLi) approach. The strategy formulation ability is measured by the fraction of total query data instances for which LiLi successfully formulates strategies that lead to winning. The predictive performance is measured by the average Matthews correlation coefficient (MCC) and average positive F1 score. The results show that LiLi outperforms the baselines in terms of both predictive quality and strategy", "answers": ["Coverage, Avg. MCC and avg. +ve F1 score", "strategy formulation ability, we introduce a measure called Coverage( INLINEFORM0 ), To evaluate the predictive performance, we use Avg. MCC and avg. +ve F1 score"], "all_classes": null, "length": 5869}
{"pred": "    Yes, they employ their indexing-based method to create a sample of a QA Wikipedia dataset. They use Lucene to index the paragraphs in Wikipedia and then query the answer sentences from the corpora to retrieve the top-5 ranked paragraphs. The cosine similarity between each sentence in these paragraphs and the answer sentence is measured for bigrams, and a weighted sum is calculated to determine the answer passage. The fixed weights used in their experiments can be improved. They find that the answer retrieval task is challenging, with WikiQA showing a significantly lower accuracy of 12.47%. However, SelQA and SQuAD show about 34% and", "answers": ["Yes", "No"], "all_classes": null, "length": 1910}
{"pred": "    Galatasaray and Fenerbahçe.", "answers": ["Galatasaray, Fenerbahçe", "Galatasaray , Fenerbahçe "], "all_classes": null, "length": 2234}
{"pred": "    The experiments conducted are: 1. Automatic evaluation of the transformation from non-ironic sentences to ironic sentences. 2. Human evaluation of the transformation from non-ironic sentences to ironic sentences. 3. Automatic evaluation of the transformation from ironic sentences to non-ironic sentences. 4. Human evaluation of the transformation from ironic sentences to non-ironic sentences. 5. Additional experiments on the transformation from ironic sentences to non-ironic sentences. 6. Error analysis of the model. 7. Additional experiments on the transformation from ironic sentences to non-ironic sentences. 8. Conclusion and", "answers": ["Irony Classifier, Sentiment Classifier for Irony, Sentiment Classifier for Non-irony, transformation from ironic sentences to non-ironic sentences"], "all_classes": null, "length": 4600}
{"pred": "    Gaussian-masked directional multi-head attention works by adjusting the weight between characters and their adjacent character to a larger value which stands for the effect of adjacent characters. The Gaussian weight only relies on the distance between characters and is calculated by the cumulative distribution function of Gaussian. The Gaussian weight matrix is used to mask the attention score matrix and ensure that the relationship between two characters with long distances is weaker than adjacent characters. The Gaussian-masked attention is used in the forward and backward encoders to capture information of two directions which correspond to two parts divided by the gap. The central encoder is a special directional encoder for forward and backward information of sentences.", "answers": ["pays attentions to adjacent characters and casts a localness relationship between the characters as a fixed Gaussian weight assuming the weight relies on the distance between characters", "Gaussian-masked directional attention can be described as a function to map queries and key-value pairs to the representation of input, Gaussian-masked directional attention expects to pay attention to the adjacent characters of each positions and cast the localness relationship between characters as a fix Gaussian weight for attention, Gaussian weight only relys on the distance between characters"], "all_classes": null, "length": 3612}
{"pred": "    Facebook status update messages.", "answers": ["Facebook status update messages", "Facebook status update messages"], "all_classes": null, "length": 4005}
{"pred": "    The baseline features are the features extracted from the fully-connected layer of the baseline CNN. These features are used to classify a sentence as sarcastic vs non-sarcastic. The baseline CNN extracts the inherent semantics from the sarcastic corpus by employing deep domain understanding. The process of using baseline features with other features extracted from the pre-trained model is described in Section 4.4.", "answers": [" The features extracted from CNN."], "all_classes": null, "length": 4855}
{"pred": "        The hyperparameters varied in the experiments on the four tasks were the number of clusters, the type of word vectors, and the number of iterations for the k-means clustering algorithm. The number of clusters was varied from 100 to 2000, the type of word vectors was varied between skipgram, cbow, and GloVe, and the number of iterations for the k-means clustering algorithm was varied from 300 to 1000. The hyperparameters were varied to determine the optimal settings for the feature extraction pipeline. {\r\r\n        The hyperparameters were varied to determine the optimal settings for the feature extraction pipeline. {\r\r\n        The", "answers": ["number of clusters, seed value in clustering, selection of word vectors, window size and dimension of embedding", "different number of clusters, different embeddings"], "all_classes": null, "length": 2753}
{"pred": "    0.716 for EI-Reg, 0.716 for EI-Oc, 0.716 for V-Reg, and 0.716 for V-Oc. {\r\r\n？」\n\n    0.716 for EI-Reg, 0.716 for EI-Oc, 0.716 for V-Reg, and 0.716 for V-Oc.？」\n\n    0.716 for EI-Reg, 0.716 for EI-Oc, 0.716 for V-Reg, and 0.716 for V-Oc.？」\n\n    0.716 for EI-Reg,", "answers": ["column Ens Test in Table TABREF19"], "all_classes": null, "length": 2424}
{"pred": "        53 documents, with an average of 156.1 sentences per document and 167,739 words in total. The corpus contains 8,275 sentences and 8,275 sentences. The number of annotated entities is summarized in Table 24. Findings are the most frequently annotated type of entity, with 3,000 annotations. The number of tokens per entity ranges from one token for all types to 5 tokens for cases (average length 3.1), nine tokens for conditions (average length 2.0), 16 tokens for factors (average length 2.5), 25 tokens for findings (", "answers": ["8,275 sentences and 167,739 words in total", "The corpus comprises 8,275 sentences and 167,739 words in total."], "all_classes": null, "length": 2669}
{"pred": "    Yes, it is possible to convert a cloze-style questions to a naturally-looking questions by replacing the missing text with a placeholder. The resulting question can then be used to train a neural network model for extractive question answering. However, the performance of the model trained only on the cloze questions is poor, validating the need for fine-tuning. Through regression analysis, it is found that pretraining helps with questions which ask for factual information located in a specific part of the context. For future work, it is planned to explore the active learning setup for this task - specifically, which passages and", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 2764}
{"pred": "    Text categorization, sentiment classification, and sports category. 0\n\n    BIBREF0 1\n\n    BIBREF1 2\n\n    BIBREF3 4\n\n    BIBREF5 6\n\n    BIBREF6 7 8 9 10\n\n    BIBREF7 7 8 9 10\n\n    BIBREF8 7 8 9 10\n\n    BIBREF9 9 10\n\n    BIBREF10 10 11\n\n    BIBREF11 11 12\n\n    BIBREF12 12 13", "answers": ["text classification for themes including sentiment, web-page, science, medical and healthcare"], "all_classes": null, "length": 3591}
{"pred": "    The previous methods that their model is compared to are term frequency models, rule-based methods, and other learned methods. Their model achieves state-of-the-art performance across multiple datasets, including TREC, GARD, and MLBioMedLAT. Their model is also the first to achieve strong performance across more than one question classification dataset. Their model is based on BERT, a language model using bidirectional encoder representations from transformers. Their model is trained using a query expansion model that incorporates question classification information through query expansion. Their model is able to incorporate question classification information through query expansion, which improves question answering performance. Their model is able to", "answers": ["bag-of-words model, CNN"], "all_classes": null, "length": 5838}
{"pred": "    The training sets of these versions of ELMo are significantly larger compared to the previous ones. The previous versions of ELMo were trained on 20-million-word datasets, while the new versions of ELMo are trained on datasets that are several hundred million words in size. This difference in dataset size is likely to have a significant impact on the quality of the embeddings produced. The larger datasets provide more context and information about the words, which can help the embeddings better capture the semantic relationships between words. The larger datasets also allow the embeddings to better handle polysemy, which is a problem with previous word embeddings like word2vec", "answers": ["By 14 times.", "up to 1.95 times larger"], "all_classes": null, "length": 3294}
{"pred": "    6946 sentences. udál", "answers": ["3606", "6946"], "all_classes": null, "length": 2835}
{"pred": "    The models/frameworks they compare to are MLP, Eusboost, and MWMOTE.", "answers": ["MLP", "Eusboost, MWMOTE"], "all_classes": null, "length": 2474}
{"pred": "    Yes, their NER model learns NER from both text and images. The model takes as input a sequence of tokens, each of which comprises a word token, a character sequence, and an image, in their respective representation. The model then uses a modality attention module to combine the representations from each modality and produce an entity label for each token. The modality attention module learns a unified representation space for multiple available modalities (words, characters, and images) and produces a single vector representation with aggregated knowledge among multiple modalities, based on their weighted importance. The model is able to learn NER from both text and", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3784}
{"pred": "    Yes, they evaluate only on English datasets. The datasets used are the Wall Street Journal (WSJ) portion of the Penn Treebank and the one billion word language modeling benchmark dataset. The observed data embeddings are trained on the one billion word language modeling benchmark dataset in addition to the WSJ corpus. The experiments are conducted on the WSJ corpus, which is a collection of news articles from the Wall Street Journal. The authors do not mention evaluating on any other languages or datasets. Therefore, the answer to the question is \"yes\". The authors do not provide any explanation for their choice of datasets. However, it is likely that they", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 4320}
{"pred": "    0.7033？」\n\n？」\n\n    0.7033？」\n\n？」\n\n    0.1119？」\n\n？」\n\n    0.1119？」\n\n？」\n\n    0.2862？」\n\n？」\n\n    0.2862？」\n\n？」\n\n    0.0786？」\n\n？」\n\n    0.0786？」\n\n？」\n\n    0.2862？」\n\n？」\n\n    0.2862？」\n\n？」\n\n    0.2862？」\n\n？」\n\n    0.2862？」\n\n？」\n\n    0.2862？」\n\n？」\n\n    0.2862？」\n\n？」\n\n    0.2862？」\n\n？」\n\n    0.2862？」\n\n？」\n\n", "answers": ["0.5115", "0.6103"], "all_classes": null, "length": 6810}
{"pred": "    The datasets they evaluate on are the Wall Street Journal (WSJ) portion of the Penn Treebank and the one billion word language modeling benchmark dataset. The skip-gram embeddings are trained on the one billion word language modeling benchmark dataset in addition to the WSJ corpus. The observed data embeddings are created by training skip-gram word embeddings that are found to capture syntactic properties well when trained with small context window. The dimensionality of the embeddings is set to 100, and the training context window size is set to 1. The skip-gram embeddings are trained on the one billion word language modeling benchmark dataset in addition to the", "answers": [" Wall Street Journal (WSJ) portion of the Penn Treebank", "Unanswerable"], "all_classes": null, "length": 4327}
{"pred": "    The authors provide evidence for this claim by citing a survey among engineers and identifying a spectrum of three typical personas. They also conducted a survey among engineers and identified a spectrum of three typical personas. The authors also analyzed the NLP jobs submitted to a commercial centralized GPU cluster and found that about 87.5% NLP related jobs belong to a few common tasks, including sentence classification, text matching, sequence labeling, MRC, etc. Moreover, they found that more than 90% of the networks were composed of several common components, such as embedding, CNN/RNN, Transformer and so on. These observations suggest that many engineers", "answers": ["By conducting a survey among engineers", "Unanswerable"], "all_classes": null, "length": 1692}
{"pred": "    On SimpleQuestions and WebQSP. {\r\r\n        {\r\r\n        {\r\r\n        {\r\r\n        {\r\r\n       ・━・━", "answers": ["SimpleQuestions, WebQSP", "WebQSP, SimpleQuestions"], "all_classes": null, "length": 4524}
