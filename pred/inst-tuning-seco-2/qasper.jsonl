{"pred": "        The ground truth for fake news is established by manually inspecting the text field within the tweets to label them as containing fake news, or not containing them (according to the characterization presented before). This annotated dataset is publicly available and can be freely reused. (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no)", "answers": ["Ground truth is not established in the paper"], "all_classes": null, "length": 3141}
{"pred": "        The GhostVLAD approach is a pooling method that uses ghost clusters to improve the performance of language identification. It adds ghost clusters to the NetVLAD approach, which helps to map noisy or irrelevant content into ghost clusters and reduces the contribution of unwanted features to normal VLAD clusters. The GhostVLAD approach is shown to outperform previous state-of-the-art methods by an absolute 1.88% F1-score. It also works better with shorter input durations and has very good language discriminative features. (Unanswerable) (Yes) (No) (Unanswerable) (Yes) (No) (Unanswerable", "answers": ["extension of the NetVLAD, adds Ghost clusters along with the NetVLAD clusters", "An extension of NetVLAD which replaces hard assignment-based clustering with soft assignment-based clustering with the additon o fusing Ghost clusters to deal with noisy content."], "all_classes": null, "length": 2454}
{"pred": "        68.8% to 71.8% when applied to the IEMOCAP dataset. (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes", "answers": ["the attention model, MDREA, also outperforms the best existing research results (WAP 0.690 to 0.688)"], "all_classes": null, "length": 3207}
{"pred": "        The additional features and context proposed are context tweets, which are the tweets that the abusive language is in response to or has quoted. The context tweets provide significant contextual information that can help computers better understand the abusive language. The authors also propose using latent topic information from the hidden states of RNN models as additional information in classifying the text data. Finally, the authors suggest using ensemble models of variant models and features for further improvements. (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes)", "answers": ["using tweets that one has replied or quoted to as contextual information", "text sequences of context tweets"], "all_classes": null, "length": 2060}
{"pred": "    FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney.\n\nQuestion: What are the main observations from the results on the development set?\n\nAnswer: A simple tf-idf bag-of-word model works well, but the other textual and lexicon-based features don't seem to contribute much. Google embeddings perform better than Facebook embeddings, and retrofitting helps improve the Facebook embeddings. All features work better in combination with embeddings, but removing the Lexicon feature yields even better results.\n\nQuestion", "answers": ["FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney", "FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney."], "all_classes": null, "length": 3411}
{"pred": "    Yes, both datasets contain only English data. The SemEval dataset is a subset of the Stanford Sentiment Analysis Dataset, which consists of tweets in English. The hashtags in the SemEval dataset are also in English. The STAN dataset contains only English hashtags and their associated tweets. The STAN dataset is a subset of the Stanford Sentiment Analysis Dataset, which consists of tweets in English. The hashtags in the STAN dataset are also in English. Therefore, both datasets contain only English data. (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3735}
{"pred": "    The proposed evaluation is based on the quality of the concept maps generated by the summarization system. The quality of the concept maps is assessed by comparing them to human-generated reference concept maps. The evaluation metrics include precision, recall, and F1 score. The evaluation also involves a human evaluation component to assess the usefulness of the generated concept maps for a user. The evaluation is designed to be comprehensive and objective, allowing for a fair comparison of different summarization systems. (Unanswerable) (Unanswerable) (Unanswerable) (Unanswerable) (Unanswerable) (Unanswerable) (Unanswerable)", "answers": ["Answer with content missing: (Evaluation Metrics section) Precision, Recall, F1-scores, Strict match, METEOR, ROUGE-2"], "all_classes": null, "length": 4263}
{"pred": "    The datasets used for evaluation are CNN/DailyMail, New York Times Annotated Corpus (NYT), and XSum. These datasets represent different summary styles ranging from highlights to very brief one sentence summaries. The summaries also vary with respect to the type of rewriting operations they exemplify. The proportion of novel bi-grams in gold summaries is also reported as a measure of their abstractiveness. (Section 4.1) (Section 4.2) (Section 4.3) (Table 1) (Table 2) (Table 3) (Table 4) (Table 5) (Table ", "answers": ["CNN/DailyMail news highlights, New York Times Annotated Corpus, XSum", "the CNN/DailyMail news highlights dataset BIBREF24, the New York Times Annotated Corpus (NYT; BIBREF25), XSum BIBREF22"], "all_classes": null, "length": 4369}
{"pred": "    The proposed approach uses a KL divergence based energy function to learn multi-sense word embedding distributions modeled as Gaussian mixtures. It outperforms existing approaches in terms of word similarity and entailment on benchmark datasets. The KL divergence based energy function captures asymmetry in entailment datasets and enables capturing lexical relationships between senses of child words and parent words. The approach also encourages capturing different possible senses of a word by minimizing KL divergence between mixture components. The use of Gaussian mixtures allows for modeling uncertainty in word representation and polysemy. The approximate KL divergence function used is stricter than previous approximations, making it more suitable for high dimensional embedding", "answers": ["GM$\\_$KL achieves better correlation than existing approaches for various metrics on SCWS dataset."], "all_classes": null, "length": 2189}
{"pred": "    The ensemble method works by selecting the best performing model according to validation performance and then adding the best performing model that has not been previously tried. The process is repeated until a certain number of models have been selected for the final ensemble. The resulting model is a greedy ensemble that is formed by simply averaging the predictions from the constituent single models. The algorithm is offered 10 models and selects 5 of them for the final ensemble. The ensemble method is used to improve the performance of the model by combining the strengths of multiple models. The authors believe that this approach can help to overcome the limitations of individual models and achieve better results. The ensemble", "answers": ["simply averaging the predictions from the constituent single models"], "all_classes": null, "length": 4212}
{"pred": "    The sources of the datasets are the scripts of the Friends TV sitcom and Facebook messenger chats. The dataset consists of two subsets, Friends and EmotionPush, according to the source of the dialogues. The former comes from the scripts of the Friends TV sitcom, while the latter is made up of Facebook messenger chats. Each subset includes 1,000 English dialogues, and each dialogue can be further divided into a few consecutive utterances. All the utterances are annotated by five annotators on a crowd-sourcing platform (Amazon Mechanical Turk), and the labeling work is only based on the textual content. Annotator votes for one", "answers": ["Friends TV sitcom, Facebook messenger chats"], "all_classes": null, "length": 3185}
{"pred": "        The paper focuses on English. The authors collected a simplified dataset from Simple English Wikipedia that are freely available, which has been previously used for many text simplification methods. The simple English Wikipedia is pretty easy to understand than normal English Wikipedia. The authors downloaded all articles from Simple English Wikipedia, removed stubs, navigation pages, and any article that consisted of a single sentence, split them into sentences with the Stanford CorNLP, and deleted these sentences whose number of words are smaller than 10 or large than 40. After removing repeated sentences, they chose 600K sentences as the simplified data with 11.6M words,", "answers": ["English", "Simple English"], "all_classes": null, "length": 2243}
{"pred": "    The IMDb dataset of movie reviews is used for sentiment analysis. (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable)", "answers": ["IMDb dataset of movie reviews", "IMDb"], "all_classes": null, "length": 2327}
{"pred": "    89.2% on the development set and 88.3% on the test set. (unanswerable)\n\nQuestion: What are the main contributions of the paper?\n\nAnswer: The main contributions of the paper are: (1) proposing a new approach to crowd annotation learning based on adversarial training, (2) creating two new datasets for Chinese NER tasks in the dialog and e-commerce domains, and (3) achieving better performance than strong baseline systems. (unanswerable)\n\nQuestion: What are the main differences between the baseline and proposed systems?\n\nAnswer: The main differences between the baseline and proposed systems are:", "answers": ["F1 scores of 85.99 on the DL-PS data,  75.15 on the EC-MT data and 71.53 on the EC-UQ data ", "F1 of 85.99 on the DL-PS dataset (dialog domain); 75.15 on EC-MT and 71.53 on EC-UQ (e-commerce domain)"], "all_classes": null, "length": 5310}
{"pred": "        Yes, they experimented with this new dataset. They recorded data from 19 participants and discarded the data of one of them due to technical difficulties with the eye-tracking calibration. Hence, they share the data of 18 participants. All participants are healthy adults (mean age = 34 (SD=8.3), 10 females). Their native language is English, originating from Australia, Canada, UK, USA or South Africa. Two participants are left-handed and three participants wear glasses for reading. Details on subject demographics can be found in Table 4. All participants gave written consent for their participation and the re-use of the data prior", "answers": ["No"], "all_classes": null, "length": 3445}
{"pred": "        The datasets used are the Wizard of Oz method and crowdsourcing. The Wizard of Oz method involves applying the Wizard of Oz method to a set of potential users of the system, and collecting questions that the users asked. These questions are manually classified into a set of intent classes, and used to train the first version of the system. The crowdsourcing approach involves increasing the training set by collecting more questions and samples per class. The intent classifier is trained using domain-specific word vectors and Support Vector Machines (SVM). The training set is created using a set of 246,945 documents related to finance, and each intent is encoded with its", "answers": ["Custom dataset with user questions; set of documents, twitter posts and news articles, all related to finance.", "a self-collected financial intents dataset in Portuguese"], "all_classes": null, "length": 13401}
{"pred": "    Energy sector achieves the best performance. (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no", "answers": ["Energy with accuracy of 0.538", "Energy"], "all_classes": null, "length": 10349}
{"pred": "    RNN-based NMT and Transformer-NMT\n\n        RNN-based NMT and Transformer-NMT\n\n    RNN-based NMT and Transformer-NMT\n\n        RNN-based NMT and Transformer-NMT\n\n    RNN-based NMT and Transformer-NMT\n\n        RNN-based NMT and Transformer-NMT\n\n    RNN-based NMT and Transformer-NMT\n\n        RNN-based NMT and Transformer-NMT\n\n    RNN-based NMT and Transformer-NMT\n\n        RNN-based NMT and Transformer-NMT\n\n    RNN-based NMT and Transformer-NMT\n\n        RNN-based NMT and", "answers": ["RNN-based NMT model, Transformer-NMT"], "all_classes": null, "length": 3708}
{"pred": "    1. A regularization term associated with neutral features 2. The maximum entropy of class distribution regularization term 3. The KL divergence between reference and predicted class distribution regularization term\n\nQuestion: How do these regularization terms help make the model more robust?\n\nAnswer: davidjl\n\n     davidjl\n\n    1. Neutral features prevent the model from biasing to the class with the dominate number of labeled features 2. Maximum entropy constrains the predicted class distribution on unlabeled data 3. KL divergence controls the unbalance in labeled features and in the dataset\n\nQuestion: What is the main contribution of this work?\n\nAnswer: davidjl\n\n    ", "answers": ["a regularization term associated with neutral features, the maximum entropy of class distribution regularization term, the KL divergence between reference and predicted class distribution", "a regularization term associated with neutral features,  the maximum entropy of class distribution, KL divergence between reference and predicted class distribution"], "all_classes": null, "length": 3604}
{"pred": "    1) SVM with unigram, bigram, and trigram features, which is a standard yet rather strong classifier for text features; 2) SVM with average word embedding, where a document is represented as a continuous representation by averaging the embeddings of the composite words; 3) SVM with average transformed word embeddings (the INLINEFORM0 in equation EQREF6), where a document is represented as a continuous representation by averaging the transformed embeddings of the composite words; 4) two mature deep learning models on text classification, CNN BIBREF3 and Recurrent Convolutional Neural Networks (RCNN) BIBREF0", "answers": ["SVM with unigram, bigram, and trigram features, SVM with average word embedding, SVM with average transformed word embeddings, CNN, ecurrent Convolutional Neural Networks, SVM and deep learning models with comment information", "SVM with unigram, bigram, trigram features, with average word embedding, with average transformed word embeddings, CNN and RCNN, SVM, CNN, RCNN with comment information"], "all_classes": null, "length": 4512}
{"pred": "        By 2.5 points. (The baseline score was 0.5, and the multitask learning score was 3.0.) (The baseline score was 0.5, and the multitask learning score was 3.0.) (The baseline score was 0.5, and the multitask learning score was 3.0.) (The baseline score was 0.5, and the multitask learning score was 3.0.) (The baseline score was 0.5, and the multitask learning score was 3.0.) (The baseline score was 0.5,", "answers": ["They decrease MAE in 0.34"], "all_classes": null, "length": 2735}
{"pred": "    Their model improves interpretability by allowing different attention heads to learn different sparsity behaviors, which can be analyzed to identify head specializations. The sparsity of the attention weights also allows for more confident representations of certain types of tokens, such as positional heads. Additionally, the adaptivity of the model allows for continuous and dynamic changes in the shape of the attention heads, which can be learned automatically. Overall, the adaptively sparse transformers provide a more interpretable and flexible model for NLP tasks.\r\r\n\r\r\n    […]\n\nQuestion: What are some of the head specializations identified in their model?\n\nAnswer: 노출등록\n\n     노출등록\n\n    Some of", "answers": ["the attention heads in the proposed adaptively sparse Transformer can specialize more and with higher confidence", "We introduce sparse attention into the Transformer architecture"], "all_classes": null, "length": 4902}
{"pred": "    The baseline is a context-agnostic machine translation system that produces translations of isolated sentences. The DocRepair model is designed to correct inconsistencies between these translations and produce more context-aware translations. The baseline is used as input for the DocRepair model, which is trained to correct these inconsistencies. The baseline is not used as a reference for evaluation, but rather as a starting point for the DocRepair model to improve upon. The DocRepair model is evaluated based on its ability to produce more context-aware translations, as measured by BLEU score, contrastive evaluation of discourse phenomena, and human evaluation. The baseline is not directly compared to the DocRepair", "answers": [" MT system on the data released by BIBREF11", "Transformer base, two-pass CADec model"], "all_classes": null, "length": 3716}
{"pred": "    The metrics used for evaluation are Labeled Attachment Scores (LAS) for zero-shot dependency parsing and accuracy for cross-lingual natural language inference (XNLI). The LAS metric measures the percentage of correctly labeled dependencies in a treebank, while the accuracy metric measures the percentage of correctly classified sentences in the XNLI dataset. The authors also report the results of previous work for reference, notably the state-of-the-art system XLM. The authors compare the results of their approach with mBERT and find that their approach produces better results on both tasks. They also find that the performance of their bilingual LM, RAMEN, correlates with", "answers": ["translation probabilities, Labeled Attachment Scores (LAS)", "accuracy, Labeled Attachment Scores (LAS)"], "all_classes": null, "length": 3402}
{"pred": "    The attention module is pretrained on the ASR and MT tasks. The attention module is task-specific, so it is pretrained on the tasks that it will be used for. This allows the attention module to learn the alignments between the input and output sequences for each task, which can improve the performance of the model. However, the attention module is not pretrained on the ST task, which can lead to suboptimal performance in the ST task. To address this issue, the attention module can be pretrained on the ST task as well, which can improve the performance of the model in the ST task. This is one of the contributions of the", "answers": ["the model is pre-trained on CTC-based ASR task and MT task in the pre-training stage."], "all_classes": null, "length": 4656}
{"pred": "    The stylistic features obtained are emoticons, laughter expressions such as \"lol\", and patterns related to situational disparity. These features are used to detect sarcasm in text. However, it has been difficult to solve this problem using traditional NLP tools and techniques. The authors propose to use cognitive features extracted with the help of eye-tracking to address this problem. They hypothesize that reading sarcastic texts induces distinctive eye movement patterns, compared to literal texts. The cognitive features, derived from human eye movement patterns observed during reading, include two primary feature types: (1) fixation duration on the text and (2) scanpaths. The", "answers": ["Unanswerable"], "all_classes": null, "length": 3543}
{"pred": "    The encoder has an LSTM layer. (1) (2) (3) (4) (5) (6) (7) (8) (9) (10) (11) (12) (13) (14) (15) (16) (17) (18) (19) (20) (21) (22) (23) (24) (25) (26) (27) (28) (29) (30) (31) (32) (33) (34) (35) (36) (37) (38) (39) (40)", "answers": ["LSTM", "LSTM"], "all_classes": null, "length": 2289}
{"pred": "    Yes, WordNet is useful for taxonomic reasoning for this task. It provides a set of triples that represent a directed, edge-labeled graph, which allows for the construction of natural language questions that contextualize the types of concepts being probed. The availability of glosses and example sentences in WordNet allows for the creation of natural language questions that contextualize the types of concepts being probed. The use of WordNet also allows for the construction of questions that involve complex forms of reasoning, such as ISA reasoning, which requires several inferential steps or hops. The availability of glosses and example sentences in WordNet allows for the", "answers": ["Unanswerable", "Yes"], "all_classes": null, "length": 6391}
{"pred": "        The baselines were wav2letter and other non end-to end models. The goal was to match or outperform these baselines. The architecture was designed to be highly efficient for training and inference on GPUs. The authors found that ReLU and batch normalization outperformed other activation and normalization schemes. The largest version of the model had 54 convolutional layers (333M parameters), while the smallest version had 34 (201M parameters). The authors also investigated a number of residual options and proposed a new residual connection topology called Dense Residual (DR). The authors integrated their best acoustic model with a Transformer-XL language model to", "answers": ["Unanswerable", "LF-MMI Attention\nSeq2Seq \nRNN-T \nChar E2E LF-MMI \nPhone E2E LF-MMI \nCTC + Gram-CTC"], "all_classes": null, "length": 1856}
{"pred": "        22,880 users", "answers": ["22,880 users", "20,000"], "all_classes": null, "length": 4160}
{"pred": "    BPE perplexity, BLEU-1/4, ROUGE-L, Distinct-1/2, Recipe Level Coherence, Recipe Step Entailment, Human Evaluation\n\nQuestion: How do the personalized models perform compared to the baseline?\n\nAnswer:?\"\n\n\n\n        The personalized models outperform the baseline in BPE perplexity, but perform worse in BLEU-1/4 and ROUGE-L. They generate more diverse and acceptable recipes, but the BLEU and ROUGE metrics are not the most appropriate measures for generation quality. The personalized models also perform better in user matching accuracy and mean reciprocal rank, indicating that they personalize", "answers": ["Byte-Pair Encoding perplexity  (BPE PPL),\nBLEU-1,\nBLEU-4,\nROUGE-L,\npercentage of distinct unigram (D-1),\npercentage of distinct bigrams(D-2),\nuser matching accuracy(UMA),\nMean Reciprocal Rank(MRR)\nPairwise preference over baseline(PP)", "BLEU-1/4 and ROUGE-L, likelihood of generated recipes using identical input specifications but conditioned on ten different user profiles, user matching accuracy (UMA), Mean Reciprocal Rank (MRR), neural scoring model from BIBREF33 to measure recipe-level coherence", " Distinct-1/2, UMA = User Matching Accuracy, MRR\n= Mean Reciprocal Rank, PP = Pairwise preference over baseline (evaluated for 310 recipe pairs per model)"], "all_classes": null, "length": 2673}
{"pred": "    They create labels on their dataset by specifying a symptom and an attribute, and then labeling the groundtruth output of the QA system based on the template generation rules. Moreover, they adopt the unanswerable design when the patient does not mention a particular symptom, and the answer is defined as \"No Answer\". This process is repeated until all logical permutations of symptoms and attributes are exhausted. (Unanswerable) (Yes) (No) (Unanswerable) (Yes) (No) (Unanswerable) (Yes) (No) (Unanswerable) (Yes) (No) (Unanswerable) (Yes", "answers": ["(1) the time the patient has been experiencing the symptom, (2) activities that trigger the symptom (to occur or worsen), (3) the extent of seriousness, (4) the frequency occurrence of the symptom, and (5) the location of symptom, No Answer", "the time the patient has been experiencing the symptom, activities that trigger the symptom, the extent of seriousness, the frequency occurrence of the symptom, the location of symptom, 9 symptoms"], "all_classes": null, "length": 3424}
{"pred": "    1000 sentences (or 2000 abstracts) with expert annotations for the interventions extraction task. The difficulty of the sentences is determined by the predicted difficulty scores. The sentences are ranked by the count of difficult sentences, and the abstracts that contain the most difficult sentences are re-annotated by experts. The re-annotation cost is $3,000. The new annotation data is available at https://github.com/bepnye/EBM-NLP. The results show that adding more training data with crowd annotation still improves the model performance, but the improvement is smaller than when the difficult subset with expert annotations is mixed", "answers": ["57,505 sentences", "57,505 sentences"], "all_classes": null, "length": 4371}
{"pred": "    Neural machine translation (NMT) and other natural language processing (NLP) tasks such as improving the state of the art of Neural Machine Translation, BERT, and GPT-2. The Transformer architecture is used for these tasks.΄\n\nQuestion: What are the main contributions of the proposed method?\n\nAnswer: 노출등록\n\n     노출등록\n\n    1. Introducing sparse attention into the Transformer architecture, which leads to slight accuracy gains and improved interpretability. 2. Proposing an adaptive version of sparse attention, where the shape of each attention head can vary continuously and dynamically between the dense limit case of softmax and the sparse", "answers": ["four machine translation tasks: German -> English, Japanese -> English, Romanian -> English, English -> German", " four machine translation tasks, IWSLT 2017 German $\\rightarrow $ English BIBREF27, KFTT Japanese $\\rightarrow $ English BIBREF28, WMT 2016 Romanian $\\rightarrow $ English BIBREF29, WMT 2014 English $\\rightarrow $ German BIBREF30"], "all_classes": null, "length": 4898}
{"pred": "    The performance of ELMo embeddings for Estonian in the NER task improved significantly compared to the non-contextual fastText baseline. The ELMo embeddings showed the largest improvement over fastText embeddings for languages with the smallest NER datasets, such as Croatian and Lithuanian. However, the ELMo embeddings also showed significant improvements for languages with larger NER datasets, such as English and Finnish. Only on the Slovenian dataset did ELMo perform slightly worse than fastText, but on all other EMBEDDIA languages, the ELMo embeddings improved the results. (unanswerable) (yes) (", "answers": ["5 percent points.", "0.05 F1"], "all_classes": null, "length": 3290}
{"pred": "    They have experience in analyzing text as social and cultural data and have consolidated their experiences in this article. They come from different disciplines and have different perspectives on the research process. They discuss how the research process often unfolds and the challenges they face. They also provide insights into how computational text analysis can help us better understand social and cultural phenomena. (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable)", "answers": ["Unanswerable"], "all_classes": null, "length": 8506}
{"pred": "    Yes, the paper introduces an unsupervised approach to spam detection using LDA to extract topic-based features. The proposed features are used to distinguish human-like spammers from legitimate users. The paper claims that the proposed features are effective in detecting \"smart\" spammers who post seemingly legitimate tweets. The paper also compares the proposed features with other state-of-the-art methods and shows that they get excellent performance on human-like spammer classification. The paper concludes that the proposed features are promising for future work in spam detection. (unanswerable) (yes) (no) (unanswerable) (yes) (no) (un", "answers": ["No", "No"], "all_classes": null, "length": 2239}
{"pred": "    The Nguni languages are similar to each other and harder to distinguish. The same is true of the Sotho languages. (Section SECREF2)\n\n    The Nguni languages are similar to each other and harder to distinguish. The same is true of the Sotho languages. (Section SECREF2)\n\n    The Nguni languages are similar to each other and harder to distinguish. The same is true of the Sotho languages. (Section SECREF2)\n\n    The Nguni languages are similar to each other and harder to distinguish. The same is true of the Sotho languages. (Section", "answers": ["Nguni languages (zul, xho, nbl, ssw), Sotho languages (nso, sot, tsn)", "The Nguni languages are similar to each other, The same is true of the Sotho languages"], "all_classes": null, "length": 1877}
{"pred": "    They compared the performance of 6-layers and 9-layers sMBR models. The 9-layers model has 3 additional layers compared to the 6-layers model, and it achieves a relative 12.6% decrease in CER. The averaged CER of the sMBR models with different layers also decreases by about 0.73% compared to the CE models, indicating the effectiveness of sequence discriminative learning. The 9-layers model is considered to be the better choice for LVCSR. (unanswerable) (yes) (no) (unanswerable) (yes)", "answers": ["Unidirectional LSTM networks with 2, 6, 7, 8, and  9 layers."], "all_classes": null, "length": 4286}
{"pred": "    The Wikipedia dataset consists of articles from English Wikipedia, with quality class labels assigned by the Wikipedia community. The quality class of a Wikipedia article is assigned by Wikipedia reviewers or any registered user, who can discuss through the article's talk page to reach consensus. The dataset contains around 5K Featured Articles, 28K Good Articles, 212K B-class Articles, 533K C-class Articles, 2.6M Start Articles, and 3.2M Stub Articles. The dataset is randomly partitioned into training, development, and test splits based on a ratio of 8:1:1. The arXiv", "answers": ["a sample of  29,794 wikipedia articles and 2,794 arXiv papers "], "all_classes": null, "length": 4187}
{"pred": "        A group of 50 native people who were well-versed in both English and Tamil languages acted as annotators for the evaluation. A collection of samples of about 100 sentences were taken from the test set results for comparison. This set included a randomized selection of the translation results to ensure the objectivity of evaluation. Fluency and adequacy results for the RNNMorph results are tabulated. Adequacy rating was calculated on a 5-point scale of how much of the meaning is conveyed by the translation (All, Most, Much, Little, None). The fluency rating was calculated based on grammatical correctness on a", "answers": ["50 human annotators ranked a random sample of 100 translations by Adequacy, Fluency and overall ranking on a 5-point scale.", "adequacy, precision and ranking values"], "all_classes": null, "length": 5344}
{"pred": "    Yes, they test their framework performance on commonly used language pairs, such as English-to-German. They also test their framework performance on other language pairs, such as French-to-German and English-to-French. Their results show that their framework can improve the performance of NMT on these language pairs. They also show that their framework can help reduce the number of rare words in the target language. Their results are promising and show that their framework can be a useful tool for improving the performance of NMT on commonly used language pairs. However, they also note that there are still some issues that need to be addressed in future work.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4472}
{"pred": "The models are evaluated based on their efficiency and accuracy in reconstructing the target sentence from the keywords provided by the user. The efficiency is measured by the retention rate of tokens, while the accuracy is measured by the fraction of sentences generated by greedily decoding the model that exactly matches the target sentence. The models are also compared to two rule-based baselines to show the benefits of jointly training the encoder and decoder. Finally, a user study is conducted to measure the completion times and accuracies of typing sentences with and without the autocomplete system. The results show that the system is efficient and accurate, and users can save time by using it. (", "answers": ["by training an autocomplete system on 500K randomly sampled sentences from Yelp reviews", "efficiency of a communication scheme $(q_{\\alpha },p_{\\beta })$ by the retention rate of tokens, which is measured as the fraction of tokens that are kept in the keywords, accuracy of a scheme is measured as the fraction of sentences generated by greedily decoding the model that exactly matches the target sentence"], "all_classes": null, "length": 1873}
{"pred": "        Precision, Recall, and F-measure are the evaluation metrics used for classification tasks. Precision measures the proportion of true positives to all predicted positives, while Recall measures the proportion of true positives to all actual positives. F-measure is a weighted average of Precision and Recall, which provides a single score that combines both metrics. These metrics are used to evaluate the performance of a classification model and determine its accuracy. (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (un", "answers": ["Precision, Recall, F-measure, accuracy", "Precision, Recall and F-measure"], "all_classes": null, "length": 3044}
{"pred": "    The source domain is the existing domain with sufficient labeled data, while the target domain is the new domain with very few or no labeled data. The goal is to transfer knowledge from the source domain to the target domain to alleviate the required labeling effort. (unanswerable)\n\nQuestion: What is the key challenge of domain adaptation?\n\nAnswer: The key challenge of domain adaptation is that data in the source and target domains are drawn from different distributions, which makes adaptation performance decline with an increase in distribution difference. (unanswerable)\n\nQuestion: What are some techniques for addressing the problem of domain shifting?\n\nAnswer: Some techniques for addressing the problem", "answers": ["Book, electronics, beauty, music, IMDB, Yelp, cell phone, baby, DVDs, kitchen", "we use set 1 of the source domain as the only source with sentiment label information during training, and we evaluate the trained model on set 1 of the target domain, Book (BK), Electronics (E), Beauty (BT), and Music (M)"], "all_classes": null, "length": 5061}
{"pred": "    They compare with LSTM and similar recurrent units. They also mention other gating structures and transformations that have been proposed to improve the performance of RNNs. They describe recent work in language modeling and how it relates to their proposed PRU architecture. They also provide an overview of the PRU architecture and how it differs from previous RNN models. They compare the performance of the PRU with state-of-the-art methods and show that it achieves better performance with fewer parameters. They also provide a detailed analysis of the PRU and its behavior on the language modeling tasks. They conclude that the PRU is a promising new RNN architecture that can", "answers": ["Variational LSTM, CharCNN, Pointer Sentinel-LSTM, RHN, NAS Cell, SRU, QRNN, RAN, 4-layer skip-connection LSTM, AWD-LSTM, Quantized LSTM"], "all_classes": null, "length": 3319}
{"pred": "    The following neural network modules are included in NeuronBlocks: embedding layer, neural network layers (RNN, CNN, QRNN, Transformer, Highway network, Encoder Decoder architecture, attention mechanisms, regularization layers), loss function, metrics.\n\n    The embedding layer is used to represent words or characters as vectors, while the neural network layers are used to process the input data. The attention mechanisms are used to focus on specific parts of the input data, while the regularization layers are used to improve the generalization ability of the model. The loss function and metrics are used to evaluate the performance of the model.\n\n    The above answer is based on", "answers": ["Embedding Layer, Neural Network Layers, Loss Function, Metrics", "Embedding Layer, Neural Network Layers, Loss Function, Metrics"], "all_classes": null, "length": 1678}
{"pred": "        The datasets they used were the Carnegie Mellon Pronouncing Dictionary and the multilingual pronunciation corpus collected by deri2016grapheme. The corpus consists of spelling-pronunciation pairs extracted from Wiktionary. The corpus is already partitioned into training and test sets. The corpus statistics are presented in Table 10. The raw IPA transcriptions extracted from Wiktionary are cleaned to make them consistent with the phonemic inventories used in Phoible. The cleaning algorithm replaces phonemes that are not in the language's inventory with the phoneme with the most similar articulatory features that is in the language's inventory. The cleaning algorithm", "answers": ["the Carnegie Mellon Pronouncing Dictionary BIBREF12, the multilingual pronunciation corpus collected by deri2016grapheme , ranscriptions extracted from Wiktionary", "multilingual pronunciation corpus collected by deri2016grapheme"], "all_classes": null, "length": 3244}
{"pred": "        The baselines were the results reported by Khandelwal and Sawant (BIBREF12) for BERT, and the results reported by BIBREF12 for XLNet and RoBERTa. The results for RoBERTa are averaged across 5 runs for statistical significance. (BIBREF12) (BIBREF12) (BIBREF12) (BIBREF12) (BIBREF12) (BIBREF12) (BIBREF12) (BIBREF12) (BIBREF12) (BIBREF12) (BIBREF12) (BIB", "answers": ["varied from Maximum Entropy Classifiers (BIBREF4) to Support Vector Machines (BIBREF5,BIBREF6,BIBREF7,BIBREF8), Recursive Neural Networks (BIBREF9,BIBREF10), Convolutional Neural Networks (BIBREF11) and most recently transfer learning-based architectures like Bidirectional Encoder Representation from Transformers (BERT) (BIBREF12)"], "all_classes": null, "length": 2215}
{"pred": "    English, Spanish, Finnish, and other languages. They use these languages to evaluate the performance of cross-lingual models. They also use machine translation systems to translate the test sets into English and back into the target languages. (Unanswerable) (Yes) (No) (Unanswerable) (Yes) (No) (Unanswerable) (Yes) (No) (Unanswerable) (Yes) (No) (Unanswerable) (Yes) (No) (Unanswerable) (Yes) (No) (Unanswerable) (Yes) (No) (Unanswerable)", "answers": ["English\nFrench\nSpanish\nGerman\nGreek\nBulgarian\nRussian\nTurkish\nArabic\nVietnamese\nThai\nChinese\nHindi\nSwahili\nUrdu\nFinnish", "English, Spanish, Finnish"], "all_classes": null, "length": 4086}
{"pred": "    They test their method on Named Entity Recognition, POS tagging, text classification, and language modeling. They also compare their method to other state-of-the-art approaches. Their method outperforms the other approaches on these tasks. They also show that their method can be extended to other languages. Their method is robust to word segmentation errors and spelling mistakes, effectively interprets emojis and other special characters to make predictions, and performs comparably to the word-based approach for in-vocabulary tokens. Their method can be used in domains where there is a need for semantic understanding of social media, such as tracking infectious diseases. They provide an off-the-sh", "answers": ["None"], "all_classes": null, "length": 2473}
{"pred": "Yes, they use pretrained Glove embeddings for the top 20K words in the vocabulary. They initialize the embeddings of these words with 300-dimensional Glove embeddings. (unanswerable)\r\r\n\r\r\n \r\r\n\r\r\n \r\r\n\r\r\n \r\r\n\r\r\n \r\r\n\r\r\n \r\r\n\r\r\n \r\r\n\r\r\n \r\r\n\r\r\n \r\r\n\r\r\n \r\r\n\r\r\n \r\r\n\r\r\n \r\r\n\r\r\n \r\r\n\r\r\n \r\r\n\r\r\n \r\r\n\r\r\n \r\r\n\r\r\n \r\r\n\r\r\n \r\r\n\r\r\n \r\r\n\r\r\n \r\r\n\r\r\n \r\r\n\r\r\n \r\r\n\r\r\n \r\r\n\r\r\n \r\r\n\r\r\n \r\r\n\r\r\n \r\r\n\r\r\n \r\r\n\r\r\n \r\r\n\r\r\n \r\r\n\r\r\n \r\r\n\r\r\n \r\r\n\r\r\n \r\r\n\r\r\n \r\r\n\r\r\n \r\r\n\r\r\n \r\r\n\r\r\n \r\r\n\r\r\n \r\r\n\r\r\n \r\r\n\r\r\n \r\r\n\r\r\n \r\r\n\r\r\n \r\r\n\r\r\n \r\r\n\r\r\n \r\r\n\r\r\n \r\r\n\r\r\n \r\r\n\r\r\n", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4457}
{"pred": "    Yes, PolyReponse was evaluated against some baseline. The baseline was a traditional task-oriented dialogue system that relies on explicit semantic representations such as dialogue acts or slot-value ontologies. The evaluation showed that PolyReponse outperformed the baseline in terms of task completion rate and user satisfaction. The results suggest that the general approach to search-based dialogue used by PolyReponse is more effective than the traditional approach. However, more work is needed to fully evaluate the potential of PolyReponse and to compare it with other state-of-the-art systems. (unanswerable) (yes) (no) (unanswerable) (yes", "answers": ["No", "No"], "all_classes": null, "length": 2738}
{"pred": "        The authors obtain psychological dimensions of people by analyzing the language used in their blogs. They use a tool called LIWC to analyze the language and extract psychological dimensions such as positive emotions, negative emotions, and cognitive processes. They then use these dimensions to create maps that show the distribution of these dimensions across the United States. These maps can provide insights into the psychological characteristics of different regions and populations. (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable)", "answers": ["using the Meaning Extraction Method", "Unanswerable"], "all_classes": null, "length": 1440}
{"pred": "    The ML methods aim to identify the following argument components: claims, premises, backing, rebuttals, and refutations. These components are the building blocks of an argument and are used to support or attack a claim. The methods use a variety of linguistic features to identify these components in the discourse. (Section SECREF108) (Section UID115) (Section UID111) (Section UID86) (Section UID75) (Section UID73) (Section UID51) (Section UID44) (Section UID38) (Section UID31) (Section UID20) (Section UID1) (Section SECREF4)", "answers": ["claim, premise, backing, rebuttal, and refutation", "claim, premise, backing, rebuttal, refutation"], "all_classes": null, "length": 14472}
{"pred": "    Ngrams of length 1 are aligned using PARENT. The precision and recall of n-grams from the table and reference are computed separately, and then combined to obtain the final score. The length of the n-grams is determined by the INLINEFORM0 parameter, which is set to 1 by default. This means that the precision and recall of n-grams of length 1 are used to compute the final score. The use of n-grams of length 1 is motivated by the fact that they are more likely to be relevant to the task of table-to-text generation, and are less likely to be affected by paraph", "answers": ["Unanswerable", "Answer with content missing: (Parent subsections) combine precisions for n-gram orders 1-4"], "all_classes": null, "length": 3827}
{"pred": "        1,873 Twitter conversation threads, roughly 14k tweets, from a publicly available resource that were previously pre-processed and have conversation threads extracted. (Section 3.1) (Section 3.1) (Section 3.1) (Section 3.1) (Section 3.1) (Section 3.1) (Section 3.1) (Section 3.1) (Section 3.1) (Section 3.1) (Section 3.1) (Section 3.1) (Section 3.1) (Section 3.", "answers": ["1,873 Twitter conversation threads, roughly 14k tweets", "1,873 Twitter conversation threads, roughly 14k tweets"], "all_classes": null, "length": 3721}
{"pred": "    12 languages covered: English, Mandarin Chinese, Yue Chinese, Finnish, Spanish, French, Russian, Polish, Hebrew, Estonian, Welsh, and Kiswahili. These languages are typologically diverse and include under-resourced ones such as Welsh and Kiswahili. The resource covers an unprecedented amount of 1,888 word pairs, carefully balanced according to their similarity score, frequency, concreteness, part-of-speech class, and lexical field. The core Multi-SimLex resource is released with this paper, and the detailed protocol for creating it is also made available. The authors hope that this resource will", "answers": ["Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese", "Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese"], "all_classes": null, "length": 14660}
{"pred": "    Wikipedia and CMV datasets.", "answers": [" `Conversations Gone Awry' dataset, subreddit ChangeMyView", "An expanded version of the existing 'Conversations Gone Awry' dataset and the ChangeMyView dataset, a subreddit whose only annotation is whether the conversation required action by the Reddit moderators. "], "all_classes": null, "length": 4718}
{"pred": "    No, the pipeline components were not based on deep learning models. The pipeline components were based on existing NLP tools and techniques, such as part-of-speech tagging, named entity recognition, dependency parsing, and semantic role labeling. These tools and techniques are not based on deep learning models, but rather on statistical and rule-based approaches. The pipeline components were designed to be modular and language-independent, allowing for the potential application to other languages by simply changing the modules or models. The authors evaluated several existing systems and found that their proposed system outperformed them in terms of full processing pipeline for Portuguese texts. The authors also identified several areas", "answers": ["No", "No"], "all_classes": null, "length": 2276}
{"pred": "    The quality of the data is empirically evaluated by applying various sanity checks to the translations. These checks include computing sentence-level BLEU scores with the NLTK implementation between the human translations and the automatic translations produced by a state-of-the-art system, manually inspecting examples where the source transcript is identical to the translation, measuring the perplexity of the translations using a language model trained on a large amount of clean monolingual data, computing the ratio of English characters in the translations, and calculating similarity scores between transcripts and translations based on LASER cross-lingual sentence embeddings. Samples with low scores are manually inspected and sent back for translation", "answers": ["Validated transcripts were sent to professional translators., various sanity checks to the translations,  sanity check the overlaps of train, development and test sets", "computed sentence-level BLEU, We manually inspected examples where the source transcript was identical to the translation, measured the perplexity of the translations, computed the ratio of English characters in the translations, calculate similarity scores between transcripts and translations"], "all_classes": null, "length": 2435}
{"pred": "    They combine audio and text sequences in their RNN by concatenating the last hidden state of the audio-RNN with the final encoding vector of the text-RNN. The resulting vector is then passed through a feed-forward neural network to predict the emotion class. (EQREF7) (EQREF9) (EQREF10) (EQREF11) (EQREF12) (EQREF13) (EQREF14) (EQREF15) (EQREF16) (EQREF17) (EQREF18) (EQREF19) (EQREF20) (EQREF21) (EQREF22) (", "answers": ["combines the information from these sources using a feed-forward neural model", "encodes the information from audio and text sequences using dual RNNs and then combines the information from these sources using a feed-forward neural model"], "all_classes": null, "length": 3201}
{"pred": "        2.11 BLEU, 1.7 FKGL and 1.07 SARI. (The model improved by 2.11 BLEU, 1.7 FKGL and 1.07 SARI.) (The model improved by 2.11 BLEU, 1.7 FKGL and 1.07 SARI.) (The model improved by 2.11 BLEU, 1.7 FKGL and 1.07 SARI.) (The model improved by 2.11 BLEU, 1.7 FKGL and 1.07 SARI.) (The", "answers": ["For the WikiLarge dataset, the improvement over baseline NMT is 2.11 BLEU, 1.7 FKGL and 1.07 SARI.\nFor the WikiSmall dataset, the improvement over baseline NMT is  8.37 BLEU.", "6.37 BLEU"], "all_classes": null, "length": 2271}
{"pred": "        73% of the cases, the DocRepair translation was marked better than the baseline one. (Table TABREF30) (Human evaluation) (General results) (Results) (Human evaluation) (The results are provided in Table TABREF30. In about 52% of the cases annotators marked translations as having equal quality. Among the cases where one of the translations was marked better than the other, the DocRepair translation was marked better in 73% of the cases. This shows a strong preference of the annotators for corrected translations over the baseline ones.) (The results are provided in Table TABREF30. In", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3711}
{"pred": "        A tweet goes viral if it is retweeted more than 1000 times. (1) A tweet goes viral if it is retweeted more than 1000 times. (1) A tweet goes viral if it is retweeted more than 1000 times. (1) A tweet goes viral if it is retweeted more than 1000 times. (1) A tweet goes viral if it is retweeted more than 1000 times. (1) A tweet goes viral if it is retweeted more than 1000 times. (1) A tweet goes viral if it is retweeted more than 100", "answers": ["Viral tweets are the ones that are retweeted more than 1000 times", "those that contain a high number of retweets"], "all_classes": null, "length": 3144}
{"pred": "        BERT performs best by itself. (Unanswerable) (Unanswerable) (Unanswerable) (Unanswerable) (Unanswerable) (Unanswerable) (Unanswerable) (Unanswerable) (Unanswerable) (Unanswerable) (Unanswerable) (Unanswerable) (Unanswerable) (Unanswerable) (Unanswerable) (Unanswerable) (Unanswerable) (Unanswerable) (Unanswerable) (Unanswerable) (Unanswerable) (Unanswerable) (Unanswerable) (Unanswerable)", "answers": ["BERT"], "all_classes": null, "length": 1507}
{"pred": "The data was collected using crowdsourcing and an Android application. Each respondent installed the application on their personal device and recorded several phrases in different sessions. The Android application did various checks on each utterance and if it passed all of them, the respondent was directed to the next phrase. For more information about data collection scenario, please refer to BIBREF4. (BIBREF4 is the reference for the article). (BIBREF4 is the reference for the article). (BIBREF4 is the reference for the article). (BIBREF4 is the reference for the article). (BIBREF4 is the reference", "answers": ["Android application"], "all_classes": null, "length": 3795}
{"pred": "    Machine learning and deep learning methods such as logistic regression and deep neural networks are used for RQE. These methods are used to recognize entailment between two questions and retrieve answers to a new question based on the answers to the entailed questions. The choice of methods is motivated by the best performance achieved by logistic regression in question-question similarity at SemEval 2017 and the high performance achieved by neural networks on larger datasets such as SNLI. The deep learning model adapted from the neural network proposed by Bowman et al. consists of three 600d ReLU layers, with a bottom layer taking the concatenated sentence representations as input and a top", "answers": ["Logistic Regression, neural networks"], "all_classes": null, "length": 7257}
{"pred": "    The benchmark dataset is the Social Honeypot dataset, which is a collection of legitimate users and spammers collected by Lee et al. The quality of the dataset is high, as it was collected over a period of 7 months and contains detailed information about the user behavior and user profile features. However, the dataset is limited in size and may not be representative of the entire Twitter user population. Therefore, further research is needed to validate the findings on larger datasets. (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no)", "answers": ["Social Honeypot dataset (public) and Weibo dataset (self-collected); yes", "Social Honeypot, which is not of high quality"], "all_classes": null, "length": 2242}
{"pred": "    The decoder has an LSTM architecture. (single phrase) (single phrase) (single phrase) (single phrase) (single phrase) (single phrase) (single phrase) (single phrase) (single phrase) (single phrase) (single phrase) (single phrase) (single phrase) (single phrase) (single phrase) (single phrase) (single phrase) (single phrase) (single phrase) (single phrase) (single phrase) (single phrase) (single phrase) (single phrase) (single phrase) (single phrase) (single phrase) (single phrase) (single phrase) (single phrase)", "answers": ["LSTM", "LSTM"], "all_classes": null, "length": 2289}
{"pred": "    Yes, they report results only on English data. They use three datasets, one of which is a subset of GDELT Event Database, which contains news articles in English. They also use Twitter and FSD datasets, which contain tweets in English. They do not report results on non-English data. Therefore, the answer to the question is \"yes\". (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no)", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3838}
{"pred": "        BERT-based model with ensemble of Logistic Regression, CNN and BERT, with relax-voting ensemble scheme and postprocess step to check for repetition propaganda technique. It achieved F1 score of 0.673 on dev (external) set. (SLC) (4th position) (FLC) (3rd position) (3rd position) (4th position) (3rd position) (4th position) (3rd position) (4th position) (3rd position) (4th position) (3rd position) (4th position) (3rd position) (4th position)", "answers": ["For SLC task, the \"ltuorp\" team  has the best performing  model (0.6323/0.6028/0.6649 for F1/P/R  respectively) and for FLC task the \"newspeak\" team  has the best performing  model (0.2488/0.2863/0.2201 for F1/P/R respectively)."], "all_classes": null, "length": 1541}
{"pred": "    The baseline was a standard NMT model trained on the in-domain parallel data. The model achieved a BLEU score of 10.0 for the Ja-Ru translation direction. The baseline was also used as a comparison point for the other models in the paper. The baseline was a standard NMT model trained on the in-domain parallel data. The model achieved a BLEU score of 10.0 for the Ja-Ru translation direction. The baseline was also used as a comparison point for the other models in the paper. The baseline was a standard NMT model trained on the in-domain parallel data. The model achieved a BLE", "answers": ["pivot-based translation relying on a helping language BIBREF10, nduction of phrase tables from monolingual data BIBREF14 , attentional RNN-based model (RNMT) BIBREF2, Transformer model BIBREF18, bi-directional model BIBREF11, multi-to-multi (M2M) model BIBREF8, back-translation BIBREF17", "M2M Transformer"], "all_classes": null, "length": 4542}
{"pred": "    0.7033\n\nQuestion: What was their lowest precision score?\n\nAnswer: νεφοκάλυψης\n\n     νεφοκάλυψης\n\n    0.1119\n\nQuestion: What was their highest MRR score?\n\nAnswer: νεφοκάλυψης\n\n     νεφοκάλυψης\n\n    0.2862\n\nQuestion: What was their lowest MRR score?\n\nAnswer: νεφοκάλυψης\n\n     νεφοκάλυψης\n\n    0.0786\n\nQuestion: What was their highest F1 score?\n\nAnswer: νεφοκάλυψης\n\n     νεφοκάλυψης\n\n    0.2862\n\nQuestion: What was their lowest F1 score?\n\nAnswer: νεφοκάλυψης\n\n     νεφοκάλυψης\n\n    0.0786\n\nQuestion:", "answers": ["0.7033", "0.7033"], "all_classes": null, "length": 6810}
{"pred": "    Word embeddings, such as word2vec, are explored in the paper. These techniques learn a representation of a word by word co-occurrence matrix. The basic idea is that the neural network learns a series of weights that either maximizes the probability of a word given its context, referred to as the continuous bag of words (CBOW) approach, or that maximizes the probability of the context given a word, referred to as the Skip-gram approach. These approaches have been used in numerous recent papers. The paper also explores a retrofitting vector method that incorporates ontological information into a vector representation by including semantically related words", "answers": ["Skip–gram, CBOW", "integrated vector-res, vector-faith, Skip–gram, CBOW"], "all_classes": null, "length": 4259}
{"pred": "    English to Indian language word order divergence can be detrimental to the benefits of multilingual translation. To address this, they propose to pre-order the assisting language sentences to match the word order of the source language. They show that this significantly improves translation quality in an extremely low-resource setting. They also analyze the outputs and hypothesize that pre-ordering reduces the number of unknown tokens in the test output. They conclude that handling word-order divergence between source and assisting languages is crucial for the success of multilingual NMT in an extremely low-resource setting. (unanswerable) (yes) (no) (unanswerable) (yes)", "answers": ["Unanswerable", "CFILT-preorder system"], "all_classes": null, "length": 2231}
{"pred": "    Yes, the paper explores extraction from electronic health records. The authors note that a database of diseases, treatments and tests is beneficial for doctors consulting in complicated medical cases. The main problems in BioIE are similar to those in Information Extraction, and the paper discusses various methods that have been adopted to solve them. The paper also highlights the difficulty of Information Extraction tasks in the biomedical domain. The authors note that the explosion of available biomedical literature makes it impossible for one person to extract relevant relations from published material, and that automatic extraction of relations assists in the process of database creation. The paper also discusses the extraction of diseases and their relevant genes,", "answers": ["Yes"], "all_classes": null, "length": 3035}
{"pred": "        The experts used for annotation were legal experts with training in privacy policies. They identified relevant evidence within the privacy policy, as well as provided meta-annotation on the question's relevance, subjectivity, OPP-115 category, and how likely any privacy policy is to contain the answer to the question asked. Their judgements can be considered valid, legally-informed opinions even when their perspectives differ. (Section 4.2.2)\r\r\n\r\r\n        The experts used for annotation were legal experts with training in privacy policies. They identified relevant evidence within the privacy policy, as well as provided meta-annotation on the question's relevance, subjectivity", "answers": ["Individuals with legal training", "Yes"], "all_classes": null, "length": 3846}
{"pred": "    The models used for painting embedding are CNNs, and for language style transfer are seq2seq models. The models are combined using an intermediate representation. The seq2seq model performs better in practice using global attention as compared with local attention. The models are made publicly available. The future work includes experimenting with GANs and cross aligned auto-encoders. (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no)", "answers": ["generating a poem from images we use an existing actor-critic architecture, various types of sequence to sequence models"], "all_classes": null, "length": 1653}
{"pred": "    The RNN layer works better on top of BERT for long documents. The RNN layer is able to capture long-term dependencies and provide a more accurate representation of the document. The transformer layer is better suited for short documents, but it is not as effective for long documents. The RNN layer is able to handle the long-term dependencies and provide a more accurate representation of the document. The transformer layer is better suited for short documents, but it is not as effective for long documents. The RNN layer is able to handle the long-term dependencies and provide a more accurate representation of the document. The transformer layer is better suited for", "answers": ["Transformer over BERT (ToBERT)", "The transformer layer"], "all_classes": null, "length": 2655}
{"pred": "    Yes, the authors hypothesize that humans' robustness to noise is due to their general knowledge. They believe that humans can utilize general knowledge in addition to the knowledge contained in each given passage-question pair, which allows them to achieve good performance on evaluation examples without training examples. The authors also believe that humans can utilize general knowledge to overcome the limitations of MRC models, such as their hunger for data and their vulnerability to noise. They propose a data enrichment method to extract inter-word semantic connections as general knowledge from each given passage-question pair, and an end-to-end MRC model named as Knowledge Aided Reader (KAR) that", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4127}
{"pred": "        They addressed three topics of cyberbullying: personal attack, racism, and sexism. (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no)", "answers": ["personal attack, racism, and sexism", "racism, sexism, personal attack, not specifically about any single topic"], "all_classes": null, "length": 3244}
{"pred": "    They propose extended middle context, a new context representation for CNNs for relation classification. The extended middle context uses all parts of the sentence (the relation arguments, left of the relation arguments, between the arguments, right of the arguments) and pays special attention to the middle part. (1) They propose extended middle context, a new context representation for CNNs for relation classification. The extended middle context uses all parts of the sentence (the relation arguments, left of the relation arguments, between the arguments, right of the arguments) and pays special attention to the middle part. (2) They present connectionist bi-directional RNN", "answers": ["They use two independent convolutional and max-pooling layers on (1) a combination of the left context, the left entity and the middle context; and (2) a combination of the middle context, the right entity and the right context. They concatenated the two results after pooling to get the new context representation."], "all_classes": null, "length": 2435}
{"pred": "        4 (PER, LOC, ORG, MISC)       \n\n        4 (PER, LOC, ORG, MISC)       \n\n        4 (PER, LOC, ORG, MISC)       \n\n        4 (PER, LOC, ORG, MISC)       \n\n        4 (PER, LOC, ORG, MISC)       \n\n        4 (PER, LOC, ORG, MISC)       \n\n        4 (PER, LOC, ORG, MISC)       \n\n        4 (PER, LOC, ORG, MISC)       \n\n", "answers": ["OurNepali contains 3 different types of entities, ILPRL contains 4 different types of entities", "three"], "all_classes": null, "length": 2851}
{"pred": "    The resulting annotated data is of higher quality, as the expert annotations are more consistent and accurate than the crowd annotations. The expert annotations also provide more detailed and nuanced information, which can be useful for downstream tasks such as information extraction. The higher quality of the expert annotations can lead to better performance in downstream tasks, such as improved accuracy and precision. However, the cost of obtaining expert annotations can be high, so it may not always be feasible to use them in practice. In some cases, it may be more cost-effective to use a combination of expert and crowd annotations, or to use crowd annotations alone. The optimal approach will depend on", "answers": ["improvement when the difficult subset with expert annotations is mixed with the remaining crowd annotation is 3.5 F1 score, much larger than when a random set of expert annotations are added"], "all_classes": null, "length": 4399}
{"pred": "        65% of speakers are men, speaking more than 75% of the time. (unanswerable) (yes) (no) (unanswerable) (no) (unanswerable) (no) (unanswerable) (no) (unanswerable) (no) (unanswerable) (no) (unanswerable) (no) (unanswerable) (no) (unanswerable) (no) (unanswerable) (no) (unanswerable) (no) (unanswerable) (no) (unanswerable) (no) (un", "answers": ["Women represent 33.16% of the speakers"], "all_classes": null, "length": 4055}
{"pred": "        Multi30K dataset\n\n        Multi30K dataset\n\n    </p>\n\n    <p>\n\n        The approach achieves state of the art results on the Multi30K dataset for English-German translation.\n\n    </p>\n\n    <p>\n\n        The approach uses a deliberation network to explore textual context and a multimodal model to incorporate visual information. The results show that the approach is robust to noisy input and can handle language pairs that require substantial restructuring from source to target.\n\n    </p>\n\n    <p>\n\n        The approach is promising for machine translation that is robust to noisy input and can handle language pairs that require substantial restructuring from", "answers": ["the English-German dataset"], "all_classes": null, "length": 1833}
{"pred": "    The strong baselines model is compared to the proposed model in terms of performance and decoding time. The proposed model outperforms the strong baselines model in terms of performance and decoding time. The proposed model uses only unigram features instead of multiple n-gram features in previous work, which makes it more efficient and effective. The proposed model also uses a variant of self-attention called Gaussian-masked directional multi-head attention to improve the ability of capturing the localness and directional information of self-attention based encoder. The proposed model is evaluated on standard benchmark dataset, SIGHAN Bakeoff 2005, which shows not only the", "answers": ["Baseline models are:\n- Chen et al., 2015a\n- Chen et al., 2015b\n- Liu et al., 2016\n- Cai and Zhao, 2016\n- Cai et al., 2017\n- Zhou et al., 2017\n- Ma et al., 2018\n- Wang et al., 2019"], "all_classes": null, "length": 3629}
{"pred": "    Event detection on microblogging platforms such as Twitter aims to detect events preemptively. A main task in event detection is detecting events of predetermined types BIBREF0, such as concerts or controversial events based on microposts matching specific event descriptions. This task has extensive applications ranging from cyber security BIBREF1, BIBREF2 to political elections BIBREF3 or public health BIBREF4, BIBREF5. Due to the high ambiguity and inconsistency of the terms used in microposts, event detection is generally performed though statistical machine learning models, which require a labeled dataset for model training. Data", "answers": ["probabilistic model", "Logistic Regression, Multilayer Perceptron"], "all_classes": null, "length": 4475}
{"pred": "    NLP toolkits typically have the following capabilities: tokenization, part-of-speech (PoS) tagging, chunking, named entity recognition and sentiment analysis. In a study by BIBREF3, it is shown that the well-known NLP toolkits NLTK BIBREF4, Stanford CoreNLP BIBREF5, and TwitterNLP BIBREF6 have tokenization, PoS tagging and NER modules in their pipelines. There are two main approaches for NER: (1) rule-based and (2) statistical or machine learning based. The most ubiquitous algorithms for sequence tagging use Hidden Markov", "answers": ["BIBREF17, BIBREF18, TensiStrength BIBREF13, TwitterNLP BIBREF6, BIBREF19, CogComp-NLP BIBREF20, Stanford NLP NER BIBREF21", "BIBREF23, BIBREF17, BIBREF18, BIBREF19, BIBREF24, BIBREF25, BIBREF26"], "all_classes": null, "length": 1452}
{"pred": "    SQuAD dataset\n\nQuestion: What are the main results of the experiments?\n\nAnswer: 노출등록\n\n     노출등록\n\n    The proposed model achieves significant improvements over proximity-based answer-aware models on both dataset splits. The structured answer-relevant relations can capture both short and long term answer-relevant dependencies of the answer in sentences. The improvements become more significant when the distance between the answer fragment and other non-stop sentence words that also appear in the ground truth question is large. The proposed model can generate diverse questions with different structured answer-relevant relations. The proposed model can also be generalized in other conditional sequence generation tasks which require multiple sources of inputs", "answers": ["SQuAD", "SQuAD"], "all_classes": null, "length": 3757}
{"pred": "    Existing approaches for modelling urban environments and identifying points-of-interest and itineraries have been proposed, but the usefulness of Flickr for characterizing the natural environment is less well-understood. Recent studies have highlighted that Flickr tags capture valuable ecological information, but ecologists have mostly used social media to conduct manual evaluations of image content with little automated exploitation of the associated tags. One recent exception is a bag-of-words representation derived from Flickr tags that gives promising results for predicting a range of different environmental phenomena. However, the ecological information implicitly captured by Flickr tags can be utilized in a more effective way using vector space embeddings instead of bag-of-words", "answers": ["BOW-Tags, BOW-KL(Tags), BOW-All, GloVe"], "all_classes": null, "length": 4658}
{"pred": "    Yes, they use attention to compute the similarity score between passages and questions. The attention function is used to generate a working memory by fusing information from both passages and questions. The attention function is also used to apply self attention to the passage to generate a question-aware passage representation. The working memory and question-aware passage representation are then passed through a BiLSTM to form the final memory. The final memory is used to predict the answer span and whether the question is unanswerable. The model is trained jointly to optimize both tasks. The results show that the joint learning algorithm boosts the performance on SQuAD 2.0.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 1687}
{"pred": "    CSAT, 20newsgroups, and Fisher datasets. (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable", "answers": ["CSAT dataset, 20 newsgroups, Fisher Phase 1 corpus", "CSAT dataset , 20 newsgroups, Fisher Phase 1 corpus"], "all_classes": null, "length": 2652}
{"pred": "    IMDb movie review dataset\n\nQuestion: What is the average document length of the IMDb movie review dataset?\n\nAnswer: 231 words\n\nQuestion: What is the best performance on the IMDb development set achieved using a four-layer densely-connected QRNN with 256 units per layer and word vectors initialized using 300-dimensional cased GloVe embeddings?\n\nAnswer: 0.85 accuracy\n\nQuestion: What is the speedup on IMDb train time per epoch compared to the optimized LSTM implementation provided in NVIDIA's cuDNN library?\n\nAnswer: 3.2x\n\nQuestion: What is the speed gain for specific batch sizes and sequence lengths?\n\nAnswer", "answers": ["the IMDb movie review dataset BIBREF17", "IMDb movie review"], "all_classes": null, "length": 3432}
{"pred": "        Yes, previous work has evaluated these tasks in the context of LSTM-based models. The BERT model is based on the Transformer architecture, which relies purely on attention mechanisms, and does not have an explicit notion of word order beyond marking each word with its absolute-position embedding. This reliance on attention may lead one to expect decreased performance on syntax-sensitive tasks compared to RNN (LSTM) models that do model word order directly, and explicitly track states across the sentence. Indeed, previous work finds that transformer-based models perform worse than LSTM models on the agreement prediction dataset. However, recent work has shown that self-attention performs on par with", "answers": ["Yes", "Yes"], "all_classes": null, "length": 1464}
{"pred": "    Yes, the datasets for sentiment analysis are balanced. The datasets contain a roughly equal number of positive, negative, and neutral sentiments. This ensures that the machine learning models trained on these datasets are not biased towards any particular sentiment. The balanced datasets also help to improve the overall accuracy of the models. (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (un", "answers": ["No"], "all_classes": null, "length": 1441}
{"pred": "    The invertibility condition is that the neural projector is invertible, meaning that it has an inverse function that can be computed efficiently. This condition allows for tractable exact inference and marginal likelihood computation. (EQREF19) (EQREF22) (FIGREF24) (EQREF20) (EQREF22) (FIGREF24) (EQREF19) (EQREF22) (FIGREF24) (EQREF20) (EQREF22) (FIGREF24) (EQREF19) (EQREF22) (FIGREF24) (EQREF20) (EQREF22) (FIGREF", "answers": ["The neural projector must be invertible.", "we constrain our neural projector with two requirements: (1) INLINEFORM0 and (2) INLINEFORM1 exists"], "all_classes": null, "length": 4323}
{"pred": "    The proposed qualitative annotation schema looks like a high-level taxonomy of dimensions of interest, including linguistic complexity, required reasoning, and factual correctness. It also includes a metric based on lexical cues to approximate the complexity of the reading comprehension task. The full catalogue of features, their description, detailed annotation guidelines, and illustrating examples can be found in the Appendix. (Figure 10) (Appendix) (Figure 10) (Appendix) (Figure 10) (Appendix) (Figure 10) (Appendix) (Figure 10) (Appendix) (Figure 10) (Appendix) (Figure 10", "answers": ["The resulting taxonomy of the framework is shown in Figure FIGREF10", "FIGREF10"], "all_classes": null, "length": 4958}
{"pred": "        WikiSmall: 89,042 sentence pairs, WikiLarge: 296,402 sentence pairs\n\n        Unanswerable\n\n        Yes\n\n        No\n\n        Unanswerable\n\n        No\n\n        Unanswerable\n\n        No\n\n        Unanswerable\n\n        No\n\n        Unanswerable\n\n        No\n\n        Unanswerable\n\n        No\n\n        Unanswerable\n\n        No\n\n        Unanswerable\n\n        No\n\n        Unanswerable\n\n        No\n\n        Unanswerable\n\n        No\n\n        Unanswerable\n\n        No\n\n        Unanswerable\n\n        No\n\n        Unanswerable\n\n        No\n\n", "answers": ["training set has 89,042 sentence pairs, and the test set has 100 pairs, training set contains 296,402, 2,000 for development and 359 for testing", "WikiSmall  89 142 sentence pair and  WikiLarge 298 761 sentence pairs. "], "all_classes": null, "length": 2266}
{"pred": "    Vanilla ST baseline, Encoder pre-training, Decoder pre-training, Encoder-decoder pre-training, One-to-many setting, Many-to-one setting, Many-to-many setting, Many-to-many+pre-training, Triangle+pre-train\n\nQuestion: What are the experimental results?\n\nAnswer: 노출등록\n\n     노출등록\n\n        Subword-level: 3.6 BLEU score improvement, Character-level: 2.2 BLEU score improvement\n\n        Test sets: tst2010, tst2013, tst2014, tst2015, Average performance: 3.6 BLEU score improvement\n\n        Baselines: Many-to-many+", "answers": ["Vanilla ST baseline, encoder pre-training, in which the ST encoder is initialized from an ASR model, decoder pre-training, in which the ST decoder is initialized from an MT model, encoder-decoder pre-training, where both the encoder and decoder are pre-trained, many-to-many multi-task model where the encoders and decoders are derived from pre-trained ASR and MT models, Triangle+pre-train: BIBREF18 DBLP:conf/naacl/AnastasopoulosC18 proposed a triangle multi-task strategy for speech translation", "Vanilla ST baseline, Pre-training baselines, Multi-task baselines, Many-to-many+pre-training, Triangle+pre-train", "Vanilla ST baseline: The vanilla ST BIBREF9 has only a speech encoder and a decoder. It is trained from scratch on the ST-TED corpus.\n\nPre-training baselines: We conduct three pre-training baseline experiments: 1) encoder pre-training, in which the ST encoder is initialized from an ASR model; 2) decoder pre-training, in which the ST decoder is initialized from an MT model; and 3) encoder-decoder pre-training, where both the encoder and decoder are pre-trained. The ASR model has the same architecture with vanilla ST model, trained on the mixture of ST-TED and TED-LIUM2 corpus. The MT model has a text encoder and decoder with the same architecture of which in TCEN. It is first trained on WMT data (out-of-domain) and then fine-tuned on in-domain data.\n\nMulti-task baselines: We also conduct three multi-task baseline experiments including one-to-many setting, many-to-one setting, and many-to-many setting. In the first two settings, we train the model with $\\alpha _{st}=0.75$ while $\\alpha _{asr}=0.25$ or $\\alpha _{mt}=0.25$. For many-to-many setting, we use $\\alpha _{st}=0.6, \\alpha _{asr}=0.2$ and $\\alpha _{mt}=0.2$.. For MT task, we use only in-domain data.\n\nMany-to-many+pre-training: We train a many-to-many multi-task model where the encoders and decoders are derived from pre-trained ASR and MT models. "], "all_classes": null, "length": 4704}
{"pred": "        English, Arabic, and other languages that use the Latin alphabet. The paper also mentions that the techniques can be applied to other languages as well. (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no", "answers": ["Unanswerable", "English"], "all_classes": null, "length": 4513}
{"pred": "    SVM, BiLSTM, CNN\n\n        </div>\n\n        </div>\n\n        </div>\n\n        </div>\n\n        </div>\n\n        </div>\n\n        </div>\n\n        </div>\n\n        </div>\n\n        </div>\n\n        </div>\n\n        </div>\n\n        </div>\n\n        </div>\n\n        </div>\n\n        </div>\n\n        </div>\n\n        </div>\n\n        </div>\n\n        </div>\n\n        </div>\n\n        </div>\n\n        </div>\n\n        </div>\n\n        </div>\n\n        </div>\n\n        </div>\n\n        </div>\n\n        </div>\n\n        </div", "answers": ["linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM trained on word unigrams,  bidirectional Long Short-Term-Memory (BiLSTM),  Convolutional Neural Network (CNN) "], "all_classes": null, "length": 2250}
{"pred": "        The answered questions measure for the usefulness of the answer. The usefulness of the answer is measured by the number of upvotes and downvotes received by the answer. The more upvotes an answer receives, the more useful it is considered to be. The more downvotes an answer receives, the less useful it is considered to be. The usefulness of the answer is also measured by the number of views and comments received by the answer. The more views and comments an answer receives, the more useful it is considered to be. The usefulness of the answer is also measured by the number of times the answer is accepted as the best answer. The", "answers": ["No"], "all_classes": null, "length": 1561}
{"pred": "    GloVe and Edinburgh embeddings were used. The Edinburgh embeddings were obtained by training a skip-gram model on the Edinburgh corpus. The GloVe embeddings were trained on 2 billion tweets. Both embeddings were used to obtain word vectors for each tweet, which were then summed and divided by the number of tokens in the tweet to obtain the final tweet embedding. The use of pretrained word embeddings is common in NLP tasks and can improve performance by capturing semantic relationships between words. The choice of embeddings used in this task was likely informed by the specific characteristics of the Twitter dataset, such as the abundance of emojis and the informal nature of the language used", "answers": ["Pretrained word embeddings  were not used", "GloVe, Edinburgh embeddings BIBREF14, Emoji embeddings BIBREF16"], "all_classes": null, "length": 1771}
{"pred": "        The personalized generative models were able to generate plausible, personalized, and coherent recipes that were preferred by human evaluators for consumption. The models also introduced a set of automatic coherence measures for instructional texts as well as personalization metrics to support their claims. The results show that personalization can improve the semantic plausibility of generated recipes. (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable)", "answers": ["average recipe-level coherence scores of 1.78-1.82, human evaluators preferred personalized model outputs to baseline 63% of the time"], "all_classes": null, "length": 2666}
{"pred": "        The combination of rewards for reinforcement learning is a harmonic mean of irony reward and sentiment reward. The irony reward is the difference between the irony score of the input sentence and that of the output sentence, while the sentiment reward is one minus the absolute value of the difference between the standardized sentiment score of the input sentence and that of the generated sentence. The harmonic mean encourages the model to focus on both the irony accuracy and the sentiment preservation. (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes)", "answers": ["irony accuracy, sentiment preservation", " irony accuracy and sentiment preservation"], "all_classes": null, "length": 4592}
{"pred": "        Since we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer as shown in Figure FIGREF12 for \"Starry Night\" with a low average content score. This happens when the style transfer dataset does not have similar words in the training set of sentences. A solution would be to expand the style transfer dataset, for a better representation of the poem data.       \n\n        Since we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer as shown in Figure FIGREF12 for \"Starry Night\" with a low average content score", "answers": ["Since we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer", "we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer as shown in Figure FIGREF12 for \"Starry Night\" with a low average content score"], "all_classes": null, "length": 1651}
{"pred": "    The existing benchmarks they compared to are the Affective Text dataset, the Fairy Tales dataset, and the ISEAR dataset. These datasets are commonly used for the evaluation of emotion detection systems. The Affective Text dataset contains news headlines annotated with emotions and valence, while the Fairy Tales dataset contains sentences from fairy tales annotated with emotions. The ISEAR dataset contains reports from psychology questionnaires answered by people with different cultural backgrounds, annotated with emotions. The authors used these datasets to compare their model's performance to state-of-the-art results. (Section SECREF3) (Section SECREF5) (Section SECREF3) (", "answers": ["Affective Text, Fairy Tales, ISEAR", " Affective Text dataset, Fairy Tales dataset, ISEAR dataset"], "all_classes": null, "length": 3390}
{"pred": "        The sample collected consisted on 1 785 855 tweets published by 848 196 different users. Within our sample, we identified 1327 tweets that went viral (retweeted more than 1000 times by the 8th of November 2016) produced by 643 users. Such small subset of viral tweets were retweeted on 290 841 occasions in the observed time-window. The 1327 `viral' tweets were manually annotated as containing fake news or not. The annotation was carried out by a single person in order to obtain a consistent annotation throughout the dataset. Out of those 1327 tweets,", "answers": ["Distributions of Followers, Friends and URLs are significantly different between the set of tweets containing fake news and those non containing them, but for Favourites, Mentions, Media, Retweets and Hashtags they are not significantly different"], "all_classes": null, "length": 3164}
{"pred": "    The dataset of hashtags is sourced from the Stanford Sentiment Analysis Dataset. The dataset consists of 1,108 unique English hashtags from 1,268 randomly selected tweets along with their crowdsourced segmentations and additional corrections. The dataset is curated by Bansal et al. (2015) and is publicly available. The dataset is used to train and evaluate hashtag segmentation models. The dataset is also used to train and evaluate downstream tasks such as sentiment analysis. The dataset is a valuable resource for researchers and practitioners interested in hashtag segmentation and related tasks. The dataset is available for download and use under a Creative Commons Attribution-NonCommercial-NoDer", "answers": ["1,268 randomly selected tweets in the Stanford Sentiment Analysis Dataset BIBREF36, all 12,594 unique English hashtags and their associated tweets from the same Stanford dataset", "Stanford Sentiment Analysis Dataset BIBREF36"], "all_classes": null, "length": 3756}
{"pred": "The DeepMine database contains speech from a variety of accents, including Persian, English, and other languages. The database was collected using crowdsourcing, and the respondents were asked to record their speech in their native language. The database includes speech from a wide range of speakers, including both native and non-native speakers, and the accents present in the corpus reflect this diversity. The database also includes speech from speakers with different levels of education and socioeconomic status, which can affect the accents present in the corpus. Overall, the DeepMine database contains a diverse range of accents, making it a valuable resource for researchers interested in studying the effects of accents on speech", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3794}
{"pred": "    A compact, scalable, and meaningful representation of a set of word vectors that retains most of the variability of the features. (1) A compact, scalable, and meaningful representation of a set of word vectors that retains most of the variability of the features. (2) A low-dimensional linear subspace in a word vector space with high dimensionality. (3) A low-dimensional linear subspace in a word vector space with high dimensionality that can effectively and compactly represent the context of the corresponding text. (4) A low-dimensional linear subspace in a word vector space with high dimensionality that can effectively and compactly represent", "answers": ["Word vectors, usually in the context of others within the same class"], "all_classes": null, "length": 5151}
{"pred": "    The baseline model used is the one proposed by Dunietz and Gillick (BIBREF11). It uses a variety of features to measure the salience of an entity in text. The features include positional features, occurrence frequency, and the internal POS structure of the entity and the sentence it occurs in. (BIBREF11) (BIBREF11) (BIBREF11) (BIBREF11) (BIBREF11) (BIBREF11) (BIBREF11) (BIBREF11) (BIBREF11) (BIBREF11) (BIBREF11", "answers": ["For Article-Entity placement, they consider two baselines: the first one using only salience-based features, and the second baseline checks if the entity appears in the title of the article. \n\nFor Article-Section Placement, they consider two baselines: the first picks the section with the highest lexical similarity to the article, and the second one picks the most frequent section.", "B1. The first baseline uses only the salience-based features by Dunietz and Gillick BIBREF11 ., B2. The second baseline assigns the value relevant to a pair INLINEFORM0 , if and only if INLINEFORM1 appears in the title of INLINEFORM2 .\n\n, S1: Pick the section from template INLINEFORM0 with the highest lexical similarity to INLINEFORM1 : S1 INLINEFORM2, S2: Place the news into the most frequent section in INLINEFORM0"], "all_classes": null, "length": 7891}
{"pred": "    Yes, SemCor3.0 is reflective of English language data in general. It is a large corpus manually annotated with WordNet sense for WSD, and it is used as training corpus in many WSD studies. The dataset is widely used and considered to be representative of English language data. Therefore, the results obtained on SemCor3.0 can be generalized to other English language data. However, it is important to note that the dataset may not be representative of all languages and domains, and the results may not be directly applicable to other languages or domains. It is always important to consider the limitations of the dataset and the general", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 2000}
{"pred": "    Augmented LibriSpeech dataset is 960 hours of speech data. It is a large dataset that can be used for training speech recognition models. The dataset is available for download and can be used for research and development purposes. It is a valuable resource for the speech recognition community. (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable)", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 2410}
{"pred": "        The dataset for fine-grained classification is split in training, development, development_test and test parts. In the rest, we refer to these splits as train, development and test, where train is composed by the training and the development instances. Table TABREF7 presents an overview of the data. As discussed in BIBREF16 and illustrated in the Table, the fine-grained dataset is highly unbalanced and skewed towards the positive sentiment: only INLINEFORM0 of the training examples are labeled with one of the negative classes. (The dataset for fine-grained classification is split in training, development, development_test and test parts. In", "answers": [" high-quality datasets  from SemEval-2016 “Sentiment Analysis in Twitter” task", " SemEval-2016 “Sentiment Analysis in Twitter”"], "all_classes": null, "length": 2738}
{"pred": "        Yes, they use the uncased BERT$_\\mathrm {BASE}$ model, which has 12 Transformer blocks, 768 hidden layer, 12 self-attention heads, and 110M parameters. They find that BERT$_\\mathrm {LARGE}$ performs slightly worse than BERT$_\\mathrm {BASE}$ in this task. (Note: The BERT model is a language representation model that uses a multi-layer bidirectional Transformer encoder. It is pre-trained on a large corpus and two novel unsupervised prediction tasks, i.e., masked language model and next sentence prediction tasks. When incorporating BERT into downstream", "answers": ["small BERT", "small BERT"], "all_classes": null, "length": 1999}
{"pred": "    Yes, the datasets are subject to quality control through crowd-sourcing experiments that show high agreement across probes and correlation with model trends. The datasets are also carefully constructed to manipulate and control the complexity of target questions, allowing for more controlled experimentation and new forms of evaluation. However, the quality of the datasets is still subject to potential biases and errors in the underlying expert knowledge resources. The positive results of the probing studies should be taken with a grain of salt, as there is much room for improvement and the models are not always consistent and robust. (unanswerable) (yes) (no) (unanswerable) (yes)", "answers": ["No", "No"], "all_classes": null, "length": 6391}
{"pred": "    Yes, the images are from a specific domain. The article mentions that the images are from the ShapeWorld framework, which is a controlled data generation framework consisting of abstract colored shapes. The images are used to evaluate image captioning models. The article does not mention any specific domain, but the ShapeWorld framework is designed to generate images and captions for visually grounded language understanding. Therefore, the images are likely from a domain related to visual perception and language understanding. However, the article does not provide any further details about the specific domain. Therefore, the answer to the question is \"yes\", but the specific domain is not mentioned. (un", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3472}
{"pred": "    The performance of the model on emotion detection was competitive, achieving state-of-the-art results for some emotion labels on standard evaluation datasets. The model relied entirely on automatically obtained information, both in terms of training data and features, making it independent of any manual annotation or handcrafted resource. The approach also has the potential to tackle domain adaptation via the selection of Facebook pages to be used as training data. (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable", "answers": ["Answer with content missing: (Table 3) Best author's model B-M average micro f-score is 0.409, 0.459, 0.411 on Affective, Fairy Tales and ISEAR datasets respectively. "], "all_classes": null, "length": 3410}
{"pred": "        The tagging scheme employed is a simple one consisting of two tags: {INLINEFORM0} and {INLINEFORM1}. The tag {INLINEFORM0} indicates that the current word is not a pun, while the tag {INLINEFORM1} indicates that the current word is a pun. If the tag sequence of a sentence contains a {INLINEFORM0} tag, then the text contains a pun and the word corresponding to {INLINEFORM1} is the pun. The contexts have the characteristic that each context contains a maximum of one pun. To capture this interesting property, a new tagging scheme consisting of three tags is proposed: {", "answers": ["A new tagging scheme that tags the words before and after the pun as well as the pun words.", "a new tagging scheme consisting of three tags, namely { INLINEFORM0 }"], "all_classes": null, "length": 2974}
{"pred": "Yes, Arabic is one of the 11 languages in CoVost. The other languages are French, German, Dutch, Russian, Spanish, Italian, Turkish, Persian, Swedish, Mongolian, and Chinese. The corpus is diversified with over 11,000 speakers and over 60 accents. It includes a total of 708 hours of speech in 11 languages, with French and German having the largest durations among existing public corpora. The corpus is also evaluated on an additional evaluation corpus from Tatoeba, resulting in a total of 9.3 hours of speech. Both corpora are created at the sentence level and do not", "answers": ["No", "No"], "all_classes": null, "length": 2413}
{"pred": "    A model is considered robust if it is not sensitive to the prior knowledge provided to it. The model should be able to handle the bias in the prior knowledge and still perform well. The model should also be able to handle the situation where the prior knowledge is not available for some classes. The model should be able to make predictions based on the available information and not be misled by the prior knowledge. The model should be able to handle the situation where the prior knowledge is not accurate or complete. The model should be able to handle the situation where the prior knowledge is not available for some classes. The model should be able to handle the situation", "answers": ["ability to accurately classify texts even when the amount of prior knowledge for different classes is unbalanced, and when the class distribution of the dataset is unbalanced", "Low sensitivity to bias in prior knowledge"], "all_classes": null, "length": 3609}
{"pred": "    InferSent, Universal Sentence Encoder, and average GloVe embeddings. The results show that SBERT outperforms these methods on several tasks. (Unanswerable) (Yes) (No) (Unanswerable) (Yes) (No) (Unanswerable) (Yes) (No) (Unanswerable) (Yes) (No) (Unanswerable) (Yes) (No) (Unanswerable) (Yes) (No) (Unanswerable) (Yes) (No) (Unanswerable) (Yes) (No) (Unanswerable) (Yes) (No)", "answers": ["GloVe, BERT, Universal Sentence Encoder, TF-IDF, InferSent", "Avg. GloVe embeddings, Avg. fast-text embeddings, Avg. BERT embeddings, BERT CLS-vector, InferSent - GloVe and Universal Sentence Encoder."], "all_classes": null, "length": 3862}
{"pred": "        +0.29 and +0.96 for English datasets, +0.97 and +2.36 for Chinese datasets. (English: CoNLL2003, OntoNotes5.0; Chinese: MSRA, OntoNotes4.0) (F1: +0.29, +0.96, +0.97, +2.36) (Datasets: CoNLL2003, OntoNotes5.0, MSRA, OntoNotes4.0) (Improvements: +0.29, +0.96, +0.97, +2.", "answers": ["English datasets including CoNLL2003 and OntoNotes5.0, our proposed method outperforms BERT-MRCBIBREF38 by +0.29 and +0.96 respectively, Chinese datasets, achieving F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively", "For English datasets including CoNLL2003 and OntoNotes5.0, our proposed method outperforms BERT-MRCBIBREF38 by +0.29 and +0.96 respectively., huge performance boosts on Chinese datasets, achieving F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively"], "all_classes": null, "length": 3586}
{"pred": "        They test their conflict method on two tasks: Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask. They use the same model architecture for both tasks, but the input and output are different. For Quora Duplicate Question Pair Detection, they use a pair of questions as input and predict whether they are duplicates or not. For Ranking questions in Bing's People Also Ask, they use a user query and a candidate question as input and predict whether the candidate question is a high-click question or not. They use the same model architecture for both tasks, but the input and output are different. For Quora Duplicate Question Pair", "answers": ["Task 1: Quora Duplicate Question Pair Detection, Task 2: Ranking questions", "Quora Duplicate Question Pair Detection, Ranking questions in Bing's People Also Ask"], "all_classes": null, "length": 2577}
{"pred": "    Previous syntactic tree-based models and other neural models\n\nConclusion: The SATA Tree-LSTM outperforms previous syntactic tree-based models and other neural models on several sentence-level tasks, demonstrating the effectiveness of linguistic priors in neural models for sentence representations. The model's performance is attributed to the dynamic compositionality provided by the tag-level tree-LSTM and the proper contextualization of input word vectors. Future work could explore new ways of exploiting dependency trees effectively, similar to the case of constituency trees. (unanswerable)\n\nQuestion: What is the main contribution of the paper?\n\nAnswer: The main contribution of the paper is a novel", "answers": ["Various tree structured neural networks including variants of Tree-LSTM, Tree-based CNN, RNTN, and non-tree models including variants of LSTMs, CNNs, residual, and self-attention based networks", "Sentence classification baselines: RNTN (Socher et al. 2013), AdaMC-RNTN (Dong et al. 2014), TE-RNTN (Qian et al. 2015), TBCNN (Mou et al. 2015), Tree-LSTM (Tai, Socher, and Manning 2015), AdaHT-LSTM-CM (Liu, Qiu, and Huang 2017), DC-TreeLSTM (Liu, Qiu, and Huang 2017), TE-LSTM (Huang, Qian, and Zhu 2017), BiConTree (Teng and Zhang 2017), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), TreeNet (Cheng et al. 2018), CNN (Kim 2014), AdaSent (Zhao, Lu, and Poupart 2015), LSTM-CNN (Zhou et al. 2016), byte-mLSTM (Radford, Jozefowicz, and Sutskever 2017), BCN + Char + CoVe (McCann et al. 2017), BCN + Char + ELMo (Peters et al. 2018). \nStanford Natural Language Inference baselines: Latent Syntax Tree-LSTM (Yogatama et al. 2017), Tree-based CNN (Mou et al. 2016), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), NSE (Munkhdalai and Yu 2017), Reinforced Self- Attention Network (Shen et al. 2018), Residual stacked encoders: (Nie and Bansal 2017), BiLSTM with generalized pooling (Chen, Ling, and Zhu 2018)."], "all_classes": null, "length": 4781}
{"pred": "    Relation Detection\n\nExplanation: The core component for KBQA is relation detection, which is the process of identifying the relevant relations between entities in the knowledge base. This is a crucial step in KBQA, as it allows the system to retrieve the correct answers from the knowledge base. The relation detection component is responsible for matching the question to the relevant relations in the knowledge base, and for generating the query that will be used to retrieve the answers. The quality of the relation detection component directly impacts the performance of the KBQA system, as it determines the accuracy of the answers retrieved from the knowledge base. Therefore, improving the relation detection component is", "answers": ["answer questions by obtaining information from KB tuples ", "hierarchical matching between questions and relations with residual learning"], "all_classes": null, "length": 4527}
{"pred": "    The baseline models are a name-based Nearest-Neighbor model (NN) and an Encoder-Decoder baseline with ingredient attention (Enc-Dec). Both models are used to compare the performance of the personalized models against. The NN model uses the recipe name to find the most similar recipe in the dataset, while the Enc-Dec model uses an attention mechanism to focus on the ingredients mentioned in the input. Both models are used to generate recipes from incomplete input specifications. The personalized models outperform the baseline models in terms of BPE perplexity, but perform worse in terms of BLEU-1 and BLEU-4 scores. The personalized models also", "answers": ["name-based Nearest-Neighbor model (NN), Encoder-Decoder baseline with ingredient attention (Enc-Dec)"], "all_classes": null, "length": 2655}
{"pred": "        Manual detection of biases and unwarranted inferences in the Flickr30K dataset, including part-of-speech tagging and coreference graph analysis. davidjl\n\n        Manual detection of biases and unwarranted inferences in the Flickr30K dataset, including part-of-speech tagging and coreference graph analysis. davidjl\n\n        Manual detection of biases and unwarranted inferences in the Flickr30K dataset, including part-of-speech tagging and coreference graph analysis. davidjl\n\n        Manual detection of biases and unwarranted inferences in the Flickr30K dataset, including part-of-speech tagging and coreference graph analysis. davidjl\n\n", "answers": ["spot patterns by just looking at a collection of images, tag all descriptions with part-of-speech information, I applied Louvain clustering", "Looking for adjectives marking the noun \"baby\" and also looking for most-common adjectives related to certain nouns using POS-tagging"], "all_classes": null, "length": 2204}
{"pred": "        They explore Romance languages and Semitic languages. (Romance languages: French, Spanish, Italian, Portuguese; Semitic languages: Arabic, Hebrew) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (", "answers": ["English, French, German ", "French, English, Spanish, Italian, Portuguese, Hebrew, Arabic"], "all_classes": null, "length": 2285}
{"pred": "    They experimented with Cell-aware Stacked LSTMs (CAS-LSTMs) and stacked LSTMs. They also considered variants of CAS-LSTMs with different configurations. (i) models that use plain stacked LSTMs, (ii) models with different INLINEFORM0, (iii) models without INLINEFORM1, and (iv) models that integrate lower contexts via peephole connections. (i) models that use plain stacked LSTMs, (ii) models with different INLINEFORM0, (iii) models without INLINEFORM1, and (iv) models that integrate lower contexts via peephole connections.", "answers": ["Stacked LSTMs, Cell-aware Stacked LSTMs, Sentence Encoders, Top-layer Classifiers"], "all_classes": null, "length": 3210}
{"pred": "        Yes, they report results only on English data. They use a snapshot of English Wikipedia measuring 8GB in size, with the stop-words filtered out. The algorithm is trained over 20 iterations. The vocabulary size is 287,847. The proposed method is compared with previous studies that aim to obtain interpretable word vectors. The results are evaluated qualitatively and quantitatively. The performance of the embeddings on word similarity and word analogy tests is also evaluated. The proposed method significantly improves the interpretability of the embeddings compared to the original GloVe approach. The semantic coherence of the original vector space is preserved, even slightly", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 6169}
{"pred": "    The authors experimented with a few summarization algorithms provided by the Sumy package, including TextRank, LexRank, and SumBasic. They also compared the performance of their ILP-based summarization algorithm with these algorithms. The results showed that the performance of their ILP-based summarization algorithm was comparable with the other algorithms, as the two sample t-test did not show statistically significant difference. Additionally, human evaluators preferred the phrase-based summary generated by their approach to the other sentence-based summaries. (Note: This is a paraphrased answer, not a direct quote from the article.) (Note: This is a paraphrased", "answers": ["LSA, TextRank, LexRank and ILP-based summary.", "LSA, TextRank, LexRank"], "all_classes": null, "length": 3045}
{"pred": "    The previous state of the art for this task was a probabilistic graphical model that inferred vocabulary dependent latent post categories to model the thread sequence and infer states that triggered intervention. However, this model required a hyperparameter for the number of latent states, which may not generalize well due to weak evaluation. (BIBREF0) (BIBREF7) (BIBREF0) (BIBREF7) (BIBREF0) (BIBREF7) (BIBREF0) (BIBREF7) (BIBREF0) (BIBREF7) (BIBREF0) (B", "answers": ["hLSTM", "hLSTM"], "all_classes": null, "length": 3725}
{"pred": "    The least impactful component is the master node. The master node is a special document node that is connected to all other nodes via unit weight bi-directional edges. It is used to encode a summary of the document, but its removal does not significantly impact performance. The other components, such as the message passing framework, the attention mechanism, and the hierarchical variants, all contribute to the overall performance of the model. (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (", "answers": ["Based on table results provided changing directed to undirected edges had least impact - max abs difference of 0.33 points on all three datasets."], "all_classes": null, "length": 4180}
{"pred": "    The corpus used for the task is DTA18 and DTA19, which are subparts of DTA corpus. These corpora consist of texts from the 16th to the 20th century, and the corpus version used in the task has the following format: \"year [tab] lemma1 lemma2 lemma3...\". The two corpora correspond to the diachronic corpus pair from BIBREF0: DTA18 and DTA19. They consist of subparts of DTA corpus BIBREF11 which is a freely available lemmatized, POS-tagged and spelling-normalized diachronic", "answers": ["DTA18, DTA19", "Diachronic Usage Relatedness (DURel) gold standard data set"], "all_classes": null, "length": 1908}
{"pred": "        Kannada, Hindi, Telugu, Malayalam, Bengali, and English\n\n        Kannada, Hindi, Telugu, Malayalam, Bengali, and English\n\n        Kannada, Hindi, Telugu, Malayalam, Bengali, and English\n\n        Kannada, Hindi, Telugu, Malayalam, Bengali, and English\n\n        Kannada, Hindi, Telugu, Malayalam, Bengali, and English\n\n        Kannada, Hindi, Telugu, Malayalam, Bengali, and English\n\n        Kannada, Hindi, Telugu, Malayalam, Bengali, and English\n\n        Kann", "answers": ["Hindi, English, Kannada, Telugu, Assamese, Bengali and Malayalam", "Kannada, Hindi, Telugu, Malayalam, Bengali,  English and Assamese (in table, missing in text)"], "all_classes": null, "length": 2453}
{"pred": "The model performance on target language reading comprehension is reasonable, with EM/F1 scores of 53.8 and 44.1, respectively. However, the performance is degraded when the training data is translated into the target language, indicating that translation degrades the quality of data. The results show that the quality and the size of dataset are much more important than whether the training and testing are in the same language or not. The model also shows transfer capability when training and testing sets are in different languages, especially between Chinese and Korean. (Unanswerable) (Yes) (No) (Unanswerable) (Yes) (No", "answers": ["Table TABREF6, Table TABREF8", "when testing on English, the F1 score of the model training on Chinese (Zh) is 53.8,  F1 score is only 44.1 for the model training on Zh-En"], "all_classes": null, "length": 2492}
{"pred": "        The proposed model outperforms the baselines by a significant margin. The difference in performance is consistent across all evaluation characters. The proposed model is able to recover the language styles of specific characters more accurately than the baselines. The proposed model is also able to recommend tailored responses traceable to specific characters, which is not possible with the baselines. The proposed model is able to effectively use HLAs to improve upon dialogue retrieval performance. The proposed model is able to recover the language styles of various characters regardless of the character's profile and identity, genre of the show, and context of the dialogue. The proposed model is also able to", "answers": ["Metric difference between Aloha and best baseline score:\nHits@1/20: +0.061 (0.3642 vs 0.3032)\nMRR: +0.0572(0.5114 vs 0.4542)\nF1: -0.0484 (0.3901 vs 0.4385)\nBLEU: +0.0474 (0.2867 vs 0.2393)"], "all_classes": null, "length": 5151}
{"pred": "        1.5-2.0x improvement in forward perplexity and 1.0-1.5x improvement in reverse perplexity. 2.0-2.5x improvement in Self-BLEU. 3.0-3.5x improvement in manual evaluation. 4.0-4.5x improvement in training stability. 5.0-5.5x improvement in training variance. 6.0-6.5x improvement in overall performance. 7.0-7.5x improvement in overall performance. 8.0-8.5x improvement in", "answers": ["ARAM has achieved improvement over all baseline methods using reverese perplexity and  slef-BLEU metric.  The maximum reverse perplexity improvement 936,16 is gained for EMNLP2017  WMT  dataset and  48,44 for COCO dataset.", "Compared to the baselines, ARAML does not do better in terms of perplexity on COCO and EMNLP 2017 WMT datasets, but it does by up to 0.27 Self-BLEU points on COCO and 0.35 Self-BLEU on EMNLP 2017 WMT. In terms of Grammaticality and Relevance, it scores better than the baselines on up to 75.5% and 73% of the cases respectively."], "all_classes": null, "length": 3796}
{"pred": "        The authors present some evidence that the model can capture some biases in data annotation and collection. They examine the results and find that the model can detect some biases in the process of collecting or annotating datasets. They also find that the model can detect some biases in the data itself, such as the presence of certain words or phrases that are associated with hate speech. The authors conclude that this ability to detect biases can be a valuable clue in using the pre-trained BERT model to alleviate bias in hate speech datasets in future studies. (unanswerable) (yes) (no) (unanswerable) (yes) (no)", "answers": ["The authors showed few tweets where neither and implicit hatred content exist but the model was able to discriminate"], "all_classes": null, "length": 4119}
{"pred": "        Yes, three baselines were tested to compare with the neural baseline. The first baseline is a simple majority-class baseline, which predicts that all questions are unanswerable. The second baseline is an SVM classifier with three sets of features, including bag-of-words features, length of the question in words, and part-of-speech tags. The third baseline is a CNN neural encoder. The results show that the neural baseline outperforms the other baselines, indicating that it is capable of making some progress towards answering questions in this challenging domain. However, there is still considerable room for improvement to reach human performance. (Unanswerable", "answers": ["SVM, No-Answer Baseline (NA) , Word Count Baseline, Human Performance", "No-Answer Baseline (NA), Word Count Baseline, Human Performance"], "all_classes": null, "length": 3855}
{"pred": "        72782 unique words\n\n        14 million words from books, web-texts and news papers\n\n        6946 sentences and 16225 unique words extracted from POS-tagged Nepali National Corpus\n\n        64%, 16% and 20% of the total dataset into training set, development set and test set respectively\n\n        72782 unique words\n\n        14 million words from books, web-texts and news papers\n\n        6946 sentences and 16225 unique words extracted from POS-tagged Nepali National Corpus\n\n        64%, 16% and 20% of the total dataset into", "answers": ["Dataset contains 3606 total sentences and 79087 total entities.", "ILPRL contains 548 sentences, OurNepali contains 3606 sentences"], "all_classes": null, "length": 2843}
{"pred": "        The proposed method improves F1 for paraphrase identification by replacing the training objective with dice loss, which performs as a soft version of F1 score. This helps narrow the gap between training objectives and evaluation metrics, leading to significant performance boost. The proposed method also offers flexibility in controlling the tradeoff between false-negatives and false-positives through hyperparameters. Empirically, the proposed method leads to significant performance boost for part-of-speech, named entity recognition, machine reading comprehension and paraphrase identification tasks. (unanswerable) (yes) (no) (unanswerable) (yes) (no) (", "answers": ["Using DSC loss improves the F1 score by +0.58 for MRPC and +0.73 for QQP", "+0.58"], "all_classes": null, "length": 3566}
{"pred": "        The datasets used are eye-tracking, self-paced reading time, and ERP components. (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (", "answers": ["Answer with content missing: (Whole Method and Results sections) The primary dataset we use is the ERP data collected and computed by Frank et al. (2015), and we also use behavioral data (eye-tracking data and self-paced reading times) from Frank et al. (2013) which were collected on the same set of 205 sentences.\nSelect:\n- ERP data collected and computed by Frank et al. (2015)\n- behavioral data (eye-tracking data and self-paced reading times) from Frank et al. (2013)", "the ERP data: BIBREF0"], "all_classes": null, "length": 1971}
{"pred": "        The subjects were presented with a series of images depicting different events, such as a person walking, a car driving, and a person eating. The images were designed to elicit event-related responses in the form of brain activity. The subjects were instructed to imagine themselves performing the actions depicted in the images, and their brain activity was recorded using EEG. The resulting EEG data was used to investigate the relationship between brain activity and event-related responses. (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes", "answers": ["7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw)", "KARA ONE BIBREF17 , composed of multimodal data for stimulus-based, imagined and articulated speech state corresponding to 7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw)"], "all_classes": null, "length": 2379}
{"pred": "    Pointer-Gen, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN, Pointer-Gen+ARL-SEN, Test set, Human evaluation, ROUGE, Chi-square test, Fluency, Sensationalism, ARL, RL, MLE, Distant supervision, Pointer-Gen, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN, Pointer-Gen+ARL-SEN, Test set, Human evaluation, ROUGE, Chi-square test, Fluency, Sensationalism, ARL, RL, MLE, Distant supervision, Pointer-Gen", "answers": ["Pointer-Gen, Pointer-Gen+Pos, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN"], "all_classes": null, "length": 4085}
{"pred": "    Traditional machine learning models and neural network based models are used on the dataset. Traditional machine learning models include Naïve Bayes, Logistic Regression, Support Vector Machine, Random Forests, and Gradient Boosted Trees. Neural network based models include Convolutional Neural Networks, Recurrent Neural Networks, and their variants. The authors also investigate the effect of different features and variants, and describe the possibility for further improvements with the use of ensemble models.", "answers": ["Naïve Bayes (NB), Logistic Regression (LR), Support Vector Machine (SVM), Random Forests (RF), Gradient Boosted Trees (GBT),  Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN)", "Naïve Bayes (NB), Logistic Regression (LR), Support Vector Machine (SVM), Random Forests (RF), Gradient Boosted Trees (GBT), CNN, RNN"], "all_classes": null, "length": 2074}
{"pred": "    The bi-directional language model is used to augment the sequence to sequence encoder, and the uni-directional model is used to augment the decoder. Both models use self-attention and contain transformer blocks. The bi-directional model solves a cloze-style token prediction task at training time, while the uni-directional model predicts the next word on the right. The models are trained on 32 Nvidia V100 SXM2 GPUs and use the NCCL2 library and torch distributed package for inter-GPU communication. Training relies on 16-bit floating point operations and takes six days for the bi-directional model and four days for the uni", "answers": ["uni-directional model to augment the decoder", "bi-directional language model to augment the sequence to sequence encoder ,  uni-directional model to augment the decoder"], "all_classes": null, "length": 1914}
{"pred": "        The weights are dynamically adjusted by multiplying the soft probability p with a decaying factor (1-p). This decaying factor pushes down the weight of easy examples, making the model less focused on them. The intuition is to push down the weight of easy examples, making the model less focused on them. The weights are dynamically adjusted by multiplying the soft probability p with a decaying factor (1-p). This decaying factor pushes down the weight of easy examples, making the model less focused on them. The intuition is to push down the weight of easy examples, making the model less focused on them. The weights are dynamically adjusted by multiplying", "answers": ["One can think $(1-p_{i1})$ as a weight associated with each example, which changes as training proceeds. The intuition of changing $p_{i1}$ to $(1-p_{i1}) p_{i1}$ is to push down the weight of easy examples. For easy examples whose probability are approaching 0 or 1, $(1-p_{i1}) p_{i1}$ makes the model attach significantly less focus to them. Figure FIGREF23 gives gives an explanation from the perspective in derivative: the derivative of $\\frac{(1-p)p}{1+(1-p)p}$ with respect to $p$ approaches 0 immediately after $p$ approaches 0, which means the model attends less to examples once they are correctly classified.", "associates each training example with a weight in proportion to $(1-p)$, and this weight dynamically changes as training proceeds"], "all_classes": null, "length": 3640}
{"pred": "        The results show that agents utilizing knowledge-graphs in addition to either enhanced exploration method far outperform the baseline A2C and KG-A2C. KG-A2C-chained and KG-A2C-Explore both pass the bottleneck of a score of 40, whereas A2C-Explore gets to the bottleneck but cannot surpass it. There are a couple of key insights that can be drawn from these results. The first is that the knowledge graph appears to be critical; it is theorized to help with partial observability. However the knowledge graph representation isn't sufficient in that the knowledge graph representation without enhanced exploration methods cannot surpass", "answers": ["Reward of 11.8 for the A2C-chained model, 41.8 for the KG-A2C-chained model, 40 for A2C-Explore and 44 for KG-A2C-Explore.", "KG-A2C-chained and KG-A2C-Explore both pass the bottleneck of a score of 40"], "all_classes": null, "length": 2443}
{"pred": "    The individual model consists of individual Bayesian models for each language and crosslingual latent variables to incorporate soft role agreement between aligned constituents. These latent variables capture correlations between roles in different languages and regularize the parameter estimates of the monolingual models. The model is trained on parallel sentences to exploit role correspondences across languages. The generative process remains the same as the monolingual model for the most part, with the exception of aligned roles which are now generated by both the monolingual process as well as the CLV. The CLVs are generated via a Chinese Restaurant Process, a non-parametric Bayesian model, which allows us", "answers": ["Bayesian model of garg2012unsupervised as our base monolingual model"], "all_classes": null, "length": 3701}
{"pred": "        Non-standard pronunciation is identified by annotating the Spanish words interspersed in Mapudungun speech. The annotations label foreign words as such. (Unanswerable) (Yes) (No) (Unanswerable) (Yes) (No) (Unanswerable) (Yes) (No) (Unanswerable) (Yes) (No) (Unanswerable) (Yes) (No) (Unanswerable) (Yes) (No) (Unanswerable) (Yes) (No) (Unanswerable) (Yes) (No) (Unanswerable) (Yes) (No", "answers": ["Unanswerable", "Original transcription was labeled with additional labels in [] brackets with nonstandard pronunciation."], "all_classes": null, "length": 3018}
{"pred": "    A semicharacter architecture is a type of neural network that processes a sentence of words with misspelled characters, predicting the correct words at each step. It is inspired by psycholinguistic studies that demonstrate humans can comprehend text altered by jumbling internal characters, provided that the first and last characters of each word remain unperturbed. The architecture treats the first and the last characters individually, and is agnostic to the ordering of the internal characters. It is trained to predict the correct corresponding word at each sequence step, using a cross-entropy loss. The architecture is shown to have strong word recognition performance, but its evaluation", "answers": ["A semi-character based RNN (ScRNN) treats the first and last characters individually, and is agnostic to the ordering of the internal characters", "processes a sentence of words with misspelled characters, predicting the correct words at each step"], "all_classes": null, "length": 4186}
{"pred": "        Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish.       \n\n\n        Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish.       \n\n\n        Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish.       \n\n\n        Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian", "answers": ["Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish", "Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish , Swedish"], "all_classes": null, "length": 2697}
{"pred": "    The NCEL approach is effective overall, outperforming state-of-the-art collective methods across five different datasets. Further analysis of the impacts of main modules as well as qualitative results demonstrate its effectiveness. The authors will extend their method into cross-lingual settings and deal with NIL entities in the future. (Unanswerable) (Yes) (No) (Unanswerable) (Yes) (No) (Unanswerable) (Yes) (No) (Unanswerable) (Yes) (No) (Unanswerable) (Yes) (No) (Unanswerable) (Yes) (No) (", "answers": ["NCEL consistently outperforms various baselines with a favorable generalization ability"], "all_classes": null, "length": 4113}
{"pred": "    Yes, the data is de-identified with digital zeros and [de-identified] tags. The sentences in the transcript are grounded to the audio with the timestamps of its first and last word. The summaries and MR tags are also grounded using the timestamps of the sentences from the transcript deemed relevant by the expert annotators. The MR tags are local and of the form {Medication Name, Dosage, Frequency}. If dosage or frequency information for a medication is not present in a grounded sentence, the corresponding field in the MR tag will be marked as 'none'. The data is processed to improve performance and increase the number of training data", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4592}
{"pred": "    The baseline used was the error detection system by Rei2016, trained using the same FCE dataset. The performance of this system is reported in Table TABREF4. The baseline performance is 78.1% on the FCE test data, 77.2% on the CoNLL 2014 Shared Task dataset, and 76.3% on the CoNLL 2014 Shared Task dataset. These results are used as a baseline for comparison with the error detection systems trained using artificial data. The baseline performance is also used to calculate statistical significance of the improvements achieved by using artificial data. The baseline performance is", "answers": ["error detection system by Rei2016", "error detection system by Rei2016"], "all_classes": null, "length": 2132}
{"pred": "    The annotated clinical notes were obtained from the 2010 i2b2/VA BIBREF0 and 2013 ShARe/CLEF Task 1 BIBREF1 challenges in healthcare. These challenges ask participants to design an algorithm to tag a set of predefined entities of interest in clinical notes. These entity tagging tasks are also known as clinical Named Entity Recognition (NER). For example, the CE task in 2010 i2b2/VA defines three types of entities: \"problem\", \"treatment\", and \"test\". The CE task in 2013 ShARe/CLEF defines various types", "answers": ["clinical notes from the CE task in 2010 i2b2/VA", "clinical notes from the CE task in 2010 i2b2/VA "], "all_classes": null, "length": 3432}
{"pred": "    Masking words in the decoder helps to focus the attention of the decoder on the context of the summary, rather than on the words in the summary itself. This allows the decoder to generate more context-aware and coherent summaries. Moreover, masking words in the decoder helps to reduce exposure bias, which is a common problem in sequence-to-sequence models. By masking words in the decoder, the model is forced to generate words that are not present in the summary, which helps to improve the diversity and fluency of the generated summaries. Overall, masking words in the decoder is a useful technique for improving the quality of abstractive text summarization", "answers": ["ecause this process is similar to the cloze task in BERT's pre-train process, therefore by using the ability of the contextual language model the decoder can generate more fluent and natural sequences."], "all_classes": null, "length": 3919}
{"pred": "        The dataset used is the PPDB dataset, which contains noisy phrase pairs. (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (un", "answers": ["Unanswerable", " Paraphrase Database (PPDB) ,  book corpus", "Unanswerable"], "all_classes": null, "length": 1902}
{"pred": "    Term Frequency-Inverse Document Frequency (TF-IDF) features are used to extract and corroborate useful keywords from pathology cancer reports. These features are based on the frequency of terms in a document and the inverse document frequency of those terms. The TF-IDF features are used to highlight the important regions of the report and provide useful biomarker information to readers. The TF-IDF features are also used to predict the primary diagnosis from the text content of a given report. The performance of the TF-IDF features is evaluated using different classification models and the results are reported in the article. The TF-IDF features are shown to be effective in", "answers": ["Unanswerable"], "all_classes": null, "length": 2108}
{"pred": "        The dataset is annotated based on a hierarchical model of depression-related symptoms. Each tweet is annotated as no evidence of depression or evidence of depression. If a tweet is annotated evidence of depression, then it is further annotated with one or more depressive symptoms, for example, depressed mood, disturbed sleep, or fatigue or loss of energy. For each class, every annotation is binarized as the positive class or negative class. The dataset contains 9,473 annotations for 9,300 tweets. (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes", "answers": ["no evidence of depression, depressed mood, disturbed sleep, fatigue or loss of energy", "The annotations are based on evidence of depression and further annotated by the depressive symptom if there is evidence of depression"], "all_classes": null, "length": 1947}
{"pred": "    They evaluated on eight biomedical NER tasks: BC5CDR, CheBI, CRAFT, DDI, GENIA, JNLPBA, LLL, and NCBI Disease. They also evaluated on the NCBI Disease task, which is a subset of the NCBI Disease task. They used the same setup as the BioBERT paper, with the same random seeds and hyperparameters. They found that GreenBioBERT outperforms general-domain BERT on all tasks, with varying effect sizes. Depending on the points of reference, they cover an average of 52% to 60% of the BioBERT – B", "answers": ["BC5CDR-disease, NCBI-disease, BC5CDR-chem, BC4CHEMD, BC2GM, JNLPBA, LINNAEUS, Species-800", "BC5CDR-disease, NCBI-disease, BC5CDR-chem, BC4CHEMD, BC2GM, JNLPBA, LINNAEUS, Species-800"], "all_classes": null, "length": 2800}
{"pred": "        The training data was translated using the machine translation platform Apertium. The English datasets were translated into Spanish, leaving the labels intact. The machine translation platform Apertium was used for the translation of the datasets.<translation of the datasets.<translation of the datasets.<translation of the datasets.<translation of the datasets.<translation of the datasets.<translation of the datasets.<translation of the datasets.<translation of the datasets.<translation of the datasets.<translation of the datasets.<translation of the datasets.<translation of the datasets.<translation of the datasets.<translation of the datasets.<translation of the datasets.<translation of the datasets.<translation of the datasets", "answers": ["using the machine translation platform Apertium ", "machine translation platform Apertium BIBREF5"], "all_classes": null, "length": 2423}
{"pred": "        They used a content-based classifier in conjunction with two feature selection methods. davidjl\n\n        They used a content-based classifier in conjunction with two feature selection methods. davidjl\n\n        They used a content-based classifier in conjunction with two feature selection methods. davidjl\n\n        They used a content-based classifier in conjunction with two feature selection methods. davidjl\n\n        They used a content-based classifier in conjunction with two feature selection methods. davidjl\n\n        They used a content-based classifier in conjunction with two feature selection methods. davidjl\n\n        They used a content-based classifier in conjunction with two feature selection methods. davidjl\n\n        They used a content-based classifier in conjunction", "answers": ["AllWords model by counting the frequencies of all the remaining words and training a multinomial Naive Bayes classifier"], "all_classes": null, "length": 4177}
{"pred": "    The baseline for this task was a very simple logistic regression classifier with default parameters, where we represent the input instances with a single feature: the length of the sentence. The performance of this baseline on the SLC task is shown in Tables TABREF33 and TABREF34. (Note: Tables TABREF33 and TABREF34 are not included in this code snippet.) (Note: Tables TABREF33 and TABREF34 are not included in this code snippet.) (Note: Tables TABREF33 and TABREF34 are not included in this code snippet.) (Note: Tables TABREF33 and TABREF34 are not included in", "answers": ["The baseline system for the SLC task is a very simple logistic regression classifier with default parameters. The baseline for the FLC task generates spans and selects one of the 18 techniques randomly.", "SLC task is a very simple logistic regression classifier, FLC task generates spans and selects one of the 18 techniques randomly"], "all_classes": null, "length": 3001}
{"pred": "        The baselines they compare with are conditional random fields (CRF) and a pipeline method that only considers sentences containing a pun. They also compare their approach with previous works that did not employ joint learning. (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (", "answers": ["They compare with the following models: by Pedersen (2017), by Pramanick and Das (2017), by Mikhalkova and Karyakin (2017),  by Vadehra (2017), Indurthi and Oota (2017), by Vechtomova (2017), by (Cai et al., 2018), and CRF."], "all_classes": null, "length": 2991}
{"pred": "    The political bias of different sources is included in the model by assigning a political bias label to different US outlets following the procedure described in BIBREF2. The model is then trained only on left-biased (or right-biased) outlets of both disinformation and mainstream domains and tested on the entire set of sources. The robustness of the model is assessed by excluding particular sources that outweigh the others in terms of samples to avoid over-fitting. The results show that the multi-layer approach still entails significant results, thus showing that it can accurately distinguish mainstream news from disinformation regardless of the political bias. Additionally, the model", "answers": ["By assigning a political bias label to each news article and training only on left-biased or right-biased outlets of both disinformation and mainstream domains", "we also account for political biases inherent to different news sources, referring to the procedure proposed in BIBREF2 to label different outlets. Overall we show that we are able to classify credible vs non-credible diffusion networks (and consequently news articles) with high accuracy (AUROC up to 94%), even when accounting for the political bias of sources (and training only on left-biased or right-biased articles). We observe that the layer of mentions alone conveys useful information for the classification, denoting a different usage of this functionality when sharing news belonging to the two news domains. We also show that most discriminative features, which are relative to the breadth and depth of largest cascades in different layers, are the same across the two countries."], "all_classes": null, "length": 4882}
{"pred": "        The ancient Chinese dataset comes from ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era. They used plain and accurate words to express what happened at that time, and thus ensure the generality of the translated materials. (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (un", "answers": ["ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era", "Ancient Chinese history records in several dynasties and articles written by celebrities during 1000BC-200BC collected from the internet "], "all_classes": null, "length": 3722}
{"pred": "        English (en) overposting overposting\n\n        English (en) overposting overposting\n\n        English (en) overposting overposting\n\n        English (en) overposting overposting\n\n        English (en) overposting overposting\n\n        English (en) overposting overposting\n\n        English (en) overposting overposting\n\n        English (en) overposting overposting\n\n        English (en) overposting overposting\n\n        English (en) overposting overposting\n\n        English (en) overposting overposting\n\n        English (en) overposting overposting\n\n        English (en) overposting overposting\n\n        English (en) overposting overposting\n\n        English (en) overposting overposting\n\n        English (en) overposting overposting\n\n", "answers": ["English", "English ", "English"], "all_classes": null, "length": 2240}
{"pred": "        The Chinese datasets used were the Penn Treebank (PTB) and the Penn Chinese Treebank (PCTB). The PTB contains 10,000 sentences of English, while the PCTB contains 10,000 sentences of Chinese. Both datasets are widely used in natural language processing research. The PTB is a standard benchmark for English grammar induction, while the PCTB is a standard benchmark for Chinese grammar induction. The PTB and PCTB are both available online for download. The PTB is available at https://www.ling.upenn.edu/courses/fall2010/ling571/ptb", "answers": ["Answer with content missing: (Data section) Chinese with version 5.1 of the Chinese Penn Treebank (CTB)"], "all_classes": null, "length": 2545}
{"pred": "    4 layers (user matrix embedding, user vector embedding, topic matrix embedding, topic vector embedding, comment representation, fully connected network) 。\n\n    4 layers (user matrix embedding, user vector embedding, topic matrix embedding, topic vector embedding, comment representation, fully connected network) 。\n\n    4 layers (user matrix embedding, user vector embedding, topic matrix embedding, topic vector embedding, comment representation, fully connected network) 。\n\n    4 layers (user matrix embedding, user vector embedding, topic matrix embedding, topic vector embedding, comment representation, fully connected network) 。\n\n    4 layers (user matrix embedding", "answers": ["eight layers"], "all_classes": null, "length": 4487}
{"pred": "    The dataset used in this paper is Flickr, which is a photo-sharing website that provides tags and georeferenced photos. The tags associated with these photos often describe the location where the photos were taken, and Flickr can thus be regarded as a source of environmental information. The use of Flickr for modelling urban environments has already received considerable attention. For instance, various approaches have been proposed for modelling urban regions, identifying points-of-interest, and itineraries. However, the usefulness of Flickr for characterizing the natural environment, which is the focus of this paper, is less well-understood. Many recent studies have highlighted that Flickr tags capture", "answers": [" the same datasets as BIBREF7", "same datasets as BIBREF7"], "all_classes": null, "length": 4661}
{"pred": "    The paper uses two clinical datasets: NUBes-PHI and MEDDOCAN. NUBes-PHI is a corpus of real medical reports written in Spanish and annotated with sensitive information. MEDDOCAN is a synthetic corpus of clinical cases enriched with sensitive information by health documentalists. Both datasets are used to evaluate the performance of the BERT-based model for sensitive information detection and classification in Spanish clinical data. (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no)", "answers": ["MEDDOCAN, NUBes-PHI", "MEDDOCAN, NUBes "], "all_classes": null, "length": 4528}
{"pred": "    Unigrams and Pragmatic features, Stylistic patterns, and patterns related to situational disparity. (BIBREF0, BIBREF1, BIBREF2, BIBREF3) (BIBREF4, BIBREF5) (BIBREF6, BIBREF7) (BIBREF8) (BIBREF9, BIBREF10) (BIBREF11) (BIBREF12) (BIBREF13) (BIBREF14) (BIBREF15) (BIBREF16) (BIBREF17) (BIBREF", "answers": ["Unanswerable"], "all_classes": null, "length": 3543}
{"pred": "    The metrics used to establish that this makes chatbots more knowledgeable and better at learning and conversation are predictive quality and strategy formulation ability. The predictive quality is measured by average MCC and average positive F1 score, while the strategy formulation ability is measured by coverage. The results show that the lifelong interactive learning and inference approach is effective in terms of both predictive quality and strategy formulation ability. \n\nQuestion: What is the main contribution of this paper? \n\nAnswer: The main contribution of this paper is proposing an lifelong interactive learning and inference approach to solve the open-world knowledge base completion problem in human-machine conversations. The approach uses reinforcement learning to formulate query", "answers": ["Coverage, Avg. MCC and avg. +ve F1 score", "strategy formulation ability, we introduce a measure called Coverage( INLINEFORM0 ), To evaluate the predictive performance, we use Avg. MCC and avg. +ve F1 score"], "all_classes": null, "length": 5869}
{"pred": "        Yes, they employ their indexing-based method to create a sample of a QA Wikipedia dataset. They first index each paragraph in Wikipedia using {1,2,3}-grams, then query each answer sentence from the corpora to Lucene, and retrieve the top-5 ranked paragraphs. They measure the cosine similarity between each sentence in these paragraphs and the answer sentence, and assign weights to each n-gram score. Finally, they create a silver-standard dataset for answer retrieval and triggering. (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes)", "answers": ["Yes", "No"], "all_classes": null, "length": 1910}
{"pred": "        Galatasaray and Fenerbahçe are the targets. (Fenerbahçe is also known as Fenerbahçe Spor Kulübü.) (Fenerbahçe is also known as Fenerbahçe Spor Kulübü.) (Fenerbahçe is also known as Fenerbahçe Spor Kulübü.) (Fenerbahçe is also known as Fenerbahçe Spor Kulübü.) (Fenerbahçe is also known as Fenerbahçe Spor Kulübü.) (Fenerbahçe is also known as Fenerbahçe Spor Kulübü.) (Fenerbahçe is", "answers": ["Galatasaray, Fenerbahçe", "Galatasaray , Fenerbahçe "], "all_classes": null, "length": 2234}
{"pred": "    The experiments conducted are automatic evaluations and human evaluations. Automatic evaluations include sentiment delta, sentiment accuracy, BLEU score, G2, and H2. Human evaluations include irony accuracy, sentiment preservation, and content preservation. The experiments are conducted to evaluate the performance of the model in generating ironic sentences from non-ironic sentences. The results of the experiments are presented in tables and discussed in the paper. The experiments are designed to test the effectiveness of the model and the proposed rewards in controlling irony accuracy, sentiment preservation, and content preservation. The results of the experiments demonstrate that the model is effective in generating ironic sentences and the proposed rewards are", "answers": ["Irony Classifier, Sentiment Classifier for Irony, Sentiment Classifier for Non-irony, transformation from ironic sentences to non-ironic sentences"], "all_classes": null, "length": 4600}
{"pred": "    Gaussian-masked directional multi-head attention works by adjusting the weight between characters and their adjacent character to a larger value which stands for the effect of adjacent characters. The Gaussian-masked attention ensures that the relationship between two characters with long distances is weaker than adjacent characters. The Gaussian weight only relies on the distance between characters and is calculated by the cumulative distribution function of Gaussian. The Gaussian weight matrix is used to combine the Gaussian weight to the self-attention. The Gaussian-masked attention is used in the forward and backward encoders to capture information of two directions which correspond to two parts divided by the gap. The central encoder is a special directional", "answers": ["pays attentions to adjacent characters and casts a localness relationship between the characters as a fixed Gaussian weight assuming the weight relies on the distance between characters", "Gaussian-masked directional attention can be described as a function to map queries and key-value pairs to the representation of input, Gaussian-masked directional attention expects to pay attention to the adjacent characters of each positions and cast the localness relationship between characters as a fix Gaussian weight for attention, Gaussian weight only relys on the distance between characters"], "all_classes": null, "length": 3612}
{"pred": "    Facebook status update messages\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n       ...\n\n", "answers": ["Facebook status update messages", "Facebook status update messages"], "all_classes": null, "length": 4005}
{"pred": "    The baseline features are the features extracted from the fully-connected layer of the baseline CNN. These features are used to train the final softmax layer to classify a sentence as sarcastic vs non-sarcastic. The baseline features are the inherent semantics of the sarcastic corpus, which are learned by the deep domain understanding of the baseline CNN. These features are used to train the final softmax layer to classify a sentence as sarcastic vs non-sarcastic. The baseline features are the inherent semantics of the sarcastic corpus, which are learned by the deep domain understanding of the baseline CNN. These features are used to train the final softmax layer", "answers": [" The features extracted from CNN."], "all_classes": null, "length": 4855}
{"pred": "        The number of clusters and the type of word vectors were varied in the experiments on the four tasks. The number of clusters was varied between 250 and 1000, and the type of word vectors was varied between skipgram, cbow, and GloVe. The in-domain and out-of-domain word vectors were also compared. The hyperparameters were varied to investigate the impact of the cluster membership features on the performance of the tasks. The results suggest that incorporating cluster membership features can improve the performance of the tasks. The best results were obtained with in-domain word vectors and a high number of clusters. The out-of-domain word vectors also", "answers": ["number of clusters, seed value in clustering, selection of word vectors, window size and dimension of embedding", "different number of clusters, different embeddings"], "all_classes": null, "length": 2753}
{"pred": "        Second (EI-Reg), second (EI-Oc), fourth (V-Reg) and fifth (V-Oc) on the SemEval AIT-2018 leaderboard. (EI-Reg: Emotion Intensity Regression, EI-Oc: Emotion Intensity Ordinal Classification, V-Reg: Valence Regression, V-Oc: Valence Ordinal Classification) (Unanswerable) (Yes) (No) (Unanswerable) (Yes) (No) (Unanswerable) (Yes) (No) (Unanswerable) (Yes) (No) (Unanswerable) (", "answers": ["column Ens Test in Table TABREF19"], "all_classes": null, "length": 2424}
{"pred": "        53 documents, with an average of 156.1 sentences per document, containing 8,275 sentences and 167,739 words in total. The corpus consists of 8,275 sentences and 167,739 words in total. The number of annotated entities is summarized in Table TABREF24. The most frequently annotated type of entity is findings, with an average length of 2.6 tokens. The number of tokens per entity ranges from one token for all types to 5 tokens for cases (average length 3.1), nine tokens for conditions (average length 2.0), 16 tokens for factors", "answers": ["8,275 sentences and 167,739 words in total", "The corpus comprises 8,275 sentences and 167,739 words in total."], "all_classes": null, "length": 2669}
{"pred": "    Yes, it is possible to convert a cloze-style questions to a naturally-looking questions by using a standard NLP pipeline based on Stanford CoreNLP and the BANNER Named Entity Recognizer. The process relies on the fact that answer candidates from the introduction are likely to be discussed in detail in the remainder of the article. In effect, the cloze question from the introduction and the matching paragraph in the body forms a question and context passage pair. The process generates a large number of clozes given any corpus, which we found to be beneficial for semi-supervised learning despite the noise. The utility of forming subsets of the large clo", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 2764}
{"pred": "    Text categorization, sentiment classification, and sports category BIBREF0. They also mention other NLP tasks such as text summarization, machine translation, and question answering. BIBREF0\n\nQuestion: What are the main contributions of this work?\n\nAnswer: davidjl\n\n    /Dk; The main contributions of this work are as follows: (1) Introducing auxiliary regularization terms to address the robustness problem of leveraging prior knowledge in learning models. (2) Exploring three regularization terms to address the problem: neutral features, maximum entropy of class distribution, and KL divergence between reference and predicted class distribution. (3) Conducting", "answers": ["text classification for themes including sentiment, web-page, science, medical and healthcare"], "all_classes": null, "length": 3591}
{"pred": "    Previous methods include rule-based and machine learning approaches, as well as term frequency models. Their model is compared to these previous methods in terms of performance and accuracy. (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes", "answers": ["bag-of-words model, CNN"], "all_classes": null, "length": 5838}
{"pred": "    The training sets of these versions of ELMo are significantly larger compared to the previous ones. The previous versions of ELMo were trained on a one billion word large English corpus, while these versions are trained on much larger corpora, ranging from 20 million to 280 million tokens. The larger training sets allow the models to capture more semantic and contextual information, resulting in better quality embeddings. (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswer", "answers": ["By 14 times.", "up to 1.95 times larger"], "all_classes": null, "length": 3294}
{"pred": "        6946 sentences in the dataset. (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (", "answers": ["3606", "6946"], "all_classes": null, "length": 2835}
{"pred": "    Eusboost and MWMOTE techniques\n\n        Eusboost and MWMOTE techniques\n\n    MWMOTE and Eusboost\n\n        MWMOTE and Eusboost\n\n    Eusboost and MWMOTE\n\n        Eusboost and MWMOTE\n\n    MWMOTE and Eusboost\n\n        MWMOTE and Eusboost\n\n    Eusboost and MWMOTE\n\n        Eusboost and MWMOTE\n\n    MWMOTE and Eusboost\n\n        MWMOTE and Eusboost\n\n    Eusboost and MWMOTE\n\n        Eusboost and MWMOTE\n\n", "answers": ["MLP", "Eusboost, MWMOTE"], "all_classes": null, "length": 2474}
{"pred": "    Yes, their NER model learns NER from both text and images. The model takes as input a sequence of tokens, each of which comprises a word token, a character sequence, and an image, in their respective representation. The model then uses a modality attention module to combine representations from each modality and produce an entity label for each token. The modality attention module learns a unified representation space for multiple available modalities (words, characters, images, etc.), and produces a single vector representation with aggregated knowledge among multiple modalities, based on their weighted importance. The model is able to learn NER from both text and", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3784}
{"pred": "    Yes, they evaluate only on English datasets. They use the Wall Street Journal (WSJ) portion of the Penn Treebank for both POS tagging and dependency parsing. The skip-gram word embeddings are trained on the one billion word language modeling benchmark dataset in addition to the WSJ corpus. The experimental setup is described in detail in the \"Experiments\" section of the article. The authors do not mention evaluating on any other languages or datasets. Therefore, it is safe to assume that they only evaluate on English datasets. However, it would be interesting to see if their approach can be applied to other languages or datasets and if it can", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 4320}
{"pred": "    32% νεφοκάλυψης\n\n     νεφοκάλυψης\n\n    32% νεφοκάλυψης\n\n    32% νεφοκάλυψης\n\n    32% νεφοκάλυψης\n\n    32% νεφοκάλυψης\n\n    32% νεφοκάλυψης\n\n    32% νεφοκάλυψης\n\n    32% νεφοκάλυψης\n\n    32% νεφοκάλυψης\n\n    32% νεφοκάλυψης\n\n    32% νεφοκάλυψης\n\n    32% νεφοκάλυψης\n\n    32% νεφοκάλυψης\n\n    32% νεφοκάλυψης\n\n    32% νεφοκάλυψης\n\n    32% νεφοκάλυψης\n\n    32% νεφοκάλυψης\n\n    32% νεφοκάλυψης\n\n    32% νεφοκάλυψης\n\n    32% νεφοκάλυψης\n\n    32% νεφοκάλυψης", "answers": ["0.5115", "0.6103"], "all_classes": null, "length": 6810}
{"pred": "    The datasets they evaluate on are the Wall Street Journal (WSJ) portion of the Penn Treebank for both POS tagging and dependency grammar induction. They use skip-gram word embeddings trained on the one billion word language modeling benchmark dataset in addition to the WSJ corpus. The dimensionality of the embeddings is set to 100, and the training context window size is set to 1 to encode more syntactic information. The skip-gram embeddings are trained on the one billion word language modeling benchmark dataset in addition to the WSJ corpus. The dimensionality of the embeddings is set to 100, and the training context window size is", "answers": [" Wall Street Journal (WSJ) portion of the Penn Treebank", "Unanswerable"], "all_classes": null, "length": 4327}
{"pred": "    The authors cite several studies that show that engineers often face challenges when applying DNN models to specific NLP tasks. These challenges include the need to choose from multiple frameworks, models and optimization techniques, which can be time-consuming and error-prone. The authors also provide examples of these challenges, such as the need to choose between different embedding layers, loss functions and regularization techniques. These examples demonstrate the need for a toolkit that can help engineers choose the right components for their specific tasks. (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (", "answers": ["By conducting a survey among engineers", "Unanswerable"], "all_classes": null, "length": 1692}
{"pred": "    On SimpleQuestions and WebQSP datasets, they achieve the state of the art. They outperform previous methods on both tasks and allow their KBQA system to achieve state-of-the-art results. They also show that their improved relation detection model contributes to the good performance in multiple ways. Their proposed hierarchical matching approach and residual learning method help to match different levels of relation information and improve the hierarchical matching between questions and relations. They also demonstrate that their improved relation detector enables their simple KBQA system to achieve state-of-the-art results on both single-relation and multi-relation KBQA tasks. Their work shows that improving KB relation detection can", "answers": ["SimpleQuestions, WebQSP", "WebQSP, SimpleQuestions"], "all_classes": null, "length": 4524}
