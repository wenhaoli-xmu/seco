{"pred": "        The ground truth for fake news is established by manually inspecting the text field within the tweets to label them as containing fake news, or not containing them (according to the characterization presented before). This annotated dataset is publicly available and can be freely reused. (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no)", "answers": ["Ground truth is not established in the paper"], "all_classes": null, "length": 3141}
{"pred": "The GhostVLAD approach is an extension of the NetVLAD approach, which was originally proposed for face recognition. The GhostVLAD approach adds ghost clusters to the NetVLAD clusters to map any noisy or irrelevant content into ghost clusters. The ghost clusters are not included during the feature aggregation stage, which helps to reduce the contribution of noisy and unwanted features to the normal VLAD clusters. The GhostVLAD approach has been shown to improve language identification performance for Indian languages. (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no)", "answers": ["extension of the NetVLAD, adds Ghost clusters along with the NetVLAD clusters", "An extension of NetVLAD which replaces hard assignment-based clustering with soft assignment-based clustering with the additon o fusing Ghost clusters to deal with noisy content."], "all_classes": null, "length": 2454}
{"pred": "        68.8% to 71.8% when applied to the IEMOCAP dataset.       \n\n        68.8% to 71.8% when applied to the IEMOCAP dataset.       \n\n        68.8% to 71.8% when applied to the IEMOCAP dataset.       \n\n        68.8% to 71.8% when applied to the IEMOCAP dataset.       \n\n        68.8% to 71.8% when applied to the IEMOCAP dataset.       \n\n        68.8% to ", "answers": ["the attention model, MDREA, also outperforms the best existing research results (WAP 0.690 to 0.688)"], "all_classes": null, "length": 3207}
{"pred": "        The additional features and context proposed are context tweets, which are the tweets that the abusive tweets have replied to or quoted. The context tweets provide significant contextual information that can help computers better understand the abusive tweets. The context tweets are used as an additional feature of neural network models. The context tweets are directly used as an additional feature of neural network models. The context tweets are concatenated with the outputs of their corresponding labeled tweets. The context tweets are used to improve the baseline models. The context tweets are used to improve the scores of several metrics. The context tweets are used to improve the recall and F1 scores for \"hateful\" labels", "answers": ["using tweets that one has replied or quoted to as contextual information", "text sequences of context tweets"], "all_classes": null, "length": 2060}
{"pred": "    FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney.", "answers": ["FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney", "FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney."], "all_classes": null, "length": 3411}
{"pred": "        Yes, both datasets contain only English data. The hashtags are from Twitter and the SemEval dataset is from the Sentiment Analysis in Twitter shared task. The task is to predict whether a tweet expresses positive, negative, or neutral sentiment. The hashtags are used to help identify the sentiment. The SemEval dataset contains tweets with hashtags and the task is to predict the sentiment of the tweet based on the hashtag and the text of the tweet. The hashtags are used to help identify the sentiment. The SemEval dataset contains tweets with hashtags and the task is to predict the sentiment of the tweet based on the hashtag and the text of the tweet.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3735}
{"pred": "    The proposed evaluation is based on the concept map generated by the system, which is compared to the reference concept map created by experts. The evaluation metrics include the number of concepts and relations, the similarity between the concepts and relations, and the overall coherence of the concept map. The evaluation is performed by human judges who are asked to assess the quality of the concept map generated by the system. The judges are given the reference concept map and the concept map generated by the system, and they are asked to rate the quality of the system-generated concept map on a scale of 1 to 5, with 1 being the lowest quality and ", "answers": ["Answer with content missing: (Evaluation Metrics section) Precision, Recall, F1-scores, Strict match, METEOR, ROUGE-2"], "all_classes": null, "length": 4263}
{"pred": "        CNN/DailyMail, New York Times Annotated Corpus (NYT), and XSum. The summaries vary in style and rewriting operations, ranging from highlights to very brief one-sentence summaries. Novel n-grams are used to measure the abstractiveness of the summaries. The proportion of novel n-grams in gold summaries is reported as a measure of their abstractiveness. The datasets are used to evaluate the performance of the proposed model on different types of summaries. (CNN/DailyMail, NYT, XSum) (Novel n-grams, Abstractiveness, Style, Rewriting operations) (Gold summaries, Abstractiveness) (", "answers": ["CNN/DailyMail news highlights, New York Times Annotated Corpus, XSum", "the CNN/DailyMail news highlights dataset BIBREF24, the New York Times Annotated Corpus (NYT; BIBREF25), XSum BIBREF22"], "all_classes": null, "length": 4369}
{"pred": "This approach compares favorably to other WSD approaches employing word embeddings. The proposed approach uses a variant of max-margin objective based on the asymmetric KL divergence energy function to capture textual entailment, which helps in capturing polysemous nature of words and reducing uncertainty per word. The approach also uses a stricter bound on KL between Gaussian mixtures to approximate KL divergence between Gaussian mixtures. The results show that the proposed approach performs better than other approaches on the benchmark word similarity and entailment datasets. The approach also captures lexical inference relations such as causality and hypernymy, which helps in capturing textual entailment. Overall, the proposed approach is", "answers": ["GM$\\_$KL achieves better correlation than existing approaches for various metrics on SCWS dataset."], "all_classes": null, "length": 2189}
{"pred": "        The ensemble method works by averaging the predictions from the constituent single models. The single models are selected using a greedy algorithm that starts with the best performing model according to validation performance and then tries adding the best performing model that has not been previously tried. The model is kept in the ensemble if it improves the validation performance and discarded otherwise. This way, each model is tried once and the best performing models are selected for the final ensemble. The algorithm is offered 10 models and selects 5 of them for the final ensemble. The ensemble method is used to improve the performance of the model by combining the predictions of multiple models. The idea is", "answers": ["simply averaging the predictions from the constituent single models"], "all_classes": null, "length": 4212}
{"pred": "    The sources of the datasets are Friends TV sitcom and Facebook messenger chats. The Friends dataset is composed of dialogues from the scripts of the Friends TV sitcom, while the EmotionPush dataset is made up of Facebook messenger chats. Both datasets are annotated by five annotators on a crowd-sourcing platform (Amazon Mechanical Turk), and the labeling work is only based on the textual content. The Friends dataset is speech-based, while the EmotionPush dataset is chat-based. The Friends dataset is annotated dialogues from the TV sitcom, while the EmotionPush dataset is made up of Facebook messenger chats. The Friends dataset is speech-based, while", "answers": ["Friends TV sitcom, Facebook messenger chats"], "all_classes": null, "length": 3185}
{"pred": "        English\n\n        English\n\n        The paper focuses on English. The paper uses English Wikipedia and Simple English Wikipedia as the source of data for text simplification. The paper also uses English as the language for the evaluation of the text simplification models. The paper does not mention any other languages. Therefore, we can conclude that the paper focuses on English. \n\n        English\n\n        The paper focuses on English. The paper uses English Wikipedia and Simple English Wikipedia as the source of data for text simplification. The paper also uses English as the language for the evaluation of the text simplification models. The paper does not mention any other languages.", "answers": ["English", "Simple English"], "all_classes": null, "length": 2243}
{"pred": "        IMDb dataset of movie reviews by BIBREF11.       ยนตรยนตรยนตรยนตรยนตรยนตรยนตรยนตรยนตรยนตรยนตรยนตรยนตรยนตรยนตรยนตรยนตรยนตรยนตรยนตรยนตรยนตรยนตรยนตรยนตรยนตรยนตรยนตรยนตรยนตรยนตรยนตรยนตรยนตรยนตรยนตรยนตรยนตรยนตรยนตรยนตรยนตรยนตรยนตรยนตรยนตรยนตรยนตรยนตรยนตรยนตรยนตรยนตรยนตรยนตรยนตรยนตรยนตรยนตรยนตรยนตรยนตรยนตรยนตรยนตรยนตรยนตรยนตรยนตรยนตรยนตรยนตรยนตรยนตรยนตรยนตรยนตรยนตรยนตรยนตรยนตรยนตรยนตรยนตรยนตรยนตรยนตรยนตรยนตรยนตรยนตรยนตรยนตรยนตรยนตรยนตรยนตรยนตรยนตรยนตรยนตรยนตรยนตรยนตรยนตรยนตรยนตรยนตรยนตรยนตรยนตรยนตรยนตรยนตรยนตร", "answers": ["IMDb dataset of movie reviews", "IMDb"], "all_classes": null, "length": 2327}
{"pred": "    92.3% on the development set and 91.2% on the test set. The system outperforms all other systems by a significant margin. (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (", "answers": ["F1 scores of 85.99 on the DL-PS data,  75.15 on the EC-MT data and 71.53 on the EC-UQ data ", "F1 of 85.99 on the DL-PS dataset (dialog domain); 75.15 on EC-MT and 71.53 on EC-UQ (e-commerce domain)"], "all_classes": null, "length": 5310}
{"pred": "Yes, they experimented with this new dataset. They recorded data from 19 participants and discarded the data of one of them due to technical difficulties with the eye-tracking calibration. Hence, they share the data of 18 participants. All participants are healthy adults (mean age = 34 (SD=8.3), 10 females). Their native language is English, originating from Australia, Canada, UK, USA or South Africa. Two participants are left-handed and three participants wear glasses for reading. Details on subject demographics can be found in Table 4. All participants gave written consent for their participation and the re-use of the data prior to", "answers": ["No"], "all_classes": null, "length": 3445}
{"pred": "        The datasets used are the Wizard of Oz method and crowdsourcing. The Wizard of Oz method involves applying the Wizard of Oz method to a set of potential users of the system, and collecting questions that the users asked. These questions are manually classified into a set of intent classes, and used to train the first version of the system. The crowdsourcing approach involves increasing the training set by adding new classes and samples whenever the system is updated. The intent classifier is trained using domain-specific word vectors created from a set of 246,945 documents related to finance. The training set is used to deploy the first classifier into the system. The training", "answers": ["Custom dataset with user questions; set of documents, twitter posts and news articles, all related to finance.", "a self-collected financial intents dataset in Portuguese"], "all_classes": null, "length": 13401}
{"pred": "    Energy sector achieved the best performance.", "answers": ["Energy with accuracy of 0.538", "Energy"], "all_classes": null, "length": 10349}
{"pred": "    They compared the performance of the SMT and various NMT models on their built dataset. The models to be tested and their configurations are as follows: SMT: Moses toolkit, KenLM, GIZA++ RNN-based NMT: Basic RNN-based NMT model, target language reversal, residual connection, word2vec Transformer-NMT: Transformer model, training configuration shown in Table 32. They used BLEU scores as metrics. The results are reported in Table 34. For RNN-based NMT, they found that target language reversal, residual connection, and word2vec can further improve the performance of the", "answers": ["RNN-based NMT model, Transformer-NMT"], "all_classes": null, "length": 3708}
{"pred": "    1. A regularization term associated with neutral features. 2. The maximum entropy of class distribution regularization term. 3. The KL divergence between reference and predicted class distribution. 4. The regularization terms are introduced to make the model more robust and practical. 5. The neutral features are features that are not informative indicators of any classes. 6. The maximum entropy term constrains the predicted class distribution on unlabeled data. 7. The KL divergence term controls the unbalance in labeled features and in the dataset. 8. The regularization terms are effective in improving the performance of the model. 9. The", "answers": ["a regularization term associated with neutral features, the maximum entropy of class distribution regularization term, the KL divergence between reference and predicted class distribution", "a regularization term associated with neutral features,  the maximum entropy of class distribution, KL divergence between reference and predicted class distribution"], "all_classes": null, "length": 3604}
{"pred": "    1) SVM with unigram, bigram, and trigram features, which is a standard yet rather strong classifier for text features; 2) SVM with average word embedding, where a document is represented as a continuous representation by averaging the embeddings of the composite words; 3) SVM with average transformed word embeddings (the INLINEFORM0 in equation EQREF6), where a document is represented as a continuous representation by averaging the transformed embeddings of the composite words; 4) two mature deep learning models on text classification, CNN BIBREF3 and Recurrent Convolutional Neural Networks (RCNN) BIBREF0", "answers": ["SVM with unigram, bigram, and trigram features, SVM with average word embedding, SVM with average transformed word embeddings, CNN, ecurrent Convolutional Neural Networks, SVM and deep learning models with comment information", "SVM with unigram, bigram, trigram features, with average word embedding, with average transformed word embeddings, CNN and RCNN, SVM, CNN, RCNN with comment information"], "all_classes": null, "length": 4512}
{"pred": "        5.5 points. (0.5 points for the baseline and 5 points for the multitask learning approach). (0.5 points for the baseline and 5 points for the multitask learning approach). (0.5 points for the baseline and 5 points for the multitask learning approach). (0.5 points for the baseline and 5 points for the multitask learning approach). (0.5 points for the baseline and 5 points for the multitask learning approach). (0.5 points for the baseline and 5 points for the multitask learning approach). (0.5 points for the", "answers": ["They decrease MAE in 0.34"], "all_classes": null, "length": 2735}
{"pred": "    The model improves interpretability by allowing different attention heads to learn different sparsity behaviors, which can be analyzed to identify head specializations. The sparsity of the attention weights also allows for more confident representations of certain types of tokens, such as positional heads. The model also learns to combine sparse and dense attention, which can be useful for certain tasks. The authors also provide a qualitative analysis of the interpretability capabilities of their models.", "answers": ["the attention heads in the proposed adaptively sparse Transformer can specialize more and with higher confidence", "We introduce sparse attention into the Transformer architecture"], "all_classes": null, "length": 4902}
{"pred": "The baseline is a context-agnostic machine translation system that produces translations of isolated sentences. The DocRepair model is trained to correct inconsistencies between these translations.雅黑\n\nQuestion: what is the main contribution of the DocRepair model?\n\nAnswer: The main contribution of the DocRepair model is that it can correct inconsistencies between sentence-level translations of a context-agnostic MT system using only monolingual document-level data.雅黑\n\nQuestion: what are the main results of the evaluation?\n\nAnswer: The main results of the evaluation are that the DocRepair model shows substantial improvements in translation quality as measured by BLEU, targeted contrastive evaluation of several discourse phenomena", "answers": [" MT system on the data released by BIBREF11", "Transformer base, two-pass CADec model"], "all_classes": null, "length": 3716}
{"pred": "    The metrics used for evaluation are Labeled Attachment Scores (LAS) for zero-shot dependency parsing and accuracy for cross-lingual natural language inference (XNLI). The LAS score measures the percentage of correctly labeled dependencies in a treebank, while the accuracy score measures the percentage of correctly classified sentences in the XNLI dataset. The results are presented in tables and figures.", "answers": ["translation probabilities, Labeled Attachment Scores (LAS)", "accuracy, Labeled Attachment Scores (LAS)"], "all_classes": null, "length": 3402}
{"pred": "        The attention module is pretrained on the ASR and MT tasks. The attention module is task-specific, so it is pretrained on the attention mechanism used in each task. The attention module is not pretrained on the ST task. The attention module is not pretrained on the ST task because it is not used in the ST task. The attention module is not pretrained on the ST task because it is not used in the ST task. The attention module is not pretrained on the ST task because it is not used in the ST task. The attention module is not pretrained on the ST task because it is not used in the ST task. The attention module", "answers": ["the model is pre-trained on CTC-based ASR task and MT task in the pre-training stage."], "all_classes": null, "length": 4656}
{"pred": "    The stylistic features obtained are emoticons, laughter expressions such as \"lol\", and patterns related to situational disparity. These features are used to detect sarcasm in text.", "answers": ["Unanswerable"], "all_classes": null, "length": 3543}
{"pred": "    The encoder has an LSTM layer. (1) (2) (3) (4) (5) (6) (7) (8) (9) (10) (11) (12) (13) (14) (15) (16) (17) (18) (19) (20) (21) (22) (23) (24) (25) (26) (27) (28) (29) (30) (31) (32) (33) (34) (35) (36) (37) (38) (39) (40)", "answers": ["LSTM", "LSTM"], "all_classes": null, "length": 2289}
{"pred": "    Yes, WordNet is useful for taxonomic reasoning for this task. It provides a set of triples that represent a directed, edge-labeled graph, which allows for the construction of natural language questions that contextualize the types of concepts being probed. The availability of glosses and example sentences in WordNet allows for the creation of natural language questions that contextualize the types of concepts being probed. The use of WordNet also allows for the evaluation of the extent to which models are able to handle complex forms of reasoning that require several inferential steps or hops. The availability of glosses and example sentences in WordNet allows for the", "answers": ["Unanswerable", "Yes"], "all_classes": null, "length": 6391}
{"pred": "        The baselines were the conventional automatic speech recognition (ASR) systems, which typically consist of several independently learned components: an acoustic model to predict context-dependent sub-phoneme states (senones) from audio, a graph structure to map senones to phonemes, and a pronunciation model to map phonemes to words. Hybrid systems combine hidden Markov models to model state dependencies with neural networks to predict states. Newer approaches such as end-to-end (E2E) systems reduce the overall complexity of the final system. The baselines were also the prior work that has explored using time-delay neural networks (TDNN), other", "answers": ["Unanswerable", "LF-MMI Attention\nSeq2Seq \nRNN-T \nChar E2E LF-MMI \nPhone E2E LF-MMI \nCTC + Gram-CTC"], "all_classes": null, "length": 1856}
{"pred": "        22,880 users", "answers": ["22,880 users", "20,000"], "all_classes": null, "length": 4160}
{"pred": "    BPE perplexity, BLEU-1/4, ROUGE-L, Distinct-1/2, Recipe-level coherence, Recipe step entailment, Human evaluation.", "answers": ["Byte-Pair Encoding perplexity  (BPE PPL),\nBLEU-1,\nBLEU-4,\nROUGE-L,\npercentage of distinct unigram (D-1),\npercentage of distinct bigrams(D-2),\nuser matching accuracy(UMA),\nMean Reciprocal Rank(MRR)\nPairwise preference over baseline(PP)", "BLEU-1/4 and ROUGE-L, likelihood of generated recipes using identical input specifications but conditioned on ten different user profiles, user matching accuracy (UMA), Mean Reciprocal Rank (MRR), neural scoring model from BIBREF33 to measure recipe-level coherence", " Distinct-1/2, UMA = User Matching Accuracy, MRR\n= Mean Reciprocal Rank, PP = Pairwise preference over baseline (evaluated for 310 recipe pairs per model)"], "all_classes": null, "length": 2673}
{"pred": "        No Answer, Yes, No, Unanswerable, Yes, No, Unanswerable, Yes, No, Unanswerable, Yes, No, Unanswerable, Yes, No, Unanswerable, Yes, No, Unanswerable, Yes, No, Unanswerable, Yes, No, Unanswerable, Yes, No, Unanswerable, Yes, No, Unanswerable, Yes, No, Unanswerable, Yes, No, Unanswerable, Yes, No, Unanswerable, Yes, No, Unanswerable, Yes, No, Unanswerable, Yes, No,", "answers": ["(1) the time the patient has been experiencing the symptom, (2) activities that trigger the symptom (to occur or worsen), (3) the extent of seriousness, (4) the frequency occurrence of the symptom, and (5) the location of symptom, No Answer", "the time the patient has been experiencing the symptom, activities that trigger the symptom, the extent of seriousness, the frequency occurrence of the symptom, the location of symptom, 9 symptoms"], "all_classes": null, "length": 3424}
{"pred": "        1000 sentences.  // The task-specific encoder is trained on 1000 sentences.  // The task-specific encoder is trained on 1000 sentences.  // The task-specific encoder is trained on 1000 sentences.  // The task-specific encoder is trained on 1000 sentences.  // The task-specific encoder is trained on 1000 sentences.  // The task-specific encoder is trained on 1000 sentences.  // The task-specific encoder is trained on 1000 sentences.  // The task-specific encoder is trained on 1000 sentences.  // The task-specific encoder is trained on ", "answers": ["57,505 sentences", "57,505 sentences"], "all_classes": null, "length": 4371}
{"pred": "    Neural machine translation tasks.rbrakk\n\n    English $\\rightarrow$ German, Romanian $\\rightarrow$ English, Japanese $\\rightarrow$ English, and English $\\rightarrow$ German.rbrakk\n\n    English $\\rightarrow$ German, Romanian $\\rightarrow$ English, Japanese $\\rightarrow$ English, and English $\\rightarrow$ German.rbrakk\n\n    English $\\rightarrow$ German, Romanian $\\rightarrow$ English, Japanese $\\rightarrow$ English, and English $\\rightarrow$ German.rbrakk\n\n    English $\\rightarrow$ German, Romanian $\\rightarrow$ English, Japanese $\\rightarrow$ English, and English $\\rightarrow$ German.rbrakk\n\n    English $\\rightarrow$ German, Romanian", "answers": ["four machine translation tasks: German -> English, Japanese -> English, Romanian -> English, English -> German", " four machine translation tasks, IWSLT 2017 German $\\rightarrow $ English BIBREF27, KFTT Japanese $\\rightarrow $ English BIBREF28, WMT 2016 Romanian $\\rightarrow $ English BIBREF29, WMT 2014 English $\\rightarrow $ German BIBREF30"], "all_classes": null, "length": 4898}
{"pred": "    The improvement in performance for Estonian in the NER task is 0.05. The fastText baseline score is 0.65, and the ELMo score is 0.70. The improvement is significant, indicating that the ELMo embeddings are better suited for the task. The ELMo embeddings capture more contextual information and are able to distinguish between named entities more accurately. The ELMo embeddings also perform better on other languages, such as Croatian and Lithuanian, where the NER datasets are smaller. Overall, the ELMo embeddings show a significant improvement over the fastText baseline in the N", "answers": ["5 percent points.", "0.05 F1"], "all_classes": null, "length": 3290}
{"pred": "    They have backgrounds in computational linguistics, political science, and the social sciences. They have experience working with large-scale textual data and have conducted research on topics such as hate speech, rumors, and conversion. They have also worked on developing computational methods for analyzing text as social and cultural data. Their work has been published in leading journals and conferences in their respective fields. They have also presented their work at international conferences and workshops. Their work has been recognized for its contributions to the field of computational linguistics and the social sciences. They have received grants and awards for their research. They have also collaborated with researchers from other disciplines to develop new", "answers": ["Unanswerable"], "all_classes": null, "length": 8506}
{"pred": "    Yes, the paper introduces an unsupervised approach to spam detection. The proposed method uses LDA to extract topic-based features from the historical tweets of each user, which are then used to discriminate human-like spammers from legitimate users. The method is unsupervised because it does not require labeled data to train a classifier. Instead, it uses the topic distribution of each user to extract features that can distinguish spammers from legitimate users. The paper does not mention any supervised methods for spam detection, so it is likely that the proposed method is unsupervised. However, the paper does not explicitly state whether the method is supervised or uns", "answers": ["No", "No"], "all_classes": null, "length": 2239}
{"pred": "    The Nguni languages are similar to each other and harder to distinguish. The same is true of the Sotho languages. (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (un", "answers": ["Nguni languages (zul, xho, nbl, ssw), Sotho languages (nso, sot, tsn)", "The Nguni languages are similar to each other, The same is true of the Sotho languages"], "all_classes": null, "length": 1877}
{"pred": "    They compared the performance of 6-layers and 9-layers sMBR models. The 9-layers model was initialized with Xavier initialization, while the 6-layers model was initialized with the first 6 layers of the 7-layers model. The deeper LSTM was also trained in the same way. The teacher model of the 9-layers model was the 8-layers model trained by sMBR, while the other teacher model was CE. The layer-wise training method was found to improve the performance of the deeper LSTM, as the model was deep. The averaged CER of sMBR models", "answers": ["Unidirectional LSTM networks with 2, 6, 7, 8, and  9 layers."], "all_classes": null, "length": 4286}
{"pred": "    29,794 articles.", "answers": ["a sample of  29,794 wikipedia articles and 2,794 arXiv papers "], "all_classes": null, "length": 4187}
{"pred": "        A group of 50 native people who were well-versed in both English and Tamil languages acted as annotators for the evaluation. A collection of samples of about 100 sentences were taken from the test set results for comparison. This set included a randomized selection of the translation results to ensure the objectivity of evaluation. Fluency and adequacy results for the RNNMorph results are tabulated. Adequacy rating was calculated on a 5-point scale of how much of the meaning is conveyed by the translation (All, Most, Much, Little, None). The fluency rating was calculated based on grammatical correctness on a", "answers": ["50 human annotators ranked a random sample of 100 translations by Adequacy, Fluency and overall ranking on a 5-point scale.", "adequacy, precision and ranking values"], "all_classes": null, "length": 5344}
{"pred": "        Yes, they test their framework performance on commonly used language pairs, such as English-to-German. They also test their framework performance on under-resourced language pairs and zero-resourced language pairs. Their framework shows promising results in both scenarios.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4472}
{"pred": "The models are evaluated based on the efficiency and accuracy of the communication schemes they learn. The efficiency is measured by the retention rate of tokens, while the accuracy is measured by the fraction of sentences generated by greedily decoding the model that exactly matches the target sentence. The models are also evaluated for their robustness and ability to adapt to different user preferences. The user study shows that the models are efficient and accurate, and that users can easily adapt to the autocomplete system. The models are also shown to be robust to changes in the user's writing style and preferences. The models are evaluated in terms of their ability to learn communication schemes that are efficient", "answers": ["by training an autocomplete system on 500K randomly sampled sentences from Yelp reviews", "efficiency of a communication scheme $(q_{\\alpha },p_{\\beta })$ by the retention rate of tokens, which is measured as the fraction of tokens that are kept in the keywords, accuracy of a scheme is measured as the fraction of sentences generated by greedily decoding the model that exactly matches the target sentence"], "all_classes": null, "length": 1873}
{"pred": "    Precision, recall, and F-measure are looked at for classification tasks. These metrics are used to evaluate the performance of a classifier in identifying the correct class for a given instance. Precision measures the proportion of instances that are correctly classified as belonging to a particular class, while recall measures the proportion of instances that are correctly classified as belonging to a particular class among all instances that actually belong to that class. F-measure is a weighted average of precision and recall, which takes into account both the proportion of correctly classified instances and the proportion of instances that are actually in the class. These metrics are used to evaluate the performance of a classifier in", "answers": ["Precision, Recall, F-measure, accuracy", "Precision, Recall and F-measure"], "all_classes": null, "length": 3044}
{"pred": "    The source domain is the domain where sufficient labeled data is available, while the target domain is the domain where very few or no labeled data is available. The goal is to transfer knowledge from the source domain to the target domain in order to alleviate the required labeling effort. (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes)", "answers": ["Book, electronics, beauty, music, IMDB, Yelp, cell phone, baby, DVDs, kitchen", "we use set 1 of the source domain as the only source with sentiment label information during training, and we evaluate the trained model on set 1 of the target domain, Book (BK), Electronics (E), Beauty (BT), and Music (M)"], "all_classes": null, "length": 5061}
{"pred": "        They compare with LSTM and other recent state-of-the-art methods.", "answers": ["Variational LSTM, CharCNN, Pointer Sentinel-LSTM, RHN, NAS Cell, SRU, QRNN, RAN, 4-layer skip-connection LSTM, AWD-LSTM, Quantized LSTM"], "all_classes": null, "length": 3319}
{"pred": "    Embedding Layer, Neural Network Layers, Loss Function, Metrics. davidjl\n\n    Embedding Layer, Neural Network Layers, Loss Function, Metrics. davidjl\n\n    Embedding Layer, Neural Network Layers, Loss Function, Metrics. davidjl\n\n    Embedding Layer, Neural Network Layers, Loss Function, Metrics. davidjl\n\n    Embedding Layer, Neural Network Layers, Loss Function, Metrics. davidjl\n\n    Embedding Layer, Neural Network Layers, Loss Function, Metrics. davidjl\n\n    Embedding Layer, Neural Network Layers, Loss Function, Metrics. davidjl\n\n    Embedding Layer, Neural Network Layers, Loss Function, Metrics. davidjl\n\n", "answers": ["Embedding Layer, Neural Network Layers, Loss Function, Metrics", "Embedding Layer, Neural Network Layers, Loss Function, Metrics"], "all_classes": null, "length": 1678}
{"pred": "    The datasets used were the Carnegie Mellon Pronouncing Dictionary and the multilingual pronunciation corpus collected by deri2016grapheme. The corpus consists of spelling–pronunciation pairs extracted from Wiktionary. The corpus is already partitioned into training and test sets. The corpus statistics are presented in Table 10. The raw IPA transcriptions extracted from Wiktionary are cleaned using an algorithm that attempts to make the transcriptions consistent with the phonemic inventories used in Phoible. The cleaned transcriptions are used in the experiments. The training corpus is limited to 10,000 words per language. The maximum number of training words", "answers": ["the Carnegie Mellon Pronouncing Dictionary BIBREF12, the multilingual pronunciation corpus collected by deri2016grapheme , ranscriptions extracted from Wiktionary", "multilingual pronunciation corpus collected by deri2016grapheme"], "all_classes": null, "length": 3244}
{"pred": "        The baselines were the results reported by Khandelwal and Sawant (BIBREF12) for BERT on the speculation detection and scope resolution tasks. The results for XLNet and RoBERTa are averaged across 5 runs for statistical significance. (BIBREF12) reports state-of-the-art results on negation scope resolution on BF, BA and SFU datasets. (BIBREF12) also reports results for BERT on the negation cue detection and scope resolution tasks. (BIBREF12) reports state-of-the-art results on negation scope resolution on BF, BA and SFU datasets.", "answers": ["varied from Maximum Entropy Classifiers (BIBREF4) to Support Vector Machines (BIBREF5,BIBREF6,BIBREF7,BIBREF8), Recursive Neural Networks (BIBREF9,BIBREF10), Convolutional Neural Networks (BIBREF11) and most recently transfer learning-based architectures like Bidirectional Encoder Representation from Transformers (BERT) (BIBREF12)"], "all_classes": null, "length": 2215}
{"pred": "    English, Spanish, Finnish, and other languages.", "answers": ["English\nFrench\nSpanish\nGerman\nGreek\nBulgarian\nRussian\nTurkish\nArabic\nVietnamese\nThai\nChinese\nHindi\nSwahili\nUrdu\nFinnish", "English, Spanish, Finnish"], "all_classes": null, "length": 4086}
{"pred": "    They test their method on Named Entity Recognition, POS tagging, text classification, and language modeling. They also test it on a task of document recommendation. Their method shows improved performance over word-based models on all these tasks. They also show that the learned embeddings can generalize to an unrelated task of document recommendation, justifying the use of hashtags as supervision for learning text representations. Their method shows improved performance over word-based models on all these tasks. They also show that the learned embeddings can generalize to an unrelated task of document recommendation, justifying the use of hashtags as supervision for learning text representations. Their method shows improved performance over word-based models", "answers": ["None"], "all_classes": null, "length": 2473}
{"pred": "Yes, they use pretrained Glove embeddings. They initialize the embeddings of the top 20K words in the vocabulary with 300-dimensional Glove embeddings. davidjl\n\nQuestion: What is the main contribution of this work?\n\nAnswer: overposting\n\n     overposting\n\nThe main contribution of this work is the introduction of neural components for fused bifocal attention and gated orthogonalization to address stay-on and never look back behavior while decoding. These components are designed to capture the specific characteristics of the task of generating natural language descriptions from structured data. davidjl\n\nQuestion: How does the proposed model outperform existing models?\n\nAnswer: overposting\n\n     overposting\n\nThe proposed", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4457}
{"pred": "        Yes, PolyReponse was evaluated against some baseline. The baseline was a traditional task-oriented dialogue system that relies on explicit semantic representations such as dialogue acts or slot-value ontologies. The evaluation showed that PolyReponse outperformed the baseline in terms of task completion rate and user satisfaction. The results suggest that the general approach to search-based dialogue used by PolyReponse is more effective than the traditional approach. Moreover, the multi-modal responses provided by PolyReponse were found to be more natural and engaging than the responses provided by the baseline system. Overall, the evaluation results indicate that PolyReponse is a promising approach to search-based", "answers": ["No", "No"], "all_classes": null, "length": 2738}
{"pred": "        They obtain psychological dimensions of people by analyzing the language used in their blogs. The blog posts are cleaned of HTML tags and tokenized, and the frequency of certain words and word categories is used to generate maps that reflect the demographics, language use, and psycholinguistic and semantic properties of the population represented in the dataset. These maps can provide valuable insights into the psychological dimensions of people.\r\r\n\r\r\n        Unanswerable.\r\r\n\r\r\n        Yes.\r\r\n\r\r\n        No.\r\r\n\r\r\n        Unanswerable.\r\r\n\r\r\n        Yes.\r\r\n\r\r\n        No.\r\r\n\r\r\n        Unanswerable.\r\r\n\r\r\n        Yes.\r\r\n\r\r\n        No.\r\r\n\r\r\n        Unanswerable.\r\r\n\r\r\n", "answers": ["using the Meaning Extraction Method", "Unanswerable"], "all_classes": null, "length": 1440}
{"pred": "        The ML methods aim to identify claims, premises, backing, rebuttals, and refutations. These components are the main building blocks of an argument, and identifying them is crucial for understanding the structure and logic of an argument. The methods use a variety of linguistic features to identify these components, such as n-grams, structural features, syntactic features, topic distribution, sentiment distribution, semantic features, coreference features, discourse features, and features based on word embeddings. The goal is to develop a system that can automatically identify these components in user-generated Web content, which can then be used for various applications such as argument mining,", "answers": ["claim, premise, backing, rebuttal, and refutation", "claim, premise, backing, rebuttal, refutation"], "all_classes": null, "length": 14472}
{"pred": "        4-grams (n=4) are aligned using PARENT. The n-grams are aligned using an entailment model to decide if they are entailed by the table. The entailment model is trained on the training set of WikiBio. The n-grams are aligned using an entailment model to decide if they are entailed by the table. The entailment model is trained on the training set of WikiBio. The n-grams are aligned using an entailment model to decide if they are entailed by the table. The entailment model is trained on the training set of WikiBio. The n-grams", "answers": ["Unanswerable", "Answer with content missing: (Parent subsections) combine precisions for n-gram orders 1-4"], "all_classes": null, "length": 3827}
{"pred": "        1,873 conversation threads, roughly 14k tweets. (Twitter) (14k tweets) (1,873 conversation threads) (Twitter) (14k tweets) (1,873 conversation threads) (Twitter) (14k tweets) (1,873 conversation threads) (Twitter) (14k tweets) (1,873 conversation threads) (Twitter) (14k tweets) (1,873 conversation threads) (Twitter) (14k tweets) (1,873 conversation threads) (Twitter) (14k tweets) (1,873 conversation threads) (Twitter) (14k tweets) (", "answers": ["1,873 Twitter conversation threads, roughly 14k tweets", "1,873 Twitter conversation threads, roughly 14k tweets"], "all_classes": null, "length": 3721}
{"pred": "        English, Finnish, Spanish, Mandarin Chinese, French, Russian, Polish, Hebrew, Estonian, Welsh, Kiswahili, and Yue Chinese.        //        //        //        //        //        //        //        //        //        //        //        //        //        //        //        //        //        //        //        //        //        //        //        //        //        //        //        //        //        //        //        //        //        //        //        //        //        //        //        //        //        //        //        //        //        //        //        //", "answers": ["Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese", "Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese"], "all_classes": null, "length": 14660}
{"pred": "    Wikipedia data and Reddit CMV data.", "answers": [" `Conversations Gone Awry' dataset, subreddit ChangeMyView", "An expanded version of the existing 'Conversations Gone Awry' dataset and the ChangeMyView dataset, a subreddit whose only annotation is whether the conversation required action by the Reddit moderators. "], "all_classes": null, "length": 4718}
{"pred": "No, the pipeline components were not based on deep learning models. The pipeline components were based on existing NLP tools and techniques, such as part-of-speech tagging, named entity recognition, dependency parsing, and semantic role labeling. These tools and techniques are not based on deep learning models. The pipeline components were designed to be modular and independent of specific language, allowing for easy adaptation to other languages. The pipeline components were evaluated against existing systems and found to perform better than the existing systems. The pipeline components were developed within the context of the Agatha project, which aimed to develop an intelligent analysis system for open source information for surveillance and crime", "answers": ["No", "No"], "all_classes": null, "length": 2276}
{"pred": "    The quality of the data is empirically evaluated by applying various sanity checks to the translations. These checks include computing sentence-level BLEU scores with the NLTK implementation between the human translations and the automatic translations produced by a state-of-the-art system, manually inspecting examples where the source transcript is identical to the translation, measuring the perplexity of the translations using a language model trained on a large amount of clean monolingual data, computing the ratio of English characters in the translations, and calculating similarity scores between transcripts and translations based on LASER cross-lingual sentence embeddings. Samples with low scores are manually inspected and sent back for translation", "answers": ["Validated transcripts were sent to professional translators., various sanity checks to the translations,  sanity check the overlaps of train, development and test sets", "computed sentence-level BLEU, We manually inspected examples where the source transcript was identical to the translation, measured the perplexity of the translations, computed the ratio of English characters in the translations, calculate similarity scores between transcripts and translations"], "all_classes": null, "length": 2435}
{"pred": "    The authors propose a novel multimodal dual recurrent encoder model that simultaneously utilizes text data, as well as audio signals, to permit the better understanding of speech data. The model encodes the information from audio and text sequences using dual RNNs and then combines the information from these sources using a feed-forward neural model to predict the emotion class. The authors conduct extensive experiments to show that their proposed model outperforms other state-of-the-art methods in classifying the four emotion categories, and accuracies ranging from 68.8% to 71.8% are obtained when the model is applied to the IEMOCAP dataset.", "answers": ["combines the information from these sources using a feed-forward neural model", "encodes the information from audio and text sequences using dual RNNs and then combines the information from these sources using a feed-forward neural model"], "all_classes": null, "length": 3201}
{"pred": "        2.11 BLEU, 1.7 FKGL and 1.07 SARI. (The model improved by 2.11 BLEU, 1.7 FKGL and 1.07 SARI.)       \n\n        2.11 BLEU, 1.7 FKGL and 1.07 SARI. (The model improved by 2.11 BLEU, 1.7 FKGL and 1.07 SARI.)       \n\n        2.11 BLEU, 1.7 FKGL and 1.07 SARI. (The model improved by ", "answers": ["For the WikiLarge dataset, the improvement over baseline NMT is 2.11 BLEU, 1.7 FKGL and 1.07 SARI.\nFor the WikiSmall dataset, the improvement over baseline NMT is  8.37 BLEU.", "6.37 BLEU"], "all_classes": null, "length": 2271}
{"pred": "        52% of the cases. (Table TABREF30) (Human evaluation) (DocRepair translation was marked better in 73% of the cases.) (Table TABREF30) (Human evaluation) (Strong preference of the annotators for corrected translations over the baseline ones.) (Table TABREF30) (Human evaluation) (52% of the cases, 73% of the cases, Strong preference) (Table TABREF30) (Human evaluation) (Strong preference of the annotators for corrected translations over the baseline ones.) (Table TABREF30) (Human evaluation) (52% of the cases, ", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3711}
{"pred": "    A tweet goes viral if it is retweeted more than 1000 times. (1) A tweet goes viral if it is retweeted more than 1000 times. (2) A tweet goes viral if it is retweeted more than 1000 times. (3) A tweet goes viral if it is retweeted more than 1000 times. (4) A tweet goes viral if it is retweeted more than 1000 times. (5) A tweet goes viral if it is retweeted more than 1000 times. (6) A tweet goes viral if it is retweeted more than 100", "answers": ["Viral tweets are the ones that are retweeted more than 1000 times", "those that contain a high number of retweets"], "all_classes": null, "length": 3144}
{"pred": "        BERT performs best by itself. (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no", "answers": ["BERT"], "all_classes": null, "length": 1507}
{"pred": "The data was collected using crowdsourcing and an Android application. Each respondent installed the application on their personal device and recorded several phrases in different sessions. The Android application did various checks on each utterance and if it passed all of them, the respondent was directed to the next phrase. For more information about data collection scenario, please refer to BIBREF4.\r\r\n\r\r\n        نويسنده", "answers": ["Android application"], "all_classes": null, "length": 3795}
{"pred": "    Deep learning models and logistic regression classifiers are used for RQE. The deep learning model is based on a neural network proposed by Bowman et al. (2015), while the logistic regression classifier uses a feature-based approach. The deep learning model uses GloVe word embeddings, while the logistic regression classifier uses a list of nine features including similarity measures and morphosyntactic features. The deep learning model achieves better results on three datasets (SNLI, MultiNLI, and Quora), while the logistic regression classifier achieves the best accuracy on the Clinical-RQE dataset. When tested on medical RQE data, the deep learning model performs worse", "answers": ["Logistic Regression, neural networks"], "all_classes": null, "length": 7257}
{"pred": "    The benchmark dataset is the Social Honeypot dataset, which is a collection of legitimate users and spammers on Twitter. The quality of the dataset is high, as it was created by deploying 60 seed social accounts on Twitter to attract spammers and collecting their interactions with legitimate users. The dataset has been extensively explored in the paper and has been used to validate the effectiveness of the proposed features. The dataset is publicly available and can be accessed by researchers. The quality of the dataset is high, as it was created by deploying 60 seed social accounts on Twitter to attract spammers and collecting their interactions with legitimate users. The dataset", "answers": ["Social Honeypot dataset (public) and Weibo dataset (self-collected); yes", "Social Honeypot, which is not of high quality"], "all_classes": null, "length": 2242}
{"pred": "    The decoder has an LSTM one component at a time, e.g. the tag PRO;NOM;SG;1 is predicted as a sequence of four components, PRO, NOM, SG, 1. davidjl\n\nQuestion: What is the main objective of the auxiliary task?\n\nAnswer:?\"\n\n\n\n    The main objective of the auxiliary task is to predict the MSD tag of the target form. davidjl\n\nQuestion: What is the advantage of using an LSTM to encode the context?\n\nAnswer:?\"\n\n\n\n    The advantage of using an LSTM to encode the context is that it can capture long-term dependencies in the data. davidjl\n\nQuestion: What is", "answers": ["LSTM", "LSTM"], "all_classes": null, "length": 2289}
{"pred": "        No, they report results on English and Chinese data. They also report results on different datasets, including social media (FSD and Twitter) and news media (Google). The results show that AEM performs better than the baseline approaches on all datasets.", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3838}
{"pred": "        BERT with ensemble voting scheme and postprocessing step. It achieved F1 score of 0.673 on dev (external) set. (r19)        overposting", "answers": ["For SLC task, the \"ltuorp\" team  has the best performing  model (0.6323/0.6028/0.6649 for F1/P/R  respectively) and for FLC task the \"newspeak\" team  has the best performing  model (0.2488/0.2863/0.2201 for F1/P/R respectively)."], "all_classes": null, "length": 1541}
{"pred": "        The baseline was the M2M Transformer NMT model (b3) which achieved best results for most of the translation directions and competitive results for the rest. (b3) is the baseline. (b3) is the baseline. (b3) is the baseline. (b3) is the baseline. (b3) is the baseline. (b3) is the baseline. (b3) is the baseline. (b3) is the baseline. (b3) is the baseline. (b3) is the baseline. (b3) is the baseline. (b3) is the baseline", "answers": ["pivot-based translation relying on a helping language BIBREF10, nduction of phrase tables from monolingual data BIBREF14 , attentional RNN-based model (RNMT) BIBREF2, Transformer model BIBREF18, bi-directional model BIBREF11, multi-to-multi (M2M) model BIBREF8, back-translation BIBREF17", "M2M Transformer"], "all_classes": null, "length": 4542}
{"pred": "    0.7033\n\nQuestion: What was their lowest precision score?\n\nAnswer: νεφοκάλυψης\n\n    0.1119\n\nQuestion: What was their highest F-measure score?\n\nAnswer: νεφοκάλυψης\n\n    0.2862\n\nQuestion: What was their lowest F-measure score?\n\nAnswer: νεφοκάλυψης\n\n    0.0786\n\nQuestion: What was their highest MRR score?\n\nAnswer: νεφοκάλυψης\n\n    0.4167\n\nQuestion: What was their lowest MRR score?\n\nAnswer: νεφοκάλυψης\n\n    0.1800\n\nQuestion: What was their highest macro F1 score?\n\nAnswer: νεφοκάλυψης\n\n", "answers": ["0.7033", "0.7033"], "all_classes": null, "length": 6810}
{"pred": "    Word embeddings are explored in the paper. The basic idea is that the neural network learns a representation of a word by word co-occurrence matrix. The continuous bag of words (CBOW) approach maximizes the probability of a word given its context, while the Skip-gram approach maximizes the probability of the context given a word. These approaches have been used in numerous recent papers. Muneeb et al. (2015) and Chiu et al. (2016) trained both the Skip-gram and CBOW models over the PubMed Central Open Access (PMC) corpus. Pakhomov et al. (201", "answers": ["Skip–gram, CBOW", "integrated vector-res, vector-faith, Skip–gram, CBOW"], "all_classes": null, "length": 4259}
{"pred": "    They match words before reordering them by translating each word in the source language into English using a bilingual dictionary. The resulting English word is then used to look up the corresponding word in the assisting language. This process is repeated for all words in the source language sentence. The resulting sentence in the assisting language is then reordered to match the word order of the source language.雅黑\n\nQuestion: How does pre-ordering improve translation quality?\n\nAnswer:雅黑\n\n    Pre-ordering improves translation quality by reducing the word order divergence between the source and assisting languages. This leads to more consistent contextual representations generated by the encoder, which in turn leads to", "answers": ["Unanswerable", "CFILT-preorder system"], "all_classes": null, "length": 2231}
{"pred": "        No, the paper does not explore extraction from electronic health records. The paper focuses on extraction from scientific articles. The paper mentions that electronic health records are beneficial for doctors consulting in complicated medical cases, but does not explore extraction from them. The paper also mentions that extraction from electronic health records is a challenging task due to the complexity of the data and the need for domain-specific knowledge. The paper does not provide any specific examples of extraction from electronic health records. The paper does not explore extraction from electronic health records. The paper focuses on extraction from scientific articles. The paper mentions that electronic health records are beneficial for doctors consulting in complicated medical cases", "answers": ["Yes"], "all_classes": null, "length": 3035}
{"pred": "    The experts used for annotation were seven legal experts with training in privacy law. They were tasked with identifying relevant evidence within the privacy policy, as well as providing meta-annotation on the question's relevance, subjectivity, OPP-115 category, and how likely any privacy policy is to contain the answer to the question asked. davidjl\n\nQuestion: What is the distribution of questions across OPP-115 categories?\n\nAnswer: davidjl\n\n     davidjl\n\n    First party and third party related questions are the largest categories, forming nearly 66.4% of all questions asked to the privacy assistant. davidjl\n\nQuestion: What are the sources of disagreement among", "answers": ["Individuals with legal training", "Yes"], "all_classes": null, "length": 3846}
{"pred": "    The models used for painting embedding are a CNN-RNN generative model and a sequence-to-sequence model trained by policy gradient. The models used for language style transfer are a sequence-to-sequence model with global attention and a sequence-to-sequence model with pointer networks. The models are combined using an intermediate representation.rbrakk\n\nQuestion: What are the limitations of the current approach?\n\nAnswer:limitations\n\n    The current approach does not have an end-to-end dataset, which means that the generated English poem may not work well with Shakespeare style transfer. This happens when the style transfer dataset does not have similar words in the training set of sentences", "answers": ["generating a poem from images we use an existing actor-critic architecture, various types of sequence to sequence models"], "all_classes": null, "length": 1653}
{"pred": "    The RNN layer works better on top of BERT for long documents. The transformer layer works better for short documents. The RNN layer is able to capture long-term dependencies in the text, while the transformer layer is better at capturing short-term dependencies. The RNN layer also has a lower computational complexity than the transformer layer, making it more efficient for long documents. However, the transformer layer is more powerful and can handle longer sequences than the RNN layer. Therefore, the choice of layer depends on the specific task and the length of the input text. For long documents, the RNN layer is generally preferred, while for short", "answers": ["Transformer over BERT (ToBERT)", "The transformer layer"], "all_classes": null, "length": 2655}
{"pred": "    Yes, the authors hypothesize that humans' robustness to noise is due to their general knowledge. They believe that humans can utilize general knowledge in addition to the knowledge contained in each given passage-question pair, which allows them to achieve good performance on evaluation examples without training examples. The authors also believe that humans can utilize general knowledge to overcome the limitations of neural networks in MRC models. (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4127}
{"pred": "        The authors addressed three topics of cyberbullying: personal attack, racism, and sexism. They used three datasets to study the prevalence of cyberbullying on these topics across different social media platforms. The Formspring dataset contains examples of cyberbullying on a teen-oriented Q&A forum, the Twitter dataset contains examples of racism and sexism, and the Wikipedia dataset contains examples of personal attack. The authors also discuss the challenges of detecting cyberbullying in social media, such as the subjectivity of defining what constitutes cyberbullying and the difficulty of identifying cyberbullies due to the option of anonymity in certain social networks. They propose a deep", "answers": ["personal attack, racism, and sexism", "racism, sexism, personal attack, not specifically about any single topic"], "all_classes": null, "length": 3244}
{"pred": "        They obtain the new context representation by splitting the context into three disjoint regions based on the two relation arguments: the left context, the middle context and the right context. Since in most cases the middle context contains the most relevant information for the relation, they want to focus on it but not ignore the other regions completely. Hence, they propose to use two contexts: (1) a combination of the left context, the left entity and the middle context; and (2) a combination of the middle context, the right entity and the right context. Due to the repetition of the middle context, they force the network to pay special attention to", "answers": ["They use two independent convolutional and max-pooling layers on (1) a combination of the left context, the left entity and the middle context; and (2) a combination of the middle context, the right entity and the right context. They concatenated the two results after pooling to get the new context representation."], "all_classes": null, "length": 2435}
{"pred": "        4 (PER, LOC, ORG, MISC)", "answers": ["OurNepali contains 3 different types of entities, ILPRL contains 4 different types of entities", "three"], "all_classes": null, "length": 2851}
{"pred": "    The resulting annotated data is of higher quality. The expert annotations are more consistent and accurate than the crowd annotations. The expert annotations also provide more useful information for the information extraction task. The resulting annotated data is more useful for training the information extraction model. The expert annotations are more valuable than the crowd annotations. The expert annotations are more reliable and trustworthy than the crowd annotations. The expert annotations are more likely to be correct and accurate than the crowd annotations. The expert annotations are more likely to be useful and informative than the crowd annotations. The expert annotations are more likely to be relevant and important for the information extraction task. The expert annotations are", "answers": ["improvement when the difficult subset with expert annotations is mixed with the remaining crowd annotation is 3.5 F1 score, much larger than when a random set of expert annotations are added"], "all_classes": null, "length": 4399}
{"pred": "        65% of the speakers are men, speaking more than 75% of the time. (unanswerable) (yes) (no) (unanswerable) (no) (unanswerable) (no) (unanswerable) (no) (unanswerable) (no) (unanswerable) (no) (unanswerable) (no) (unanswerable) (no) (unanswerable) (no) (unanswerable) (no) (unanswerable) (no) (unanswerable) (no) (unanswerable) (no) (", "answers": ["Women represent 33.16% of the speakers"], "all_classes": null, "length": 4055}
{"pred": "    The approach achieves state of the art results on the Multi30K dataset. (Standard setup) (Standard setup) (Standard setup) (Standard setup) (Standard setup) (Standard setup) (Standard setup) (Standard setup) (Standard setup) (Standard setup) (Standard setup) (Standard setup) (Standard setup) (Standard setup) (Standard setup) (Standard setup) (Standard setup) (Standard setup) (Standard setup) (Standard setup) (Standard setup) (Standard setup) (Standard setup) (Standard setup) (Standard setup) (Standard setup) (Standard setup) (Standard setup)", "answers": ["the English-German dataset"], "all_classes": null, "length": 1833}
{"pred": "        The strong baselines model is compared to the models proposed by BIBREF20 and BIBREF18. These models use unsupervised segmentation features to improve performance. The models proposed by BIBREF20 and BIBREF18 are considered strong baselines because they achieve state-of-the-art performance on the SIGHAN Bakeoff 2005 dataset. The models proposed by BIBREF20 and BIBREF18 use unsupervised segmentation features to improve performance. The models proposed by BIBREF20 and BIBREF18 are considered strong baselines because they achieve state-of-the-art performance on the SIGHAN Bake", "answers": ["Baseline models are:\n- Chen et al., 2015a\n- Chen et al., 2015b\n- Liu et al., 2016\n- Cai and Zhao, 2016\n- Cai et al., 2017\n- Zhou et al., 2017\n- Ma et al., 2018\n- Wang et al., 2019"], "all_classes": null, "length": 3629}
{"pred": "    Multilayer Perceptron (MLP) and Logistic Regression (LR) are used as the target models. (1) (2) (3) (4) (5) (6) (7) (8) (9) (10) (11) (12) (13) (14) (15) (16) (17) (18) (19) (20) (21) (22) (23) (24) (25) (26) (27) (28) (29) (30) (31) (32) (33) (34) (35", "answers": ["probabilistic model", "Logistic Regression, Multilayer Perceptron"], "all_classes": null, "length": 4475}
{"pred": "    NLTK, Stanford CoreNLP, and TwitterNLP. (NLTK, Stanford CoreNLP, TwitterNLP) (NLTK, Stanford CoreNLP, TwitterNLP) (NLTK, Stanford CoreNLP, TwitterNLP) (NLTK, Stanford CoreNLP, TwitterNLP) (NLTK, Stanford CoreNLP, TwitterNLP) (NLTK, Stanford CoreNLP, TwitterNLP) (NLTK, Stanford CoreNLP, TwitterNLP) (NLTK, Stanford CoreNLP, TwitterNLP) (NLTK, Stanford CoreNLP, Twitter", "answers": ["BIBREF17, BIBREF18, TensiStrength BIBREF13, TwitterNLP BIBREF6, BIBREF19, CogComp-NLP BIBREF20, Stanford NLP NER BIBREF21", "BIBREF23, BIBREF17, BIBREF18, BIBREF19, BIBREF24, BIBREF25, BIBREF26"], "all_classes": null, "length": 1452}
{"pred": "    The SQuAD dataset is used for experiments. The SQuAD dataset contains 536 Wikipedia articles and 100k crowd-sourced question-answer pairs. The questions are written by crowd-workers and the answers are spans of tokens in the articles. The data is split into training, development, and test sets. The data statistics are given in Table 27. The evaluation metrics used are BLEU-1, BLEU-2, BLEU-3, BLEU-4, METEOR, and ROUGE-L. The baseline models compared are s2s, NQG++, M2S+cp, s2", "answers": ["SQuAD", "SQuAD"], "all_classes": null, "length": 3757}
{"pred": "    Existing approaches for modelling urban environments and identifying points-of-interest and itineraries have been proposed. However, the usefulness of Flickr for characterizing the natural environment is less well-understood. Some recent studies have highlighted that Flickr tags capture valuable ecological information, which can be used as a complementary source to more traditional sources. However, ecologists have mostly used social media to conduct manual evaluations of image content with little automated exploitation of the associated tags. One recent exception is a study that found bag-of-words representations derived from Flickr tags to give promising results for predicting a range of different environmental phenomena. The main hypothesis in this paper is that by", "answers": ["BOW-Tags, BOW-KL(Tags), BOW-All, GloVe"], "all_classes": null, "length": 4658}
{"pred": "Yes, they use attention to compute the similarity score between passages and questions. The attention function is used to compute the similarity score between passages and questions as: A question-aware passage representation is computed as: After that, they use the method of BIBREF13 to apply self attention to the passage. Finally, the final memory is formed by concatenating the question-aware passage representation and the passage representation. The attention mechanism is used to focus on the relevant parts of the passage and question, which helps the model to better understand the context and generate more accurate answers. (Note: The attention mechanism is not explicitly mentioned in the article, but it", "answers": ["Yes", "Yes"], "all_classes": null, "length": 1687}
{"pred": "        CSAT dataset for CSAT prediction, consisting of spoken transcripts (automatic via ASR). 20 newsgroups for topic identification task, consisting of written text; Fisher Phase 1 corpus for topic identification task, consisting of spoken transcripts (manual).       ", "answers": ["CSAT dataset, 20 newsgroups, Fisher Phase 1 corpus", "CSAT dataset , 20 newsgroups, Fisher Phase 1 corpus"], "all_classes": null, "length": 2652}
{"pred": "    IMDb movie review dataset\n\n    IMDb movie review dataset\n\n    IMDb movie review dataset\n\n    IMDb movie review dataset\n\n    IMDb movie review dataset\n\n    IMDb movie review dataset\n\n    IMDb movie review dataset\n\n    IMDb movie review dataset\n\n    IMDb movie review dataset\n\n    IMDb movie review dataset\n\n    IMDb movie review dataset\n\n    IMDb movie review dataset\n\n    IMDb movie review dataset\n\n    IMDb movie review dataset\n\n    IMDb movie review dataset\n\n    IMDb movie review dataset\n\n    IMDb movie review dataset\n\n    IMDb movie review dataset\n\n    IMDb movie review dataset\n\n    IMDb movie review dataset\n\n    IMDb movie review dataset\n\n    IMDb", "answers": ["the IMDb movie review dataset BIBREF17", "IMDb movie review"], "all_classes": null, "length": 3432}
{"pred": "Yes, previous work has evaluated these tasks in the context of LSTM-based models. The tasks include subject-verb agreement, reflexive anaphora, and negative polarity items. The stimuli used in these previous works were manually constructed and controlled, allowing for greater coverage and control than naturally occurring sentences. The evaluation protocol was also adapted to the bidirectional nature of the BERT model, which required discarding some of the stimuli. The results showed that the BERT models performed very well on all the syntactic tasks, with high scores in the vast majority of cases. The BERT-Base model outperformed the BERT-Large model", "answers": ["Yes", "Yes"], "all_classes": null, "length": 1464}
{"pred": "        No, the datasets for sentiment analysis are not balanced. The datasets are skewed towards positive and neutral sentiments, with negative sentiments being underrepresented. This can lead to biased results in sentiment analysis. To address this issue, researchers have proposed various techniques such as oversampling, undersampling, and resampling to balance the datasets. These techniques aim to improve the performance of sentiment analysis models by ensuring that the training data is representative of the real-world distribution of sentiments. (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswer", "answers": ["No"], "all_classes": null, "length": 1441}
{"pred": "        The invertibility condition is that the neural projector is invertible, meaning that the inverse of the projection exists and is also invertible. This condition allows for tractable exact inference and marginal likelihood computation. (EQREF19) (EQREF22) (FIGREF24) (FIGREF24) (FIGREF24) (FIGREF24) (FIGREF24) (FIGREF24) (FIGREF24) (FIGREF24) (FIGREF24) (FIGREF24) (FIGREF24) (FIGREF24) (FIGREF24) (FIGREF24) (FIGREF24) (FIG", "answers": ["The neural projector must be invertible.", "we constrain our neural projector with two requirements: (1) INLINEFORM0 and (2) INLINEFORM1 exists"], "all_classes": null, "length": 4323}
{"pred": "    The proposed qualitative annotation schema looks like a taxonomy of dimensions of interest, including factual correctness, required reasoning, knowledge, and linguistic complexity. The dimensions are further enriched with metrics based on lexical cues to approximate a lower bound for the complexity of the reading comprehension task. The resulting taxonomy is shown in Figure 10. The full catalogue of features, their description, detailed annotation guidelines, and illustrating examples can be found in Appendix. (Figure 10) (Appendix) (Dimensions of Interest: Factual Correctness, Required Reasoning, Knowledge, Linguistic Complexity) (Metrics based on lexical cues) (Full catalogue of features, their", "answers": ["The resulting taxonomy of the framework is shown in Figure FIGREF10", "FIGREF10"], "all_classes": null, "length": 4958}
{"pred": "        89,042 sentence pairs for WikiSmall and 296,402 sentence pairs for WikiLarge. 11.6M words and 82K vocabulary for simplified data. 2,000 sentences for development and 359 sentences for testing. 2,359 sentences split into 2,000 for development and 359 for testing. 2,000 for development and 359 for testing. 2,000 for development and 359 for testing. 2,000 for development and 359 for testing. 2,000 for development and 359 for testing. 2,000 for development and 359 for testing", "answers": ["training set has 89,042 sentence pairs, and the test set has 100 pairs, training set contains 296,402, 2,000 for development and 359 for testing", "WikiSmall  89 142 sentence pair and  WikiLarge 298 761 sentence pairs. "], "all_classes": null, "length": 2266}
{"pred": "    Vanilla ST baseline, Encoder pre-training, Decoder pre-training, Encoder-decoder pre-training, One-to-many setting, Many-to-one setting, Many-to-many setting, Many-to-many+pre-training, Triangle+pre-train. }\r\r\n\n\nQuestion: What are the experimental results?\n\nAnswer:?\"\n\n\n\n        Test set        Average        Vanilla ST baseline        12.4        Many-to-many+pre-training        15.6        Many-to-many+pre-training        17.8        Many-to-many+pre-training        20.0        Many-to-many+pre-training        22.2        Many-to-many+pre", "answers": ["Vanilla ST baseline, encoder pre-training, in which the ST encoder is initialized from an ASR model, decoder pre-training, in which the ST decoder is initialized from an MT model, encoder-decoder pre-training, where both the encoder and decoder are pre-trained, many-to-many multi-task model where the encoders and decoders are derived from pre-trained ASR and MT models, Triangle+pre-train: BIBREF18 DBLP:conf/naacl/AnastasopoulosC18 proposed a triangle multi-task strategy for speech translation", "Vanilla ST baseline, Pre-training baselines, Multi-task baselines, Many-to-many+pre-training, Triangle+pre-train", "Vanilla ST baseline: The vanilla ST BIBREF9 has only a speech encoder and a decoder. It is trained from scratch on the ST-TED corpus.\n\nPre-training baselines: We conduct three pre-training baseline experiments: 1) encoder pre-training, in which the ST encoder is initialized from an ASR model; 2) decoder pre-training, in which the ST decoder is initialized from an MT model; and 3) encoder-decoder pre-training, where both the encoder and decoder are pre-trained. The ASR model has the same architecture with vanilla ST model, trained on the mixture of ST-TED and TED-LIUM2 corpus. The MT model has a text encoder and decoder with the same architecture of which in TCEN. It is first trained on WMT data (out-of-domain) and then fine-tuned on in-domain data.\n\nMulti-task baselines: We also conduct three multi-task baseline experiments including one-to-many setting, many-to-one setting, and many-to-many setting. In the first two settings, we train the model with $\\alpha _{st}=0.75$ while $\\alpha _{asr}=0.25$ or $\\alpha _{mt}=0.25$. For many-to-many setting, we use $\\alpha _{st}=0.6, \\alpha _{asr}=0.2$ and $\\alpha _{mt}=0.2$.. For MT task, we use only in-domain data.\n\nMany-to-many+pre-training: We train a many-to-many multi-task model where the encoders and decoders are derived from pre-trained ASR and MT models. "], "all_classes": null, "length": 4704}
{"pred": "        English and Arabic.rbrakk\n\n        English and Arabic.rbrakk\n\n        English and Arabic.rbrakk\n\n        English and Arabic.rbrakk\n\n        English and Arabic.rbrakk\n\n        English and Arabic.rbrakk\n\n        English and Arabic.rbrakk\n\n        English and Arabic.rbrakk\n\n        English and Arabic.rbrakk\n\n        English and Arabic.rbrakk\n\n        English and Arabic.rbrakk\n\n        English and Arabic.rbrakk\n\n        English and Arabic.rbrakk\n\n        English and Arabic.rbrakk\n\n        English and Arabic.rbrakk\n\n        English and Arabic.rbrakk\n\n        English and Arabic.rbrakk\n\n        English and Arabic.rbrakk\n\n        English", "answers": ["Unanswerable", "English"], "all_classes": null, "length": 4513}
{"pred": "    SVM, BiLSTM, CNN.", "answers": ["linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM trained on word unigrams,  bidirectional Long Short-Term-Memory (BiLSTM),  Convolutional Neural Network (CNN) "], "all_classes": null, "length": 2250}
{"pred": "        The answered questions measure for the usefulness of the answer. The answer is considered useful if it is relevant, accurate, and complete. The answer is also considered useful if it is well-written and easy to understand. The answer is not considered useful if it is irrelevant, inaccurate, or incomplete. The answer is also not considered useful if it is poorly written or difficult to understand. The answer is not considered useful if it is not relevant to the question. The answer is also not considered useful if it is not accurate or complete. The answer is not considered useful if it is not well-written or easy to understand. The answer is not considered", "answers": ["No"], "all_classes": null, "length": 1561}
{"pred": "    GloVe and Edinburgh embeddings were used.雅黑\n\nQuestion: what are the limitations of the system?\n\nAnswer:limitations of the system are that it has difficulties in capturing the overall sentiment and it leads to amplifying or vanishing intensity signals. It also has difficulties in understanding sarcastic tweets and predicting sentences having deeper emotion and sentiment.雅黑\n\nQuestion: what are some potential future directions for research?\n\nAnswer:potential future directions for research include using sentence embeddings to improve performance, benchmarking the performance of the system, and exploring other affective computing tasks on social media text.雅黑\n\nQuestion: what are some potential applications of the system?\n\n", "answers": ["Pretrained word embeddings  were not used", "GloVe, Edinburgh embeddings BIBREF14, Emoji embeddings BIBREF16"], "all_classes": null, "length": 1771}
{"pred": "        The results on the new dataset show that the personalized generative models can generate plausible, personalized, and coherent recipes preferred by human evaluators for consumption. The models also introduce a set of automatic coherence measures for instructional texts as well as personalization metrics to support their claims. The personalized models outperform the baseline in BPE perplexity, and achieve higher user matching accuracy and mean reciprocal rank. The Prior Name model achieves the best user matching accuracy and mean reciprocal rank, revealing that prior recipe names are strong signals for personalization. Moreover, the addition of attention mechanisms to capture these signals improves language modeling performance over a strong non-personalized", "answers": ["average recipe-level coherence scores of 1.78-1.82, human evaluators preferred personalized model outputs to baseline 63% of the time"], "all_classes": null, "length": 2666}
{"pred": "        The combination of rewards for reinforcement learning is a harmonic mean of irony reward and sentiment reward. The irony reward is the difference between the irony score of the input sentence and that of the output sentence, while the sentiment reward is one minus the absolute value of the difference between the standardized sentiment score of the input sentence and that of the generated sentence. The harmonic mean encourages the model to focus on both the irony accuracy and the sentiment preservation.", "answers": ["irony accuracy, sentiment preservation", " irony accuracy and sentiment preservation"], "all_classes": null, "length": 4592}
{"pred": "        Since we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer as shown in Figure FIGREF12 for \"Starry Night\" with a low average content score. This happens when the style transfer dataset does not have similar words in the training set of sentences. A solution would be to expand the style transfer dataset, for a better representation of the poem data.       ยนตร\n\n       ยนตร\n\n       ยนตร\n\n       ยนตร\n\n       ยนตร\n\n       ยนตร\n\n       ยนตร\n\n       ยนตร\n\n       ยนตร\n\n       ยนตร\n\n       ยนตร\n\n       ยนตร\n\n       ยนตร\n\n       ยนตร\n\n       ยนตร\n\n", "answers": ["Since we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer", "we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer as shown in Figure FIGREF12 for \"Starry Night\" with a low average content score"], "all_classes": null, "length": 1651}
{"pred": "        The existing benchmarks they compared to were the Affective Text dataset, the Fairy Tales dataset, and the ISEAR dataset. These datasets are commonly used for the evaluation of emotion detection systems. The Affective Text dataset contains news headlines annotated with emotions and valence, while the Fairy Tales dataset contains sentences from fairy tales annotated with emotions. The ISEAR dataset contains reports by approximately 3000 respondents from all over the world, for a total of 7665 sentences labelled with an emotion. The authors mapped the emotions used in these datasets to a subset of emotions that they used in their experiments: anger, joy, sadness, and", "answers": ["Affective Text, Fairy Tales, ISEAR", " Affective Text dataset, Fairy Tales dataset, ISEAR dataset"], "all_classes": null, "length": 3390}
{"pred": "    The distribution results showed that tweets containing fake news were created more recently, had less favourites, used more hashtags, and had a higher proportion of unverified accounts. They also had a higher number of followers and a higher ratio of friends/followers. Additionally, they used fewer mentions and had more URLs. Finally, the content of viral fake news was highly polarized. (10 points) (10 points) (10 points) (10 points) (10 points) (10 points) (10 points) (10 points) (10 points) (10 points) (10 points) (10 points) (10 points) (", "answers": ["Distributions of Followers, Friends and URLs are significantly different between the set of tweets containing fake news and those non containing them, but for Favourites, Mentions, Media, Retweets and Hashtags they are not significantly different"], "all_classes": null, "length": 3164}
{"pred": "    The dataset of hashtags is sourced from the Stanford Sentiment Analysis Dataset. The dataset consists of 1,108 unique English hashtags from 1,268 randomly selected tweets. The hashtags are manually segmented by crowdsourced workers. The dataset also includes additional corrections made by the authors. The authors also created a new expert curated dataset, which includes all 12,594 unique English hashtags and their associated tweets from the same Stanford dataset. The authors used the same Stanford dataset to create both datasets. The authors also used the Stanford Sentiment Analysis Dataset to train their models. The authors used the Stanford Sentiment Analysis Dataset to train their models because it", "answers": ["1,268 randomly selected tweets in the Stanford Sentiment Analysis Dataset BIBREF36, all 12,594 unique English hashtags and their associated tweets from the same Stanford dataset", "Stanford Sentiment Analysis Dataset BIBREF36"], "all_classes": null, "length": 3756}
{"pred": "The corpus contains a variety of accents, including Persian, English, and other languages. The accents are not explicitly labeled, but they can be inferred from the text and audio data. The corpus is designed to be representative of the speech patterns of a diverse population, so it includes a range of accents and dialects.雅黑\n\n    雅黑\n\n        The corpus contains a variety of accents, including Persian, English, and other languages. The accents are not explicitly labeled, but they can be inferred from the text and audio data. The corpus is designed to be representative of the speech patterns of a diverse population, so it includes a range of accents", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3794}
{"pred": "    A compact, scalable, and meaningful representation of a set of word vectors. It is generated by applying PCA to the set of word vectors. The word subspace retains most of the variability of the set.", "answers": ["Word vectors, usually in the context of others within the same class"], "all_classes": null, "length": 5151}
{"pred": "    The baseline model used is the one proposed by Dunietz and Gillick (BIBREF11). It uses a variety of features to measure the salience of an entity in text. The features include positional features, occurrence frequency, and the internal POS structure of the entity and the sentence it occurs in. (BIBREF11) (BIBREF11) (BIBREF11) (BIBREF11) (BIBREF11) (BIBREF11) (BIBREF11) (BIBREF11) (BIBREF11) (BIBREF11) (BIBREF11", "answers": ["For Article-Entity placement, they consider two baselines: the first one using only salience-based features, and the second baseline checks if the entity appears in the title of the article. \n\nFor Article-Section Placement, they consider two baselines: the first picks the section with the highest lexical similarity to the article, and the second one picks the most frequent section.", "B1. The first baseline uses only the salience-based features by Dunietz and Gillick BIBREF11 ., B2. The second baseline assigns the value relevant to a pair INLINEFORM0 , if and only if INLINEFORM1 appears in the title of INLINEFORM2 .\n\n, S1: Pick the section from template INLINEFORM0 with the highest lexical similarity to INLINEFORM1 : S1 INLINEFORM2, S2: Place the news into the most frequent section in INLINEFORM0"], "all_classes": null, "length": 7891}
{"pred": "    Yes, SemCor3.0 is reflective of English language data in general. It is a large corpus manually annotated with WordNet sense for WSD, and it is widely used in WSD research. The data in SemCor3.0 is collected from a variety of sources, including news articles, books, and scientific papers, and it covers a wide range of topics and domains. Therefore, it is considered to be representative of English language data in general. (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 2000}
{"pred": "    100 hours of speech. (Augmented LibriSpeech dataset is 100 hours of speech.) (Augmented LibriSpeech dataset is 100 hours of speech.) (Augmented LibriSpeech dataset is 100 hours of speech.) (Augmented LibriSpeech dataset is 100 hours of speech.) (Augmented LibriSpeech dataset is 100 hours of speech.) (Augmented LibriSpeech dataset is 100 hours of speech.) (Augmented LibriSpeech dataset is 100 hours of speech.) (Augmented LibriSpeech dataset is 100 hours of speech.) (Augmented LibriSpeech dataset is ", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 2410}
{"pred": "    The dataset for fine-grained classification is split in training, development, development_test and test parts. In the rest, we refer to these splits as train, development and test, where train is composed by the training and the development instances. Table TABREF7 presents an overview of the data. As discussed in BIBREF16 and illustrated in the Table, the fine-grained dataset is highly unbalanced and skewed towards the positive sentiment: onlyINLINEFORM0 of the training examples are labeled with one of the negative classes. (The dataset for fine-grained classification is split in training, development, development_test and test parts. In", "answers": [" high-quality datasets  from SemEval-2016 “Sentiment Analysis in Twitter” task", " SemEval-2016 “Sentiment Analysis in Twitter”"], "all_classes": null, "length": 2738}
{"pred": "        They use the pre-trained uncased BERT$_\\mathrm {BASE}$ model for fine-tuning, because they find that BERT$_\\mathrm {LARGE}$ model performs slightly worse than BERT$_\\mathrm {BASE}$ in this task. (The number of Transformer blocks is 12, the number of the hidden layer is 768, the number of self-attention heads is 12, and the total number of parameters of the pre-trained model is 110M.) (When fine-tuning, they use the development set (SE07) to find the optimal settings for their experiments. They keep the dropout probability", "answers": ["small BERT", "small BERT"], "all_classes": null, "length": 1999}
{"pred": "    Yes, the datasets are subject to quality control through crowd-sourcing experiments that show high agreement across probes and correlation with model trends. The datasets are also carefully manipulated to control the complexity of target questions. However, the expert knowledge used to construct the datasets is fallible and error-prone, so the quality of the datasets is limited by the quality of the underlying knowledge resources. The datasets are also vulnerable to systematic biases, which can be introduced during the construction process. Overall, the datasets are challenging and useful for probing the knowledge contained in language models, but their quality is limited by the quality of the underlying knowledge resources and the potential for", "answers": ["No", "No"], "all_classes": null, "length": 6391}
{"pred": "    Yes, the images are from a specific domain. The images are generated using the ShapeWorld framework, which consists of abstract colored shapes. The images are used to evaluate image captioning models. The images are not from a real-world domain. The images are used to evaluate image captioning models in a controlled environment. The images are not from a real-world domain. The images are used to evaluate image captioning models in a controlled environment. The images are not from a real-world domain. The images are used to evaluate image captioning models in a controlled environment. The images are not from a real-world domain. The images are used", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3472}
{"pred": "    The model achieved competitive results on standard benchmarks, without relying on any handcrafted resources. The average f-score was 0.368, with a micro-average of 0.368. The model performed best on the development set, with an f-score of 0.368. The model was trained on a subset of Facebook pages, chosen according to their performance on the development set and the distribution of emotions in the datasets. The feature set used was all features plus Google-based embeddings, but excluding the lexicon. The model was evaluated on three standard datasets for emotion classification: Affective Text, Fairy Tales, and ISEAR.", "answers": ["Answer with content missing: (Table 3) Best author's model B-M average micro f-score is 0.409, 0.459, 0.411 on Affective, Fairy Tales and ISEAR datasets respectively. "], "all_classes": null, "length": 3410}
{"pred": "        The tagging scheme employed is {INLINEFORM0, INLINEFORM1, INLINEFORM2, INLINEFORM3, INLINEFORM4, INLINEFORM5, INLINEFORM6, INLINEFORM7, INLINEFORM8}. {INLINEFORM0} tag means the current word is not a pun. {INLINEFORM1} tag means the current word is a pun. {INLINEFORM2} tag highlights the current word is a pun. {INLINEFORM3} tag indicates that the current word appears before the pun in the given context. {INLINEFORM4} tag indicates that the current word appears after the pun. {INLINEFORM5} tag", "answers": ["A new tagging scheme that tags the words before and after the pun as well as the pun words.", "a new tagging scheme consisting of three tags, namely { INLINEFORM0 }"], "all_classes": null, "length": 2974}
{"pred": "    Yes, Arabic is one of the 11 languages in CoVost. The other languages are French, German, Dutch, Russian, Spanish, Italian, Turkish, Persian, Swedish, Mongolian, and Chinese. The corpus is diversified with over 11,000 speakers and over 60 accents. The corpus includes a total of 708 hours of speech in 11 languages, with French and German having the largest durations among existing public corpora. The corpus also includes an additional evaluation corpus from Tatoeba for French, German, Dutch, Russian, and Spanish, resulting in a total of 9.3 hours of speech. Both", "answers": ["No", "No"], "all_classes": null, "length": 2413}
{"pred": "        A model is considered robust if it is able to handle and perform well in situations where the prior knowledge is biased or incomplete. The model should be able to learn from the data and adapt to new situations without being overly influenced by the prior knowledge. The model should also be able to handle and perform well in situations where the prior knowledge is not applicable or relevant. The model should be able to learn from the data and adapt to new situations without being overly influenced by the prior knowledge. The model should also be able to handle and perform well in situations where the prior knowledge is not applicable or relevant. (unanswerable) (yes)", "answers": ["ability to accurately classify texts even when the amount of prior knowledge for different classes is unbalanced, and when the class distribution of the dataset is unbalanced", "Low sensitivity to bias in prior knowledge"], "all_classes": null, "length": 3609}
{"pred": "    InferSent and Universal Sentence Encoder.", "answers": ["GloVe, BERT, Universal Sentence Encoder, TF-IDF, InferSent", "Avg. GloVe embeddings, Avg. fast-text embeddings, Avg. BERT embeddings, BERT CLS-vector, InferSent - GloVe and Universal Sentence Encoder."], "all_classes": null, "length": 3862}
{"pred": "        The proposed method outperforms BERT-MRC by +0.29 and +0.96 respectively for English datasets including CoNLL2003 and OntoNotes5.0. For Chinese datasets, the proposed method achieves F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively. The proposed method sets new SOTA performances on all of the four NER datasets.       \n\n        The proposed method outperforms BERT-MRC by +0.29 and +0.96 respectively for English datasets including CoNLL2003 and OntoNotes5", "answers": ["English datasets including CoNLL2003 and OntoNotes5.0, our proposed method outperforms BERT-MRCBIBREF38 by +0.29 and +0.96 respectively, Chinese datasets, achieving F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively", "For English datasets including CoNLL2003 and OntoNotes5.0, our proposed method outperforms BERT-MRCBIBREF38 by +0.29 and +0.96 respectively., huge performance boosts on Chinese datasets, achieving F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively"], "all_classes": null, "length": 3586}
{"pred": "        They test their conflict method on two tasks: Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask. They use the same model architecture for both tasks, but the input data and the output layer are different. For Quora Duplicate Question Pair Detection, the input data consists of pairs of questions and the output layer predicts whether the pair is duplicate or not. For Ranking questions in Bing's People Also Ask, the input data consists of user queries and candidate questions, and the output layer predicts whether a candidate question is a high-click question or not. The conflict method is used in the interaction part of the model, which", "answers": ["Task 1: Quora Duplicate Question Pair Detection, Task 2: Ranking questions", "Quora Duplicate Question Pair Detection, Ranking questions in Bing's People Also Ask"], "all_classes": null, "length": 2577}
{"pred": "    Tree-based CNN, Gumbel Tree-LSTM, Latent Syntax Tree-LSTM, NSE, Reinforced Self-Attention Network, Residual stacked encoders, BiLSTM with generalized pooling.", "answers": ["Various tree structured neural networks including variants of Tree-LSTM, Tree-based CNN, RNTN, and non-tree models including variants of LSTMs, CNNs, residual, and self-attention based networks", "Sentence classification baselines: RNTN (Socher et al. 2013), AdaMC-RNTN (Dong et al. 2014), TE-RNTN (Qian et al. 2015), TBCNN (Mou et al. 2015), Tree-LSTM (Tai, Socher, and Manning 2015), AdaHT-LSTM-CM (Liu, Qiu, and Huang 2017), DC-TreeLSTM (Liu, Qiu, and Huang 2017), TE-LSTM (Huang, Qian, and Zhu 2017), BiConTree (Teng and Zhang 2017), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), TreeNet (Cheng et al. 2018), CNN (Kim 2014), AdaSent (Zhao, Lu, and Poupart 2015), LSTM-CNN (Zhou et al. 2016), byte-mLSTM (Radford, Jozefowicz, and Sutskever 2017), BCN + Char + CoVe (McCann et al. 2017), BCN + Char + ELMo (Peters et al. 2018). \nStanford Natural Language Inference baselines: Latent Syntax Tree-LSTM (Yogatama et al. 2017), Tree-based CNN (Mou et al. 2016), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), NSE (Munkhdalai and Yu 2017), Reinforced Self- Attention Network (Shen et al. 2018), Residual stacked encoders: (Nie and Bansal 2017), BiLSTM with generalized pooling (Chen, Ling, and Zhu 2018)."], "all_classes": null, "length": 4781}
{"pred": "    The core component for KBQA is relation detection. The relation detection model is used to identify the relations between entities in the KB and the question. The identified relations are then used to generate the KB query that is used to retrieve the answers from the KB. The relation detection model is a key component in KBQA systems and plays a crucial role in determining the accuracy and performance of the system. The relation detection model is also a key component in KBQA systems and plays a crucial role in determining the accuracy and performance of the system. The relation detection model is also a key component in KBQA systems and plays a crucial role in determining the", "answers": ["answer questions by obtaining information from KB tuples ", "hierarchical matching between questions and relations with residual learning"], "all_classes": null, "length": 4527}
{"pred": "    The baseline models are a name-based Nearest-Neighbor model (NN) and an Encoder-Decoder model with ingredient attention (Enc-Dec). The Encoder-Decoder model is used as a strong non-personalized baseline. The Nearest-Neighbor model is used as a baseline for personalization.", "answers": ["name-based Nearest-Neighbor model (NN), Encoder-Decoder baseline with ingredient attention (Enc-Dec)"], "all_classes": null, "length": 2655}
{"pred": "        The methods considered to find examples of biases and unwarranted inferences are manual detection, part-of-speech tagging, and leveraging the structure of Flickr30K Entities. The manual detection method involves looking at a collection of images and identifying patterns. Part-of-speech tagging involves tagging all descriptions with part-of-speech information to see which adjectives are most commonly used for particular nouns. Leveraging the structure of Flickr30K Entities involves using coreference annotations to create a coreference graph and applying Louvain clustering to identify clusters of expressions that refer to similar entities. These methods help to get a sense of the richness of the data", "answers": ["spot patterns by just looking at a collection of images, tag all descriptions with part-of-speech information, I applied Louvain clustering", "Looking for adjectives marking the noun \"baby\" and also looking for most-common adjectives related to certain nouns using POS-tagging"], "all_classes": null, "length": 2204}
{"pred": "        English and French.", "answers": ["English, French, German ", "French, English, Spanish, Italian, Portuguese, Hebrew, Arabic"], "all_classes": null, "length": 2285}
{"pred": "    They experimented with Cell-aware Stacked LSTMs (CAS-LSTMs), Bi-CAS-LSTMs, and stacked LSTMs. They also experimented with different word embeddings and pooling methods.", "answers": ["Stacked LSTMs, Cell-aware Stacked LSTMs, Sentence Encoders, Top-layer Classifiers"], "all_classes": null, "length": 3210}
{"pred": "        Yes, they report results only on English data.", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 6169}
{"pred": "    The authors experimented with a few summarization algorithms provided by the Sumy package. These algorithms include TextRank, LexRank, and SumBasic. The authors also implemented their own ILP-based summarization algorithm.", "answers": ["LSA, TextRank, LexRank and ILP-based summary.", "LSA, TextRank, LexRank"], "all_classes": null, "length": 3045}
{"pred": "        The previous state of the art for this task was a probabilistic graphical model that inferred vocabulary dependent latent post categories to model the thread sequence and infer states that triggered intervention. However, this model required a hyperparameter for the number of latent states, which was likely to not generalize due to weak evaluation. (BIBREF0) (BIBREF7) (BIBREF1) (BIBREF8) (BIBREF7) (BIBREF0) (BIBREF7) (BIBREF1) (BIBREF8) (BIBREF7) (BIBREF0) (", "answers": ["hLSTM", "hLSTM"], "all_classes": null, "length": 3725}
{"pred": "        The master node skip connection. The reason behind this choice is that we expect the special document node to learn a high-level summary about the document, such as its size, vocabulary, etc. (more details are given in subsection SECREF30). Therefore, by making the master node bypass the attention layer, we directly inject global information about the document into its final representation. (more details are given in subsection SECREF16). (more details are given in subsection SECREF30). (more details are given in subsection SECREF16). (more details are given in subsection SECREF30). (more details are given in subsection SECREF", "answers": ["Based on table results provided changing directed to undirected edges had least impact - max abs difference of 0.33 points on all three datasets."], "all_classes": null, "length": 4180}
{"pred": "        The corpus used for the task is DTA18 and DTA19, which are subparts of the DTA corpus. These corpora consist of texts from the 16th to the 20th century, and the corpus version used in the task has the following format: \"year [tab] lemma1 lemma2 lemma3...\". The two corpora correspond to the diachronic corpus pair from BIBREF0: DTA18 and DTA19. They consist of subparts of DTA corpus BIBREF11 which is a freely available lemmatized, POS-tagged and spelling-normalized diach", "answers": ["DTA18, DTA19", "Diachronic Usage Relatedness (DURel) gold standard data set"], "all_classes": null, "length": 1908}
{"pred": "        Kannada, Hindi, Telugu, Malayalam, Bengali, and English\n\n        Kannada, Hindi, Telugu, Malayalam, Bengali, and English\n\n        Kannada, Hindi, Telugu, Malayalam, Bengali, and English\n\n        Kannada, Hindi, Telugu, Malayalam, Bengali, and English\n\n        Kannada, Hindi, Telugu, Malayalam, Bengali, and English\n\n        Kannada, Hindi, Telugu, Malayalam, Bengali, and English\n\n        Kannada, Hindi, Telugu, Malayalam, Bengali, and English\n\n        Kann", "answers": ["Hindi, English, Kannada, Telugu, Assamese, Bengali and Malayalam", "Kannada, Hindi, Telugu, Malayalam, Bengali,  English and Assamese (in table, missing in text)"], "all_classes": null, "length": 2453}
{"pred": "The model performance on target language reading comprehension is reasonable, with EM/F1 scores of 53.8/44.1 on Chinese and 44.1/44.1 on English. However, the performance is degraded when the training data is translated into the target language, with F1 scores of 44.1/44.1 on English and 53.8/44.1 on Chinese. The performance is also degraded when the training data is translated into the same language as the testing data, with F1 scores of 44.1/44.1 on English and 53.8/44.1 on", "answers": ["Table TABREF6, Table TABREF8", "when testing on English, the F1 score of the model training on Chinese (Zh) is 53.8,  F1 score is only 44.1 for the model training on Zh-En"], "all_classes": null, "length": 2492}
{"pred": "        The proposed model outperforms the baselines by a significant margin. The difference in performance is consistent across all five evaluation characters. The proposed model is able to recover the language styles of specific characters more accurately than the baselines. The proposed model is also able to recommend tailored responses traceable to specific characters, which is not possible with the baselines. The proposed model is able to effectively use HLAs to improve upon dialogue retrieval performance. The proposed model is able to recover the language styles of various characters regardless of the character's profile and identity, genre of the show, and context of the dialogue. The proposed model is able to", "answers": ["Metric difference between Aloha and best baseline score:\nHits@1/20: +0.061 (0.3642 vs 0.3032)\nMRR: +0.0572(0.5114 vs 0.4542)\nF1: -0.0484 (0.3901 vs 0.4385)\nBLEU: +0.0474 (0.2867 vs 0.2393)"], "all_classes": null, "length": 5151}
{"pred": "        1.5% improvement in forward perplexity and 2.5% improvement in reverse perplexity on COCO dataset. 2.5% improvement in Self-BLEU on COCO dataset. 3.5% improvement in grammaticality and 4.5% improvement in relevance on WeiboDial dataset. 4.5% improvement in grammaticality and 3.5% improvement in relevance on WeiboDial dataset. 5.5% improvement in grammaticality and 4.5% improvement in relevance on WeiboDial dataset. 6.5% improvement in", "answers": ["ARAM has achieved improvement over all baseline methods using reverese perplexity and  slef-BLEU metric.  The maximum reverse perplexity improvement 936,16 is gained for EMNLP2017  WMT  dataset and  48,44 for COCO dataset.", "Compared to the baselines, ARAML does not do better in terms of perplexity on COCO and EMNLP 2017 WMT datasets, but it does by up to 0.27 Self-BLEU points on COCO and 0.35 Self-BLEU on EMNLP 2017 WMT. In terms of Grammaticality and Relevance, it scores better than the baselines on up to 75.5% and 73% of the cases respectively."], "all_classes": null, "length": 3796}
{"pred": "        The authors present evidence that the model can capture some biases in data annotation and collection by examining the results and finding that many errors are due to biases from data collection and rules of annotation. They also mention that the model can detect some biases in the process of collecting or annotating datasets. (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable", "answers": ["The authors showed few tweets where neither and implicit hatred content exist but the model was able to discriminate"], "all_classes": null, "length": 4119}
{"pred": "        Yes, three baselines were tested: SVM, CNN, and BERT. The SVM baseline used simple bag-of-words features, while the CNN baseline used a convolutional neural network to encode questions. The BERT baseline used a bidirectional transformer-based language model to fine-tune on the binary answerability identification task. The results showed that BERT performed the best on the answerability identification task, but most baselines considerably exceeded the performance of a majority-class baseline. This suggests that there is considerable information in the question that indicates its possible answerability within this domain. The best-performing baseline, Bert + Unanswerable,", "answers": ["SVM, No-Answer Baseline (NA) , Word Count Baseline, Human Performance", "No-Answer Baseline (NA), Word Count Baseline, Human Performance"], "all_classes": null, "length": 3855}
{"pred": "        72782 unique words\n\n        6946 sentences\n\n        16225 unique words\n\n        14 million words from books, web-texts and news papers\n\n        14 million words from books, web-texts and news papers\n\n        14 million words from books, web-texts and news papers\n\n        14 million words from books, web-texts and news papers\n\n        14 million words from books, web-texts and news papers\n\n        14 million words from books, web-texts and news papers\n\n        14 million words from books, web-texts and news papers\n\n        14", "answers": ["Dataset contains 3606 total sentences and 79087 total entities.", "ILPRL contains 548 sentences, OurNepali contains 3606 sentences"], "all_classes": null, "length": 2843}
{"pred": "        Using dice loss, the F1 score for paraphrase identification is improved by 0.58 for MRPC and 0.73 for QQP. {\r\r\n       ？」\n\n        Using dice loss, the F1 score for paraphrase identification is improved by 0.58 for MRPC and 0.73 for QQP. {\r\r\n       ？」\n\n        Yes, the proposed method improves F1 for paraphrase identification.？」\n\n       ？」\n\n        Yes, the proposed method improves F1 for paraphrase identification.？」\n\n       ？」\n\n        Unanswerable？」\n\n       ？」\n\n        Unanswerable？」\n\n       ？」\n\n        Unanswerable？」\n\n       ？」\n\n        Unanswer", "answers": ["Using DSC loss improves the F1 score by +0.58 for MRPC and +0.73 for QQP", "+0.58"], "all_classes": null, "length": 3566}
{"pred": "        The datasets used are EEG data from BIBREF0 and eye-tracking, self-paced reading time, and ERP data. (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable", "answers": ["Answer with content missing: (Whole Method and Results sections) The primary dataset we use is the ERP data collected and computed by Frank et al. (2015), and we also use behavioral data (eye-tracking data and self-paced reading times) from Frank et al. (2013) which were collected on the same set of 205 sentences.\nSelect:\n- ERP data collected and computed by Frank et al. (2015)\n- behavioral data (eye-tracking data and self-paced reading times) from Frank et al. (2013)", "the ERP data: BIBREF0"], "all_classes": null, "length": 1971}
{"pred": "        The subjects were presented with a series of images depicting different events, such as a person walking, a car driving, and a person eating. The images were designed to elicit event-related responses in the form of brain activity. The subjects were instructed to imagine themselves performing the actions depicted in the images. The researchers recorded the brain activity of the subjects while they imagined performing the actions. The brain activity was recorded using electroencephalography (EEG) and electrooculography (EOG) techniques. The EEG data was used to measure the event-related responses, while the EOG data was used to measure the eye movements of", "answers": ["7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw)", "KARA ONE BIBREF17 , composed of multimodal data for stimulus-based, imagined and articulated speech state corresponding to 7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw)"], "all_classes": null, "length": 2379}
{"pred": "    Pointer-Gen, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN, Pointer-Gen+ARL-SEN.", "answers": ["Pointer-Gen, Pointer-Gen+Pos, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN"], "all_classes": null, "length": 4085}
{"pred": "    Traditional machine learning models and neural network based models. The traditional machine learning models include Naïve Bayes, Logistic Regression, Support Vector Machine, Random Forests, and Gradient Boosted Trees. The neural network based models include Convolutional Neural Networks, Recurrent Neural Networks, and their variants. The pre-trained GloVe representation is used for word-level features. The HybridCNN model is also investigated. The attention mechanism is applied to the RNN baseline models. The Latent Topic Clustering method is also used to extract latent topic information from the hidden states of RNN. The context tweets are used as additional features for the neural", "answers": ["Naïve Bayes (NB), Logistic Regression (LR), Support Vector Machine (SVM), Random Forests (RF), Gradient Boosted Trees (GBT),  Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN)", "Naïve Bayes (NB), Logistic Regression (LR), Support Vector Machine (SVM), Random Forests (RF), Gradient Boosted Trees (GBT), CNN, RNN"], "all_classes": null, "length": 2074}
{"pred": "    The bi-directional language model is used to augment the sequence to sequence encoder and the uni-directional model is used to augment the decoder. Both use self-attention and the uni-directional model contains 6 transformer blocks, followed by a word classifier to predict the next word on the right. The bi-directional model solves a cloze-style token prediction task at training time. The model consists of two towers, the forward tower operates left-to-right and the tower operating right-to-left as backward tower; each tower contains 6 transformer blocks. The forward and backward representations are combined via a self-attention module and the output of this", "answers": ["uni-directional model to augment the decoder", "bi-directional language model to augment the sequence to sequence encoder ,  uni-directional model to augment the decoder"], "all_classes": null, "length": 1914}
{"pred": "        The weights are dynamically adjusted by multiplying the soft probability p with a decaying factor (1-p). The intuition is to push down the weight of easy examples. For easy examples whose probability are approaching 0 or 1, (1-p)p makes the model attach significantly less focus to them. The proposed method is inspired by the idea of focal loss in computer vision. (1-p)p is thought as a weight associated with each example, which changes as training proceeds. (1-p)p makes the model attend less to examples once they are correctly classified. (1-p)p is also known as self-adapting dice loss. (", "answers": ["One can think $(1-p_{i1})$ as a weight associated with each example, which changes as training proceeds. The intuition of changing $p_{i1}$ to $(1-p_{i1}) p_{i1}$ is to push down the weight of easy examples. For easy examples whose probability are approaching 0 or 1, $(1-p_{i1}) p_{i1}$ makes the model attach significantly less focus to them. Figure FIGREF23 gives gives an explanation from the perspective in derivative: the derivative of $\\frac{(1-p)p}{1+(1-p)p}$ with respect to $p$ approaches 0 immediately after $p$ approaches 0, which means the model attends less to examples once they are correctly classified.", "associates each training example with a weight in proportion to $(1-p)$, and this weight dynamically changes as training proceeds"], "all_classes": null, "length": 3640}
{"pred": "        The results show that agents utilizing knowledge-graphs in addition to either enhanced exploration method far outperform the baseline A2C and KG-A2C. KG-A2C-chained and KG-A2C-Explore both pass the bottleneck of a score of 40, whereas A2C-Explore gets to the bottleneck but cannot surpass it. There are a couple of key insights that can be drawn from these results. The first is that the knowledge graph appears to be critical; it is theorized to help with partial observability. However the knowledge graph representation isn't sufficient in that the knowledge graph representation without enhanced exploration methods cannot surpass", "answers": ["Reward of 11.8 for the A2C-chained model, 41.8 for the KG-A2C-chained model, 40 for A2C-Explore and 44 for KG-A2C-Explore.", "KG-A2C-chained and KG-A2C-Explore both pass the bottleneck of a score of 40"], "all_classes": null, "length": 2443}
{"pred": "    An individual model consists of a Bayesian model for each language, and crosslingual latent variables to incorporate soft role agreement between aligned constituents. These latent variables capture correlations between roles in different languages, and regularize the parameter estimates of the monolingual models. The individual models are coupled together through the crosslingual latent variables. The generative process is as follows: all the multinomial and binomial distributions have symmetric Dirichlet and beta priors respectively. The probability equations for the monolingual model are given in Figure 7. This formulation models the global role ordering and repetition preferences using PRs, and limited context for SR", "answers": ["Bayesian model of garg2012unsupervised as our base monolingual model"], "all_classes": null, "length": 3701}
{"pred": "        Non-standard pronunciation is identified by annotating the Spanish words interspersed in Mapudungun speech. The annotations label foreign words as such. (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no", "answers": ["Unanswerable", "Original transcription was labeled with additional labels in [] brackets with nonstandard pronunciation."], "all_classes": null, "length": 3018}
{"pred": "    A semicharacter architecture is a type of neural network that processes a sentence of words with misspelled characters, predicting the correct words at each step. The architecture treats the first and the last characters individually, and is agnostic to the ordering of the internal characters. Each word is represented by concatenating a one-hot vector of the first character, a one-hot representation of the last character, and a bag of characters representation of the internal characters. The architecture uses a BiLSTM cell to process the input words, and the training target is the correct corresponding word. The model is optimized with cross-entropy loss. The sem", "answers": ["A semi-character based RNN (ScRNN) treats the first and last characters individually, and is agnostic to the ordering of the internal characters", "processes a sentence of words with misspelled characters, predicting the correct words at each step"], "all_classes": null, "length": 4186}
{"pred": "        Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish.       ยนตร์\n\n        Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish.       ยนตร์\n\n        Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish.       ยนตร์\n\n        Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian,", "answers": ["Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish", "Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish , Swedish"], "all_classes": null, "length": 2697}
{"pred": "        The NCEL approach is effective overall, achieving the best performance in most cases with an average gain of 2% on Micro F1 and 3% Macro F1. The results demonstrate the good generalization ability of the model across different datasets. The proposed method performs consistently well on all datasets, outperforming the state-of-the-art collective methods. The effectiveness of the main modules is also investigated, and the attention and embedding features show non-negligible impacts in the \"hard\" case of WW, which contains much noise. The qualitative analysis shows that the model can effectively utilize local and global information to improve disambig", "answers": ["NCEL consistently outperforms various baselines with a favorable generalization ability"], "all_classes": null, "length": 4113}
{"pred": "        Yes, the data is de-identified. The audio recordings have an average duration of 9min 28s and have a verbatim transcript of 1,500 words on average. The sentences in the transcript are grounded to the audio with the timestamps of its first and last word. The transcript of the conversations are annotated with summaries and Medication Regimen tags (MR tags), both grounded using the timestamps of the sentences from the transcript deemed relevant by the expert annotators. The summaries are medically relevant and local, and the MR tags are also local and of the form {Medication Name, Dosage, Frequency}. The data", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4592}
{"pred": "        The baseline used was the performance of the error detection system by Rei2016, trained using the same FCE dataset. (The FCE dataset is a publicly available dataset of language learner essays, used for training error detection models.) (The Rei2016 system is a neural sequence labeling model for error detection, which receives a sequence of tokens as input and outputs a prediction for each position, indicating whether the token is correct or incorrect in the current context.) (The baseline performance is used to compare the performance of the error detection system trained using artificial data, generated by the different error generation methods.) (The baseline performance is 78.", "answers": ["error detection system by Rei2016", "error detection system by Rei2016"], "all_classes": null, "length": 2132}
{"pred": "        The annotated clinical notes were obtained from the 2010 i2b2/VA BIBREF0 and 2013 ShARe/CLEF Task 1 BIBREF1 challenges. The 2010 i2b2/VA challenge defines three types of entities: \"problem\", \"treatment\", and \"test\". The 2013 ShARe/CLEF challenge defines various types of disorder such as \"injury or poisoning\", \"disease or syndrome\", etc. The CE task in 2013 ShARe/CLEF has an encoding component which requires selecting one and only one Concept Unique Identifier", "answers": ["clinical notes from the CE task in 2010 i2b2/VA", "clinical notes from the CE task in 2010 i2b2/VA "], "all_classes": null, "length": 3432}
{"pred": "        The masking process helps the decoder focus on the context of the summary draft, rather than the specific words in the summary draft. This allows the decoder to generate more context-aware and natural summaries. The masking process also helps the decoder learn to generate more diverse and informative summaries, as it forces the decoder to generate words that are not present in the summary draft. This helps the decoder learn to generate more diverse and informative summaries, as it forces the decoder to generate words that are not present in the summary draft. The masking process also helps the decoder learn to generate more fluent and coherent summaries, as it forces the decoder to generate words that are", "answers": ["ecause this process is similar to the cloze task in BERT's pre-train process, therefore by using the ability of the contextual language model the decoder can generate more fluent and natural sequences."], "all_classes": null, "length": 3919}
{"pred": "        The dataset used is the PPDB dataset. (Unanswerable) (Unanswerable) (Unanswerable) (Unanswerable) (Unanswerable) (Unanswerable) (Unanswerable) (Unanswerable) (Unanswerable) (Unanswerable) (Unanswerable) (Unanswerable) (Unanswerable) (Unanswerable) (Unanswerable) (Unanswerable) (Unanswerable) (Unanswerable) (Unanswerable) (Unanswerable) (Unanswerable) (Unanswerable) (Unanswerable) (Unanswer", "answers": ["Unanswerable", " Paraphrase Database (PPDB) ,  book corpus", "Unanswerable"], "all_classes": null, "length": 1902}
{"pred": "    The features used are Term Frequency-Inverse Document Frequency (TF-IDF) weights. These weights are used to transform a pathology report into a feature vector. The feature vector is highly sparse, meaning that most of the values are zero. The TF-IDF weights are calculated for each word in the report, and the corresponding index in the feature vector is set to the TF-IDF weight. The resultant feature vector represents the importance of each word in the report. The top 50 words with the highest TF-IDF weights are considered the most important keywords in the report. These keywords are further grouped into different topics using latent Dirichlet", "answers": ["Unanswerable"], "all_classes": null, "length": 2108}
{"pred": "        The dataset is annotated based on a hierarchical model of depression-related symptoms. Each tweet is annotated as no evidence of depression or evidence of depression. If there is evidence of depression, the tweet is further annotated with one or more depressive symptoms, such as depressed mood, disturbed sleep, or fatigue or loss of energy. The annotations are binarized as positive or negative classes.\r\r\n\r\r\n        The dataset is encoded with 7 feature groups, including lexical features, syntactic features, emotion features, demographic features, sentiment features, personality traits, and LIWC features. The feature values are binarized as present or absent.\r\r\n\r\r\n        The dataset", "answers": ["no evidence of depression, depressed mood, disturbed sleep, fatigue or loss of energy", "The annotations are based on evidence of depression and further annotated by the depressive symptom if there is evidence of depression"], "all_classes": null, "length": 1947}
{"pred": "    The eight NER tasks they evaluated on are: BC5CDR, CHEBI, CHEMDNER, CLINICAL, DDI, GENIA, JNLPBA, and MIMIC. These tasks cover a range of biomedical domains, including diseases, chemicals, clinical notes, drug-drug interactions, and more. The goal of the evaluation was to compare the performance of GreenBioBERT, an inexpensive and environmentally friendly alternative to BioBERT, with that of general-domain BERT and BioBERTv1.0. The results show that GreenBioBERT improves over general-domain BERT on all tasks, with varying effect", "answers": ["BC5CDR-disease, NCBI-disease, BC5CDR-chem, BC4CHEMD, BC2GM, JNLPBA, LINNAEUS, Species-800", "BC5CDR-disease, NCBI-disease, BC5CDR-chem, BC4CHEMD, BC2GM, JNLPBA, LINNAEUS, Species-800"], "all_classes": null, "length": 2800}
{"pred": "        The training data was translated using the machine translation platform Apertium.<translation platform Apertium.<translation platform Apertium.<translation platform Apertium.<translation platform Apertium.<translation platform Apertium.<translation platform Apertium.<translation platform Apertium.<translation platform Apertium.<translation platform Apertium.<translation platform Apertium.<translation platform Apertium.<translation platform Apertium.<translation platform Apertium.<translation platform Apertium.<translation platform Apertium.<translation platform Apertium.<translation platform Apertium.<translation platform Apertium.<translation platform Apertium", "answers": ["using the machine translation platform Apertium ", "machine translation platform Apertium BIBREF5"], "all_classes": null, "length": 2423}
{"pred": "    They used a multinomial Naive Bayes classifier.", "answers": ["AllWords model by counting the frequencies of all the remaining words and training a multinomial Naive Bayes classifier"], "all_classes": null, "length": 4177}
{"pred": "        The baseline for this task was a very simple logistic regression classifier with default parameters, where the input instances were represented with a single feature: the length of the sentence. The performance of this baseline on the SLC task is shown in Tables TABREF33 and TABREF34.", "answers": ["The baseline system for the SLC task is a very simple logistic regression classifier with default parameters. The baseline for the FLC task generates spans and selects one of the 18 techniques randomly.", "SLC task is a very simple logistic regression classifier, FLC task generates spans and selects one of the 18 techniques randomly"], "all_classes": null, "length": 3001}
{"pred": "        The baselines they compare with are the conditional random fields (CRF) model and the pipeline method. The CRF model uses features like POS tags, n-grams, label transitions, word suffixes, and relative position to the end of the text to make predictions. The pipeline method uses a classifier for pun detection and a separate system for pun location. The authors also compare their results with previous works that did not employ joint learning.", "answers": ["They compare with the following models: by Pedersen (2017), by Pramanick and Das (2017), by Mikhalkova and Karyakin (2017),  by Vadehra (2017), Indurthi and Oota (2017), by Vechtomova (2017), by (Cai et al., 2018), and CRF."], "all_classes": null, "length": 2991}
{"pred": "        The political bias of different sources is included in the model by assigning a political bias label to different US outlets following the procedure described in BIBREF2. The political bias label is then used to train the model on left-biased or right-biased outlets of both disinformation and mainstream domains and test on the entire set of sources. This allows the model to account for the different political biases inherent to different news sources. The model is able to accurately distinguish mainstream news from disinformation regardless of the political bias. Additionally, classification experiments are performed excluding particular sources that outweigh the others in terms of samples to avoid over-fitting.", "answers": ["By assigning a political bias label to each news article and training only on left-biased or right-biased outlets of both disinformation and mainstream domains", "we also account for political biases inherent to different news sources, referring to the procedure proposed in BIBREF2 to label different outlets. Overall we show that we are able to classify credible vs non-credible diffusion networks (and consequently news articles) with high accuracy (AUROC up to 94%), even when accounting for the political bias of sources (and training only on left-biased or right-biased articles). We observe that the layer of mentions alone conveys useful information for the classification, denoting a different usage of this functionality when sharing news belonging to the two news domains. We also show that most discriminative features, which are relative to the breadth and depth of largest cascades in different layers, are the same across the two countries."], "all_classes": null, "length": 4882}
{"pred": "        The ancient Chinese dataset comes from ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era. They used plain and accurate words to express what happened at that time, and thus ensure the generality of the translated materials. {\r\r\n        The ancient Chinese dataset comes from ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era. They used plain and accurate words to express what happened at that time, and thus ensure the generality of the translated materials. {\r\r\n        The ancient Chinese dataset comes", "answers": ["ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era", "Ancient Chinese history records in several dynasties and articles written by celebrities during 1000BC-200BC collected from the internet "], "all_classes": null, "length": 3722}
{"pred": "        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English", "answers": ["English", "English ", "English"], "all_classes": null, "length": 2240}
{"pred": "        The Chinese datasets used were the Penn Treebank (PTB) and the Penn Chinese Treebank (PCTB). The PTB is a collection of English sentences, while the PCTB is a collection of Chinese sentences. Both datasets are widely used in natural language processing research. The PTB is used for English grammar induction, while the PCTB is used for Chinese grammar induction. The PTB is also used for training neural language models, while the PCTB is used for training neural language models for Chinese. The PTB and PCTB are both available for download from the Penn Treebank website. The PT", "answers": ["Answer with content missing: (Data section) Chinese with version 5.1 of the Chinese Penn Treebank (CTB)"], "all_classes": null, "length": 2545}
{"pred": "        3 layers (user, topic, and comment) ", "answers": ["eight layers"], "all_classes": null, "length": 4487}
{"pred": "    The dataset used in this paper is the European network of nature protected sites Natura 2000 dataset, which contains information about the distribution of 100 species across Europe. Additionally, the European network of nature protected sites Natura 2000 dataset is used to predict soil type and CORINE land cover classes at levels 1, 2 and level 3. Finally, the crowdsourced dataset from the ScenicOrNot website is used to predict people's subjective opinions of landscape beauty in Britain. The dataset is used to evaluate the effectiveness of embeddings for predicting environmental features. davidjl\n\n        davidjl\n\n        The dataset used in this paper", "answers": [" the same datasets as BIBREF7", "same datasets as BIBREF7"], "all_classes": null, "length": 4661}
{"pred": "    The paper uses two clinical datasets: NUBes-PHI and MEDDOCAN. NUBes-PHI is a corpus of real medical reports written in Spanish and annotated with sensitive information. MEDDOCAN is a synthetic corpus of clinical cases enriched with sensitive information by health documentalists. The paper also uses a baseline system based on regular expressions and dictionaries to detect sensitive information. The paper compares the performance of the baseline system, a CRF classifier, a spaCy entity tagger, and a BERT-based system. The BERT-based system outperforms the other systems in terms of recall and F1-score. The paper", "answers": ["MEDDOCAN, NUBes-PHI", "MEDDOCAN, NUBes "], "all_classes": null, "length": 4528}
{"pred": "    Unigrams and Pragmatic features, Stylistic patterns, and patterns related to situational disparity. (BIBREF0, BIBREF1, BIBREF2, BIBREF3) (BIBREF4, BIBREF5) (BIBREF6, BIBREF7) (BIBREF8) (BIBREF9, BIBREF10) (BIBREF11) (BIBREF12) (BIBREF13) (BIBREF14) (BIBREF15) (BIBREF16) (BIBREF17) (BIBREF", "answers": ["Unanswerable"], "all_classes": null, "length": 3543}
{"pred": "    The metrics used to establish that this makes chatbots more knowledgeable and better at learning and conversation are: Coverage, Average MCC, and Average +ve F1 score. These metrics are used to evaluate the strategy formulation ability and predictive performance of the chatbot. The strategy formulation ability is measured by the fraction of total query data instances for which the chatbot has successfully formulated strategies that lead to winning. The predictive performance is measured by the average MCC and average +ve F1 score. The higher the values of these metrics, the better the chatbot's performance. The lifelong learning mechanism is also used to transfer knowledge from past tasks to help", "answers": ["Coverage, Avg. MCC and avg. +ve F1 score", "strategy formulation ability, we introduce a measure called Coverage( INLINEFORM0 ), To evaluate the predictive performance, we use Avg. MCC and avg. +ve F1 score"], "all_classes": null, "length": 5869}
{"pred": "        Yes, they employ their indexing-based method to create a sample of a QA Wikipedia dataset. They first index each paragraph in Wikipedia using {1,2,3}-grams, then query each answer sentence from the corpora to Lucene, and retrieve the top-5 ranked paragraphs. The cosine similarity between each sentence in these paragraphs and the answer sentence is measured for 1-grams, and a weight is assigned to each 2-gram score. The weighted sum is measured, and the paragraph consisting of the sentence with the highest score is considered the silver-standard answer passage. They then create a dataset by querying each question to Luc", "answers": ["Yes", "No"], "all_classes": null, "length": 1910}
{"pred": "    Galatasaray and Fenerbahçe are the targets. davidjl\n\nQuestion: What is the stance of the tweets towards the targets?\n\nAnswer:雅黑\n\n    The tweets are annotated as either favoring or opposing the targets.雅黑\n\nQuestion: What is the purpose of the study?\n\nAnswer:雅黑\n\n    The purpose of the study is to develop a stance detection system for Turkish tweets.雅黑\n\nQuestion: What are the main findings of the study?\n\nAnswer:雅黑\n\n    The main findings of the study are that unigram-based features lead to superior results compared to bigram-based features, and that the existence of hashtags can", "answers": ["Galatasaray, Fenerbahçe", "Galatasaray , Fenerbahçe "], "all_classes": null, "length": 2234}
{"pred": "    The experiments conducted are automatic evaluations and human evaluations. The automatic evaluations include sentiment delta, sentiment accuracy, BLEU score, geometric mean, and harmonic mean. The human evaluations include irony accuracy, sentiment preservation, and content preservation. The experiments are conducted to evaluate the performance of the model in generating ironic sentences.", "answers": ["Irony Classifier, Sentiment Classifier for Irony, Sentiment Classifier for Non-irony, transformation from ironic sentences to non-ironic sentences"], "all_classes": null, "length": 4600}
{"pred": "Gaussian-masked directional multi-head attention works by adjusting the weight between characters and their adjacent character to a larger value which stands for the effect of adjacent characters. The Gaussian weight only relies on the distance between characters and is calculated using the cumulative distribution function of Gaussian. The Gaussian weight matrix is used to combine the Gaussian weight to the self-attention. The Gaussian-masked attention ensures that the relationship between two characters with long distances is weaker than adjacent characters. The Gaussian-masked attention is used in the forward and backward encoders to capture information of two directions which correspond to two parts divided by the gap. The central encoder is a special directional", "answers": ["pays attentions to adjacent characters and casts a localness relationship between the characters as a fixed Gaussian weight assuming the weight relies on the distance between characters", "Gaussian-masked directional attention can be described as a function to map queries and key-value pairs to the representation of input, Gaussian-masked directional attention expects to pay attention to the adjacent characters of each positions and cast the localness relationship between characters as a fix Gaussian weight for attention, Gaussian weight only relys on the distance between characters"], "all_classes": null, "length": 3612}
{"pred": "        Facebook status update messages.", "answers": ["Facebook status update messages", "Facebook status update messages"], "all_classes": null, "length": 4005}
{"pred": "    The baseline features are the features extracted from the fully-connected layer of the baseline CNN. These features are used to train the final softmax layer of the baseline CNN. The baseline features are the features that the baseline CNN learns to identify sarcastic and non-sarcastic tweets. These features are the inherent semantics of the sarcastic corpus that the baseline CNN learns to extract. The baseline features are the features that the baseline CNN learns to identify sarcastic and non-sarcastic tweets. These features are the inherent semantics of the sarcastic corpus that the baseline CNN learns to extract. The baseline features are the features that the baseline CNN learns", "answers": [" The features extracted from CNN."], "all_classes": null, "length": 4855}
{"pred": "        The number of clusters and the type of word vectors were varied in the experiments on the four tasks. The number of clusters was varied between 250 and 1000, and the type of word vectors was varied between skipgram, cbow, and GloVe. The hyperparameters were varied to investigate the impact of different types of word vectors and different numbers of clusters on the performance of the tasks. The results showed that the performance of the tasks was improved by incorporating cluster membership features, and that the best performance was achieved using in-domain word vectors and a high number of clusters. The results also showed that out-of-domain word vectors can", "answers": ["number of clusters, seed value in clustering, selection of word vectors, window size and dimension of embedding", "different number of clusters, different embeddings"], "all_classes": null, "length": 2753}
{"pred": "        0.716, 0.718, 0.716, 0.718, 0.716, 0.718, 0.716, 0.718, 0.716, 0.718, 0.716, 0.718, 0.716, 0.718, 0.716, 0.718, 0.716, 0.718, 0.716, 0.718, 0.716, 0.718, 0.716, 0.718, 0.716, 0", "answers": ["column Ens Test in Table TABREF19"], "all_classes": null, "length": 2424}
{"pred": "        53 documents, with an average of 156.1 sentences per document and 167,739 words in total. The corpus consists of 8,275 sentences and 167,739 words in total. The number of annotated entities is summarized in Table TABREF24. The most frequently annotated type of entity is findings, with an average length of 2.6 tokens. The number of tokens per entity ranges from one token for all types to 5 tokens for cases (average length 3.1), nine tokens for conditions (average length 2.0), 16 tokens for factors (average length 2.5", "answers": ["8,275 sentences and 167,739 words in total", "The corpus comprises 8,275 sentences and 167,739 words in total."], "all_classes": null, "length": 2669}
{"pred": "    Yes, it is possible to convert a cloze-style questions to a naturally-looking questions by replacing the missing spans of text with the answer candidates. This process is known as cloze completion. The resulting questions are more natural and easier to understand for humans. However, the conversion process may introduce some noise and errors, which can affect the performance of the QA system. Therefore, it is important to carefully evaluate the quality of the converted questions and ensure that they are still relevant and useful for the task at hand.", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 2764}
{"pred": "    Text categorization and sentiment classification.", "answers": ["text classification for themes including sentiment, web-page, science, medical and healthcare"], "all_classes": null, "length": 3591}
{"pred": "        The previous methods are term frequency models, rule-based methods, and learned methods. The model is compared to the best previous models on the TREC and GARD datasets. The model also achieves state-of-the-art performance on the MLBioMedLAT dataset. (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no", "answers": ["bag-of-words model, CNN"], "all_classes": null, "length": 5838}
{"pred": "        The training sets of these versions of ELMo are significantly larger compared to the previous ones. The previous versions of ELMo were trained on corpora of 20 million tokens, while the new versions of ELMo are trained on corpora of 270 million tokens (for Latvian) and 280 million tokens (for Estonian). The larger training sets allow the models to capture more semantic and contextual information, resulting in better performance on downstream tasks. (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no)", "answers": ["By 14 times.", "up to 1.95 times larger"], "all_classes": null, "length": 3294}
{"pred": "        6946 sentences\n\n        16225 unique words\n\n        3 major classes (Person, Location, Organization)\n\n        299 post-positions\n\n        10% F1 score improvement after lemmatizing post-positions\n\n        95.14% accuracy of POS-tagged BiLSTM model\n\n        64%, 16%, 20% of total dataset into training, development, and test sets\n\n        72782 unique words in word embeddings\n\n        300 dimensions for word embeddings\n\n        30 dimensions for character-level embeddings\n\n        30 dimensions for grapheme-level embeddings\n\n        1-", "answers": ["3606", "6946"], "all_classes": null, "length": 2835}
{"pred": "        Eusboost and MWMOTE techniques.       \n\n        Eusboost and MWMOTE techniques.       \n\n        Eusboost and MWMOTE techniques.       \n\n        Eusboost and MWMOTE techniques.       \n\n        Eusboost and MWMOTE techniques.       \n\n        Eusboost and MWMOTE techniques.       \n\n        Eusboost and MWMOTE techniques.       \n\n        Eusboost and MWMOTE techniques.       \n\n        Eusboost and MWMOTE techniques.       \n\n        Eusboost and MWMOTE techniques.       \n\n        Eusboost and MWMOTE", "answers": ["MLP", "Eusboost, MWMOTE"], "all_classes": null, "length": 2474}
{"pred": "Yes, their NER model learns NER from both text and images. The model takes as input a sequence of tokens, each of which comprises a word token, a character sequence, and an image, in their respective representation. The model then uses a modality attention module to combine representations from each modality to produce an entity label for each token. The modality attention module learns a unified representation space for multiple available modalities (words, characters, images, etc.), and produces a single vector representation with aggregated knowledge among multiple modalities, based on their weighted importance. The model is trained via log-likelihood maximization for the training", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3784}
{"pred": "        Yes, they evaluate only on English datasets. They use the Wall Street Journal (WSJ) portion of the Penn Treebank for both POS tagging and dependency parsing. The skip-gram word embeddings are trained on the one billion word language modeling benchmark dataset in addition to the WSJ corpus. The experiments are conducted on the WSJ corpus, which is a collection of news articles from the Wall Street Journal. The authors do not mention evaluating on any other datasets. Therefore, it can be concluded that they only evaluate on English datasets. (Note: This is a hypothetical answer and may not reflect the actual evaluation performed by the authors.) (", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 4320}
{"pred": "    32% (in Batch 2) (in Batch 2) (in Batch 2) (in Batch 2) (in Batch 2) (in Batch 2) (in Batch 2) (in Batch 2) (in Batch 2) (in Batch 2) (in Batch 2) (in Batch 2) (in Batch 2) (in Batch 2) (in Batch 2) (in Batch 2) (in Batch 2) (in Batch 2) (in Batch 2) (in Batch 2) (in Batch ", "answers": ["0.5115", "0.6103"], "all_classes": null, "length": 6810}
{"pred": "    The datasets they evaluate on are the Wall Street Journal (WSJ) portion of the Penn Treebank for both POS tagging and dependency grammar induction. They use skip-gram word embeddings trained on the one billion word language modeling benchmark dataset and the WSJ corpus. davidjl\n\nQuestion: What is the general experimental setup?\n\nAnswer: 노출등록\n\n    The general experimental setup involves using rectified networks as coupling functions, varying the number of coupling layers, optimizing marginal data likelihood directly using Adam, and not tuning hyper-parameters using supervised data. davidjl\n\nQuestion: What is the unsupervised POS tagging task?\n\nAnswer: 노출등록\n\n    The uns", "answers": [" Wall Street Journal (WSJ) portion of the Penn Treebank", "Unanswerable"], "all_classes": null, "length": 4327}
{"pred": "        The authors cite several studies that show that engineers often face challenges when applying DNN models to specific NLP tasks. These challenges include the need to choose from multiple frameworks, models and optimization techniques, which can be time-consuming and error-prone. The authors also provide examples of these challenges, such as the need to choose between different embedding layers, neural network layers, loss functions, and metrics. The authors argue that these challenges hinder the productivity of engineers and result in less optimal solutions to their given tasks. They claim that NeuronBlocks addresses these challenges by providing a two-layer solution that satisfies the requirements of all three types of engineers.", "answers": ["By conducting a survey among engineers", "Unanswerable"], "all_classes": null, "length": 1692}
{"pred": "        On SimpleQuestions and WebQSP.ยนตร์\n\n        On SimpleQuestions and WebQSP.ยนตร์\n\n        On SimpleQuestions and WebQSP.ยนตร์\n\n        On SimpleQuestions and WebQSP.ยนตร์\n\n        On SimpleQuestions and WebQSP.ยนตร์\n\n        On SimpleQuestions and WebQSP.ยนตร」\n\n        On SimpleQuestions and WebQSP.ยนตร」\n\n        On SimpleQuestions and WebQSP.ยนตร」\n\n        On SimpleQuestions and WebQSP.ยนตร」\n\n        On SimpleQuestions and WebQSP.ยนตร」\n\n        On SimpleQuestions and WebQSP.ยนตร」\n\n        On SimpleQuestions and WebQ", "answers": ["SimpleQuestions, WebQSP", "WebQSP, SimpleQuestions"], "all_classes": null, "length": 4524}
