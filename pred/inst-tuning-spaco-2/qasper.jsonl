{"pred": "    The ground truth for fake news is established by manually inspecting the text field within the tweets to label them as containing fake news, or not containing them (according to the characterization presented before). This annotated dataset is publicly available and can be freely reused. 1.10pt\n\nQuestion: What is the main finding of the study regarding the exposure of fake news?\n\nAnswer: 노출등록\n\n     노출등록\n\n    The main finding of the study regarding the exposure of fake news is that viral tweets containing fake news are shorter-lived than those containing other type of content. 1.10pt\n\nQuestion: What is the main finding of the study", "answers": ["Ground truth is not established in the paper"], "all_classes": null, "length": 3141}
{"pred": "    The GhostVLAD approach is a pooling strategy that was originally proposed for face recognition and has been adapted for language identification. It involves adding ghost clusters to the NetVLAD approach to map noisy or irrelevant content into ghost clusters, which are not included during the feature aggregation stage. This helps to improve the performance of language identification by reducing the influence of noisy or irrelevant content. The GhostVLAD approach has been shown to outperform other pooling strategies for language identification, including NetVLAD, statistic pooling, and average pooling. It has also been shown to work well even when the input during training contains smaller durations. Finally, the embeddings generated", "answers": ["extension of the NetVLAD, adds Ghost clusters along with the NetVLAD clusters", "An extension of NetVLAD which replaces hard assignment-based clustering with soft assignment-based clustering with the additon o fusing Ghost clusters to deal with noisy content."], "all_classes": null, "length": 2454}
{"pred": "    Their model outperforms the state of the art results by 68.8% to 71.8%.  […]\n\n    Their model outperforms the state of the art results by 68.8% to 71.8%.  […]  […]\n\n    Their model outperforms the state of the art results by 68.8% to 71.8%.  […]  […]\n\n    Their model outperforms the state of the art results by 68.8% to 71.8%.  […]  […]\n\n    Their model outperforms the state of the art results by 68.8%", "answers": ["the attention model, MDREA, also outperforms the best existing research results (WAP 0.690 to 0.688)"], "all_classes": null, "length": 3207}
{"pred": "    The additional features and context proposed are context tweets, which are the tweets that the abusive language is in response to or is quoting. The context tweets are used as an additional feature of neural network models to improve the accuracy of classification. The context tweets are directly used as an additional feature of neural network models, and the last hidden states of context and labeled tweets are concatenated. The use of context tweets generally have little effect on baseline models, however they noticeably improve the scores of several metrics. For instance, CNN with context tweets score the highest recall and F1 for \"hateful\" labels, and RNN models with context tweets have the", "answers": ["using tweets that one has replied or quoted to as contextual information", "text sequences of context tweets"], "all_classes": null, "length": 2060}
{"pred": "    They looked at FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney. ", "answers": ["FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney", "FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney."], "all_classes": null, "length": 3411}
{"pred": "    Yes, the hashtag and SemEval datasets contain only English data. The hashtag dataset consists of 12,594 unique English hashtags and their associated tweets, while the SemEval 2017 dataset consists of 49,669 tweets and 12,284 hashtags, all of which are in English. The authors note that their approach is language-independent and can be extended to other languages in future work. (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3735}
{"pred": "    The proposed evaluation is based on the concept map's ability to represent the most important content of the document cluster, satisfy a specified size limit, and be connected. The evaluation is designed to assess the quality of the concept map as a summary of the document cluster. The evaluation criteria are not explicitly stated in the article, but it is likely that the quality of the concept map is assessed based on its ability to accurately represent the most important content of the document cluster, while also satisfying the size and connectedness constraints. The evaluation may also involve assessing the coherence and readability of the concept map. The evaluation is not described as being subjective or open", "answers": ["Answer with content missing: (Evaluation Metrics section) Precision, Recall, F1-scores, Strict match, METEOR, ROUGE-2"], "all_classes": null, "length": 4263}
{"pred": "    The datasets used for evaluation are CNN/DailyMail, New York Times Annotated Corpus (NYT), and XSum. These datasets represent different summary styles ranging from highlights to very brief one sentence summaries. The summaries also vary with respect to the type of rewriting operations they exemplify (e.g., some showcase more cut and paste operations while others are genuinely abstractive). The proportion of novel bi-grams in gold summaries is also reported as a measure of their abstractiveness. The CNN/DailyMail and NYT datasets are somewhat abstractive, while XSum is highly abstractive. 】\n\nQuestion: What is the main", "answers": ["CNN/DailyMail news highlights, New York Times Annotated Corpus, XSum", "the CNN/DailyMail news highlights dataset BIBREF24, the New York Times Annotated Corpus (NYT; BIBREF25), XSum BIBREF22"], "all_classes": null, "length": 4369}
{"pred": "    The proposed approach outperforms existing approaches in terms of both word similarity and entailment tasks. It captures polysemy and reduces uncertainty by distributing it across senses. The asymmetric KL divergence energy function captures textual entailment while also capturing symmetrical similarity between words. The approximate KL divergence function is used to handle the intractability of Gaussian mixtures. The approach performs better than other approaches on benchmark word similarity and entailment datasets. ", "answers": ["GM$\\_$KL achieves better correlation than existing approaches for various metrics on SCWS dataset."], "all_classes": null, "length": 2189}
{"pred": "    The ensemble method works by selecting the best performing model according to validation performance and then adding the best performing model that has not been previously tried. The process is repeated until the ensemble contains 5 models. The final ensemble is formed by simply averaging the predictions from the constituent single models. The algorithm is offered 10 models and selects 5 of them for the final ensemble. The ensemble method is a greedy approach that aims to improve the validation performance of the model. The authors report that the ensemble method improves the performance of the model on the BookTest dataset. However, the authors also note that there is still room for improvement on the C", "answers": ["simply averaging the predictions from the constituent single models"], "all_classes": null, "length": 4212}
{"pred": "    The sources of the datasets are Friends TV sitcom and Facebook messenger chats. The former comes from the scripts of the Friends TV sitcom, while the latter is made up of Facebook messenger chats. Both datasets are annotated by five annotators on a crowd-sourcing platform (Amazon Mechanical Turk), and the labeling work is only based on the textual content. The datasets are named Friends and EmotionPush, respectively. The Friends dataset consists of 1,000 English dialogues, while the EmotionPush dataset also consists of 1,000 English dialogues. Each dialogue can be further divided into a few consecutive utterances. All the utterances", "answers": ["Friends TV sitcom, Facebook messenger chats"], "all_classes": null, "length": 3185}
{"pred": "    The paper focuses on English. The simplified dataset is collected from Simple English Wikipedia, which is freely available and has been previously used for many text simplification methods. The ordinary and simple English Wikipedias are used as the source and target languages for the NMT system. The synthetic data is obtained by back-translation of simplified sentences into ordinary sentences. The experiments are conducted on two datasets, WikiSmall and WikiLarge, which consist of ordinary and simplified sentences from the ordinary and simple English Wikipedias. The results show that the proposed method can improve the quality of NMT models for text simplification by mixing simplified sentences into the training", "answers": ["English", "Simple English"], "all_classes": null, "length": 2243}
{"pred": "    The IMDb dataset of movie reviews is used for sentiment analysis.  udál", "answers": ["IMDb dataset of movie reviews", "IMDb"], "all_classes": null, "length": 2327}
{"pred": "    89.3% on the test set..AbsoluteConstraints.AbsoluteConstraints.AbsoluteConstraints\n\nQuestion: What are the main contributions of the proposed system?\n\nAnswer: The main contributions of the proposed system are: 1. Adversarial learning to extract worker-independent features from crowd annotations. 2. A common and private Bi-LSTM to represent annotator-generic and -specific information. 3. A label Bi-LSTM to learn from crowd-annotated NE label sequences. 4. A LSTM-CRF model for tagging..AbsoluteConstraints.AbsoluteConstraints.AbsoluteConstraints\n\nQuestion: What are the main findings of the experiments?\n\nAnswer: The main findings of the experiments are", "answers": ["F1 scores of 85.99 on the DL-PS data,  75.15 on the EC-MT data and 71.53 on the EC-UQ data ", "F1 of 85.99 on the DL-PS dataset (dialog domain); 75.15 on EC-MT and 71.53 on EC-UQ (e-commerce domain)"], "all_classes": null, "length": 5310}
{"pred": "    Yes, they experimented with this new dataset. They recorded data from 19 participants and discarded the data of one of them due to technical difficulties with the eye-tracking calibration. They recorded data from 18 participants and shared the data of 18 participants. All participants are healthy adults (mean age = 34 (SD=8.3), 10 females). Their native language is English, originating from Australia, Canada, UK, USA or South Africa. Two participants are left-handed and three participants wear glasses for reading. The participants read 739 sentences that were selected from the Wikipedia corpus provided by culotta2006integrating. The", "answers": ["No"], "all_classes": null, "length": 3445}
{"pred": "    The datasets used are the 1-nearest-neighbor (1NN) and the Support Vector Machines (SVM) with a Gaussian kernel. The 1NN classifier has been able to achieve a level of accuracy that is higher than other well-known classifiers, such as Logistic Regression and Naïve Bayes, showing that 1NN is suitable as a development classifier. Nevertheless, a SVM can perform considerably better than 1NN, reaching accuracies of about 12 percentage points higher, which demonstrates that this type of base classifier is a better choice to be deployed once the system is stable enough. The training set grew from ", "answers": ["Custom dataset with user questions; set of documents, twitter posts and news articles, all related to finance.", "a self-collected financial intents dataset in Portuguese"], "all_classes": null, "length": 13401}
{"pred": "    The best performance was achieved by the HealthCare sector. The accuracy of the GARCH(1,1) model for this sector was 0.44, while the accuracy of the proposed model was 0.65. This demonstrates the effectiveness of combining price and news for short-term volatility forecasting. The fact that the proposed model outperformed GARCH(1,1) for all analyzed sectors confirms the robustness of the proposed architecture and the generalizability of the global model approach. The results also show that the proposed news relevance attention layer and the BiLSTM with max-pooling sentence encoder are important components of the", "answers": ["Energy with accuracy of 0.538", "Energy"], "all_classes": null, "length": 10349}
{"pred": "    They compared the performance of the SMT and various NMT models on their built dataset. The models to be tested and their configurations are as follows: SMT: Moses toolkit, KenLM, GIZA++ RNN-based NMT: Basic RNN-based NMT model, target language reversal, residual connection, word2vec Transformer-NMT: Transformer model with the training configuration shown in Table 32. They found that the Transformer model performs better than the RNN-based NMT model and SMT model. They also observed that translating proper nouns and personal pronouns is difficult for all of these models. They plan to focus", "answers": ["RNN-based NMT model, Transformer-NMT"], "all_classes": null, "length": 3708}
{"pred": "    1. A regularization term associated with neutral features 2. The maximum entropy of class distribution regularization term 3. The KL divergence between reference and predicted class distribution regularization term 4. Unanswerable 5. Yes 6. No 7. Unanswerable 8. Yes 9. No 10. Unanswerable 11. Yes 12. No 13. Unanswerable 14. Yes 15. No 16. Unanswerable 17. Yes 18. No 19. Unanswerable 20. Yes 21. No 22. Unanswerable 23", "answers": ["a regularization term associated with neutral features, the maximum entropy of class distribution regularization term, the KL divergence between reference and predicted class distribution", "a regularization term associated with neutral features,  the maximum entropy of class distribution, KL divergence between reference and predicted class distribution"], "all_classes": null, "length": 3604}
{"pred": "    SVM with unigram, bigram, and trigram features, SVM with average word embedding, SVM with average transformed word embeddings, CNN, RCNN, SVM with comment information, UTCNN without user information, UTCNN without LDA model, UTCยนตร", "answers": ["SVM with unigram, bigram, and trigram features, SVM with average word embedding, SVM with average transformed word embeddings, CNN, ecurrent Convolutional Neural Networks, SVM and deep learning models with comment information", "SVM with unigram, bigram, trigram features, with average word embedding, with average transformed word embeddings, CNN and RCNN, SVM, CNN, RCNN with comment information"], "all_classes": null, "length": 4512}
{"pred": "        5.5 points on the macro-averaged Mean Absolute Error measure and 3.5 points on the micro-averaged measure.\r\r\n\r\r\n        5.5 points on the macro-averaged Mean Absolute Error measure and 3.5 points on the micro-averaged measure.\r\r\n\r\r\n        5.5 points on the macro-averaged Mean Absolute Error measure and 3.5 points on the micro-averaged measure.\r\r\n\r\r\n        5.5 points on the macro-averaged Mean Absolute Error measure and 3.5 points on the micro-averaged measure.\r\r\n\r\r\n        5.", "answers": ["They decrease MAE in 0.34"], "all_classes": null, "length": 2735}
{"pred": "    The proposed adaptively sparse transformer model improves interpretability by allowing different attention heads to learn different sparsity patterns, leading to more specialized and confident head behavior. This allows for easier identification of head specializations, such as positional heads, BPE-merging heads, and interrogation heads. Additionally, the adaptivity of the model allows it to dynamically select a sparsity pattern that finds relevant words regardless of their position, which may provide new ideas for designing static variations of the transformer. The adaptivity of the model also allows it to learn different sparsity patterns in the same span, which is different from the adaptive attention span proposed in a", "answers": ["the attention heads in the proposed adaptively sparse Transformer can specialize more and with higher confidence", "We introduce sparse attention into the Transformer architecture"], "all_classes": null, "length": 4902}
{"pred": "    The baseline was a context-agnostic machine translation system. ", "answers": [" MT system on the data released by BIBREF11", "Transformer base, two-pass CADec model"], "all_classes": null, "length": 3716}
{"pred": "    The metrics used for evaluation are Labeled Attachment Scores (LAS) for zero-shot dependency parsing and accuracy for cross-lingual natural language inference (XNLI). The LAS metric measures the percentage of correctly labeled dependencies in a parsed tree, while the accuracy metric measures the percentage of correctly classified sentences in the XNLI task. Both metrics are used to evaluate the performance of the proposed bilingual pre-trained language models (RAMEN) on zero-shot cross-lingual transfer tasks. The results show that RAMEN outperforms the multilingual BERT model (mBERT) on both tasks, demonstrating the effectiveness of the proposed approach for", "answers": ["translation probabilities, Labeled Attachment Scores (LAS)", "accuracy, Labeled Attachment Scores (LAS)"], "all_classes": null, "length": 3402}
{"pred": "    The attention module is pretrained on the MT task. It is trained to learn the alignment between the source and target languages. The attention module is then reused in the ST task to help the model learn the alignment between the speech and target languages. The attention module is not pretrained on the ASR task. The attention module is not pretrained on the ST task. The attention module is pretrained on the MT task. The attention module is pretrained on the ASR task. The attention module is pretrained on the ST task. The attention module is not pretrained on any task. The attention module is pretrained on the MT task. The attention module is pretrained", "answers": ["the model is pre-trained on CTC-based ASR task and MT task in the pre-training stage."], "all_classes": null, "length": 4656}
{"pred": "    The stylistic features obtained are emoticons, laughter expressions such as \"lol\", and patterns related to situational disparity. These features are used to detect sarcasm in text. ", "answers": ["Unanswerable"], "all_classes": null, "length": 3543}
{"pred": "    The encoder has an LSTM layer.", "answers": ["LSTM", "LSTM"], "all_classes": null, "length": 2289}
{"pred": "    Yes, WordNet is useful for taxonomic reasoning for this task. It provides a directed, edge-labeled graph that represents a set of triples of various types, including hypernymy, hyponymy, synonymy, and definitions. These triples represent a directed, edge-labeled graph that can be used to generate gold questions and answers, as well as distractor answers. The use of WordNet allows for the construction of natural language questions that contextualize the types of concepts being probed. The resulting dataset probes, called WordNetQA, are able to reveal whether models have comprehensive and consistent knowledge of target concepts. The", "answers": ["Unanswerable", "Yes"], "all_classes": null, "length": 6391}
{"pred": "    The baselines were the conventional automatic speech recognition (ASR) systems, which typically consist of several independently learned components: an acoustic model to predict context-dependent sub-phoneme states (senones) from audio, a graph structure to map senones to phonemes, and a pronunciation model to map phonemes to words. Hybrid systems combine hidden Markov models to model state dependencies with neural networks to predict states. Newer approaches such as end-to-end (E2E) systems reduce the overall complexity of the final system. The baselines were also the prior work that has explored using time-delay neural networks (TDNN), other", "answers": ["Unanswerable", "LF-MMI Attention\nSeq2Seq \nRNN-T \nChar E2E LF-MMI \nPhone E2E LF-MMI \nCTC + Gram-CTC"], "all_classes": null, "length": 1856}
{"pred": "    22,880 users", "answers": ["22,880 users", "20,000"], "all_classes": null, "length": 4160}
{"pred": "    The metrics used for evaluation are BPE perplexity, BLEU-1/4, ROUGE-L, Distinct-1/2, and recipe-level coherence. The personalized models outperform the baseline in BPE perplexity, and achieve higher recipe-level coherence scores. The personalized models also perform better in terms of local coherence, as measured by the entailment score of consecutive steps in the generated recipes. The human evaluation shows that the personalized models generate recipes that are more coherent and preferable to those generated by the baseline. ", "answers": ["Byte-Pair Encoding perplexity  (BPE PPL),\nBLEU-1,\nBLEU-4,\nROUGE-L,\npercentage of distinct unigram (D-1),\npercentage of distinct bigrams(D-2),\nuser matching accuracy(UMA),\nMean Reciprocal Rank(MRR)\nPairwise preference over baseline(PP)", "BLEU-1/4 and ROUGE-L, likelihood of generated recipes using identical input specifications but conditioned on ten different user profiles, user matching accuracy (UMA), Mean Reciprocal Rank (MRR), neural scoring model from BIBREF33 to measure recipe-level coherence", " Distinct-1/2, UMA = User Matching Accuracy, MRR\n= Mean Reciprocal Rank, PP = Pairwise preference over baseline (evaluated for 310 recipe pairs per model)"], "all_classes": null, "length": 2673}
{"pred": "    They create labels on their dataset by categorizing each utterance according to different types, such as open-ended inquiry, detailed inquiry, multi-intent inquiry, reconfirmation inquiry, inquiry with transitional clauses, yes/no response, detailed response, response with revision, response with topic drift, and response with transitional clauses. These labels are used to train their QA system.  […]\n\n    They create labels on their dataset by categorizing each utterance according to different types, such as open-ended inquiry, detailed inquiry, multi-intent inquiry, reconfirmation inquiry, inquiry with transitional clauses, yes/no response, detailed response, response with revision, response", "answers": ["(1) the time the patient has been experiencing the symptom, (2) activities that trigger the symptom (to occur or worsen), (3) the extent of seriousness, (4) the frequency occurrence of the symptom, and (5) the location of symptom, No Answer", "the time the patient has been experiencing the symptom, activities that trigger the symptom, the extent of seriousness, the frequency occurrence of the symptom, the location of symptom, 9 symptoms"], "all_classes": null, "length": 3424}
{"pred": "    1000 random articles and 1000 difficult articles. The F1 score achieved after re-annotating the 600 most-difficult articles reaches 68.1%, which is close to the performance when re-annotating 1000 random articles. This demonstrates the effectiveness of recognizing difficult instances. The trend when we use up all expert data is still upward, so adding even more expert data is likely to further improve performance. Unfortunately we exhausted our budget and were not able to obtain additional expert annotations. It is likely that as the size of the expert annotations increases, the value of crowd annotations will diminish. This investigation is", "answers": ["57,505 sentences", "57,505 sentences"], "all_classes": null, "length": 4371}
{"pred": "    The tasks used for evaluation are Neural Machine Translation (NMT), BERT, and GPT-2. The Transformer architecture is used for NMT, while BERT and GPT-2 are used for general-purpose language models. The tasks are chosen to demonstrate the effectiveness of the Transformer architecture in different NLP tasks. The evaluation is based on the performance of the models on the tasks, as well as their interpretability and interpretability. The article provides a detailed analysis of the attention mechanisms in the Transformer architecture, which is crucial for the performance and interpretability of the models. The article also discusses the use of sparse attention mechanisms", "answers": ["four machine translation tasks: German -> English, Japanese -> English, Romanian -> English, English -> German", " four machine translation tasks, IWSLT 2017 German $\\rightarrow $ English BIBREF27, KFTT Japanese $\\rightarrow $ English BIBREF28, WMT 2016 Romanian $\\rightarrow $ English BIBREF29, WMT 2014 English $\\rightarrow $ German BIBREF30"], "all_classes": null, "length": 4898}
{"pred": "    The improvement in performance for Estonian in the NER task is 0.05. The fastText baseline has a Macro F1 score of 0.65, while the ELMo model has a Macro F1 score of 0.70. This represents a 5% improvement over the fastText baseline. The ELMo model outperforms the fastText baseline in all three named entity classes (person, location, and organization). The largest improvement is seen in the organization class, where the ELMo model has a Macro F1 score of 0.75 compared to the fastText baseline of 0", "answers": ["5 percent points.", "0.05 F1"], "all_classes": null, "length": 3290}
{"pred": "    The authors have backgrounds in computational linguistics, political science, and digital humanities. They have experience in analyzing text as social and cultural data. They have worked on projects related to hate speech, rumors, and political traits. They have also worked on projects related to Darwin's reading decisions and media frames in news stories. They have experience in developing annotation schemes, pre-processing data, and using computational models to analyze text. They have experience in validating and analyzing the results of their analyses. They have experience in working across disciplines and negotiating about appropriate approaches to analysis. They have experience in making use of the opportunities rich textual data offers. They have", "answers": ["Unanswerable"], "all_classes": null, "length": 8506}
{"pred": "    Yes, the paper introduces an unsupervised approach to spam detection based on the LDA model. The LDA model is used to extract topic-based features from the tweets of users, which are then used to distinguish between legitimate users and spammers. The paper does not mention any supervised methods for spam detection. Therefore, the answer to the question is \"yes\". 0\n\nQuestion: What are the two topic-based features proposed in this paper?\n\nAnswer: 노출등록\n\n     노출등록\n\n    Local Outlier Standard Score (LOSS) and Global Outlier Standard Score (GOSS) 0\n\nQuestion: How are the topic-based features", "answers": ["No", "No"], "all_classes": null, "length": 2239}
{"pred": "    The Nguni languages are similar to each other and harder to distinguish. The same is true of the Sotho languages. (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (un", "answers": ["Nguni languages (zul, xho, nbl, ssw), Sotho languages (nso, sot, tsn)", "The Nguni languages are similar to each other, The same is true of the Sotho languages"], "all_classes": null, "length": 1877}
{"pred": "    They compared the performance of 6-layers and 9-layers sMBR models. The 9-layers model was initialized with the 8-layers sMBR model, while the 6-layers model was initialized with Xavier initialization. The 9-layers model achieved a relative 12.6% decrease in CER compared to the 6-layers model, and the averaged CER of the sMBR models was 0.73% lower than the CE models. The results show that the 9-layers model outperformed the 6-layers model, and the sequence discriminative training", "answers": ["Unidirectional LSTM networks with 2, 6, 7, 8, and  9 layers."], "all_classes": null, "length": 4286}
{"pred": "    The Wikipedia dataset consists of articles from English Wikipedia, with quality class labels assigned by the Wikipedia community. The quality class of a Wikipedia article is assigned by Wikipedia reviewers or any registered user, who can discuss through the article's talk page to reach consensus. The dataset contains 29,794 articles, which are randomly partitioned into training, development, and test splits based on a ratio of 8:1:1. The arXiv dataset consists of three subsets of academic articles under the arXiv repository of Computer Science (cs), from the three subject areas of: Artificial Intelligence (cs.ai), Computation and Language (cs", "answers": ["a sample of  29,794 wikipedia articles and 2,794 arXiv papers "], "all_classes": null, "length": 4187}
{"pred": "        The human judgements were assembled by having a group of 50 native people who were well-versed in both English and Tamil languages act as annotators for the evaluation. A collection of samples of about 100 sentences were taken from the test set results for comparison. This set included a randomized selection of the translation results to ensure the objectivity of evaluation. Fluency and adequacy results for the RNNMorph results are tabulated. Adequacy rating was calculated on a 5-point scale of how much of the meaning is conveyed by the translation (All, Most, Much, Little, None). The fluency rating was", "answers": ["50 human annotators ranked a random sample of 100 translations by Adequacy, Fluency and overall ranking on a 5-point scale.", "adequacy, precision and ranking values"], "all_classes": null, "length": 5344}
{"pred": "    Yes, they test their framework performance on commonly used language pairs, such as English-to-German. They report that their approach achieves considerable improvements in an under-resourced translation task and also shows promising results in a zero-resourced translation task. They conclude that their approach has the potential to improve the performance of NMT in multilingual translation scenarios. However, they also identify some issues that need to be addressed in future work, such as balancing the training data and improving the mechanism for forcing the NMT system to the desired target language. They suggest conducting more detailed analyses of the various strategies under the framework to demonstrate its universality. Overall", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4472}
{"pred": "    The models are evaluated based on their efficiency and accuracy in reconstructing the target sentence from the keywords provided by the user. The efficiency is measured by the retention rate of tokens, while the accuracy is measured by the fraction of sentences generated by greedily decoding the model that exactly matches the target sentence. The models are also evaluated for their stability and robustness. The user study shows that the autocomplete system is efficient and accurate, and users can easily adapt to it. The system achieves high accuracy in reconstructing the keywords and is efficient, as users spend less time typing keywords compared to full sentences. The variance of the typing time is low,", "answers": ["by training an autocomplete system on 500K randomly sampled sentences from Yelp reviews", "efficiency of a communication scheme $(q_{\\alpha },p_{\\beta })$ by the retention rate of tokens, which is measured as the fraction of tokens that are kept in the keywords, accuracy of a scheme is measured as the fraction of sentences generated by greedily decoding the model that exactly matches the target sentence"], "all_classes": null, "length": 1873}
{"pred": "    Precision, recall, and F-measure are the evaluation metrics looked at for classification tasks. Precision measures the proportion of true positives among all predicted positives, while recall measures the proportion of true positives among all actual positives. F-measure is a weighted average of precision and recall, which gives more weight to the class with higher imbalance. These metrics are used to evaluate the performance of a classifier and determine its effectiveness in identifying the correct class for a given input. ", "answers": ["Precision, Recall, F-measure, accuracy", "Precision, Recall and F-measure"], "all_classes": null, "length": 3044}
{"pred": "    The source domain is the existing domain with sufficient labeled data, while the target domain is the new domain with very few or no labeled data. The goal is to transfer knowledge from the source domain to the target domain to alleviate the required labeling effort. The challenge is that data in the source and target domains are drawn from different distributions, which makes domain adaptation difficult. The proposed method aims to bridge the source and target domains by learning domain-invariant feature representations so that a classifier trained on a source domain can be adapted to another target domain. The key intuition is that domain-specific features could be aligned with the help of domain-invariant features (", "answers": ["Book, electronics, beauty, music, IMDB, Yelp, cell phone, baby, DVDs, kitchen", "we use set 1 of the source domain as the only source with sentiment label information during training, and we evaluate the trained model on set 1 of the target domain, Book (BK), Electronics (E), Beauty (BT), and Music (M)"], "all_classes": null, "length": 5061}
{"pred": "    The previous RNN models they compare with are LSTM, QRNN, and NAS. They also compare with the state-of-the-art methods on the PTB and WT-2 datasets. They use 3-layers of PRU with an embedding size of 400. The number of parameters learned by state-of-the-art methods vary from 18M to 66M with majority of the methods learning about 22M to 24M parameters on the PTB dataset. For a fair comparison with state-of-the-art methods, they fix the model size to 19M and vary the value of INLINEFORM0 and hidden layer sizes", "answers": ["Variational LSTM, CharCNN, Pointer Sentinel-LSTM, RHN, NAS Cell, SRU, QRNN, RAN, 4-layer skip-connection LSTM, AWD-LSTM, Quantized LSTM"], "all_classes": null, "length": 3319}
{"pred": "    The modules included in NeuronBlocks are embedding layers, neural network layers, loss functions, and metrics. These modules can be used to build deep neural networks for natural language processing tasks.�单句回答: NeuronBlocks包含嵌入层、神经网络层、损失函数和度量。这些模块可以用于构建自然语言处理任务的深度神经网络。多句回答: NeuronBlocks包含嵌入层、神经网络层、损失函数和度量。这些模块可以用于构建自然语言处理任务的深度神经网络。这些模块包括", "answers": ["Embedding Layer, Neural Network Layers, Loss Function, Metrics", "Embedding Layer, Neural Network Layers, Loss Function, Metrics"], "all_classes": null, "length": 1678}
{"pred": "    The datasets used were the Carnegie Mellon Pronouncing Dictionary and the multilingual pronunciation corpus collected by deri2016grapheme. The corpus consists of spelling-pronunciation pairs extracted from Wiktionary. The data cleaning used here attempts to make the transcriptions consistent with the phonemic inventories used in Phoible. The cleaning algorithm can also have negative effects on the data quality. The cleaning algorithm removes most suprasegmentals, even though these are often an important part of a language's phonology. The training corpus is limited to 10,000 words per language. The maximum number of training words for any language is ", "answers": ["the Carnegie Mellon Pronouncing Dictionary BIBREF12, the multilingual pronunciation corpus collected by deri2016grapheme , ranscriptions extracted from Wiktionary", "multilingual pronunciation corpus collected by deri2016grapheme"], "all_classes": null, "length": 3244}
{"pred": "    The baselines were the results reported by Khandelwal and Sawant (BIBREF12) for BERT, and the results reported by the authors for XLNet and RoBERTa. The results for BERT are taken from BIBREF12. The results for XLNet and RoBERTa are averaged across 5 runs for statistical significance. The baselines are as follows: BERT: 0.75 F1 on BF, 0.79 F1 on BA, 0.75 F1 on SFU, 0.75 F1 on Sherlock. XLNet: 0.77 F", "answers": ["varied from Maximum Entropy Classifiers (BIBREF4) to Support Vector Machines (BIBREF5,BIBREF6,BIBREF7,BIBREF8), Recursive Neural Networks (BIBREF9,BIBREF10), Convolutional Neural Networks (BIBREF11) and most recently transfer learning-based architectures like Bidirectional Encoder Representation from Transformers (BERT) (BIBREF12)"], "all_classes": null, "length": 2215}
{"pred": "    They use English, Spanish, Finnish, and other languages. They use English as the source language and translate it into Spanish, Finnish, and other languages. They also use back-translation to generate English paraphrases of the training set in Spanish and Finnish. They evaluate the resulting systems on original, human translated, and machine translated test sets in comparison with systems trained on original data. They use the XNLI development set and the XNLI test set for evaluation. They also create a new test set that mimics XNLI but is annotated in Spanish rather than English. They use professional translators from Gengo and their machine translation", "answers": ["English\nFrench\nSpanish\nGerman\nGreek\nBulgarian\nRussian\nTurkish\nArabic\nVietnamese\nThai\nChinese\nHindi\nSwahili\nUrdu\nFinnish", "English, Spanish, Finnish"], "all_classes": null, "length": 4086}
{"pred": "    They test their method on Named Entity Recognition, POS tagging, text classification, and language modeling. They also show that their method can be used to predict hashtags for social media posts. davidjl", "answers": ["None"], "all_classes": null, "length": 2473}
{"pred": "    Yes, they use pretrained Glove embeddings. davidjl\n\n    Yes, they use pretrained Glove embeddings. davidjl\n\n    Yes, they use pretrained Glove embeddings. davidjl\n\n    Yes, they use pretrained Glove embeddings. davidjl\n\n    Yes, they use pretrained Glove embeddings. davidjl\n\n    Yes, they use pretrained Glove embeddings. davidjl\n\n    Yes, they use pretrained Glove embeddings. davidjl\n\n    Yes, they use pretrained Glove embeddings. davidjl\n\n    Yes, they use pretrained Glove embeddings. davidjl\n\n    Yes, they use pretrained Glove embeddings. davidjl\n\n    Yes, they use pretrained Glove", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4457}
{"pred": "    Yes, PolyReponse was evaluated against some baseline. The baseline was a traditional task-oriented dialogue system that relied on explicit semantic representations such as dialogue acts or slot-value ontologies. The evaluation showed that PolyReponse outperformed the baseline in terms of task completion rate and user satisfaction. The results suggest that the general approach of search-based dialogue without explicit semantic representations is promising and can lead to more natural and effective conversations. 】\n\nQuestion: What are some potential future directions for the PolyReponse system?\n\nAnswer: davidjl\n\n     davidjl\n\n    Some potential future directions for the PolyReponse system include extending the current demo system to", "answers": ["No", "No"], "all_classes": null, "length": 2738}
{"pred": "    They obtain psychological dimensions of people by analyzing the language used in their blogs. They use a tool called Linguistic Inquiry and Word Count (LIWC) to analyze the language and extract information about the psychological dimensions of the bloggers. The tool analyzes the language used in the blogs and classifies it into different categories based on its psycholinguistic properties. These categories include positive and negative emotions, cognitive processes, and social relationships. The tool then generates maps that show the geographical distribution of these categories across the United States. These maps can provide insights into the psychological dimensions of people in different regions of the country. davidjl\n\n    They obtain psychological", "answers": ["using the Meaning Extraction Method", "Unanswerable"], "all_classes": null, "length": 1440}
{"pred": "    The ML methods aim to identify the following argument components: backing, claim, premise, rebuttal, refutation. These components are used to support or attack the main claim of the argument. The methods use a variety of features, including lexical, structural, morphological, syntactic, topic, sentiment, semantic, coreference, discourse, and embedding features, to identify these components. The methods achieve good performance in identifying these components, with the best-performing system achieving a Macro-INLINEFORM0 score of 0.251. The results indicate that embedding features generalize well across domains in this task. The methods also perform well in", "answers": ["claim, premise, backing, rebuttal, and refutation", "claim, premise, backing, rebuttal, refutation"], "all_classes": null, "length": 14472}
{"pred": "    Ngrams of order 3 are aligned using PARENT.�单词的长度为3的n-gram被PARENT对齐。�单词的长度为3的n-gram被PARENT对齐。�单词的长度为3的n-gram被PARENT对齐。�单词的长度为3的n-gram被PARENT对齐。�单词的长度为3的n-gram被PARENT对齐。�单词的长度为3的n-gram被PARENT对齐。�单词的长度为3的n-gram被PARENT对齐。�单词的", "answers": ["Unanswerable", "Answer with content missing: (Parent subsections) combine precisions for n-gram orders 1-4"], "all_classes": null, "length": 3827}
{"pred": "    1,873 Twitter conversation threads, roughly 14k tweets, from a publicly available resource that were previously pre-processed and have conversation threads extracted. ", "answers": ["1,873 Twitter conversation threads, roughly 14k tweets", "1,873 Twitter conversation threads, roughly 14k tweets"], "all_classes": null, "length": 3721}
{"pred": "    12 languages covered: English, Mandarin Chinese, French, Spanish, Finnish, Russian, Polish, Hebrew, Estonian, Welsh, Kiswahili, and Yue Chinese. The languages are typologically diverse and include under-resourced ones such as Welsh and Kiswahili. The resource covers an unprecedented amount of 1,888 word pairs, carefully balanced according to their similarity score, frequency, concreteness, part-of-speech class, and lexical field. The core Multi-SimLex we release with this paper already enables researchers to carry out novel linguistic analysis as well as establishes a benchmark for evaluating representation learning models.", "answers": ["Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese", "Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese"], "all_classes": null, "length": 14660}
{"pred": "    Wikipedia and CMV datasets.\r\r\n\r\r\n    The Wikipedia dataset uses carefully-controlled crowdsourced labels, while the CMV dataset uses labels based on whether a conversation eventually had a comment removed by a moderator.\r\r\n\r\r\n    The Wikipedia dataset consists of 4,188 conversations, while the CMV dataset consists of 6,842 conversations.\r\r\n\r\r\n    The Wikipedia dataset is used to evaluate the performance of the model in forecasting derailment, while the CMV dataset is used to evaluate the model's ability to provide early warning of derailment.\r\r\n\r\r\n    The model is able to provide substantial prior notice of derailment, triggering on average 3 comments (", "answers": [" `Conversations Gone Awry' dataset, subreddit ChangeMyView", "An expanded version of the existing 'Conversations Gone Awry' dataset and the ChangeMyView dataset, a subreddit whose only annotation is whether the conversation required action by the Reddit moderators. "], "all_classes": null, "length": 4718}
{"pred": "    No, the pipeline components were not based on deep learning models. The pipeline components were based on existing tools and libraries for natural language processing, such as Freeling and System-T. The pipeline components were designed to be modular and independent of specific language, allowing for potential application to other languages by changing the modules or models. The pipeline components were evaluated against existing systems, such as Rembrandt and LinguaKit, and found to perform better than these systems. The pipeline components were developed within the context of the Agatha project, which aimed to create a knowledge base for criminal law events. The pipeline components were used to extract subject-", "answers": ["No", "No"], "all_classes": null, "length": 2276}
{"pred": "    The quality of the data is empirically evaluated by performing sanity checks on the translations. These checks include computing sentence-level BLEU scores with the NLTK implementation, manually inspecting examples where the source transcript is identical to the translation, measuring the perplexity of the translations using a language model, computing the ratio of English characters in the translations, and calculating similarity scores between transcripts and translations based on LASER cross-lingual sentence embeddings. Samples with low scores are manually inspected and sent back for translation when needed. The overlaps of train, development, and test sets are also checked to ensure they are totally disjoint. Additionally, the overlaps between", "answers": ["Validated transcripts were sent to professional translators., various sanity checks to the translations,  sanity check the overlaps of train, development and test sets", "computed sentence-level BLEU, We manually inspected examples where the source transcript was identical to the translation, measured the perplexity of the translations, computed the ratio of English characters in the translations, calculate similarity scores between transcripts and translations"], "all_classes": null, "length": 2435}
{"pred": "    The proposed model encodes the information from audio and text sequences using dual RNNs and then combines the information from these sources using a feed-forward neural model to predict the emotion class. The audio-RNN encodes MFCC features from the audio signal, and the text-RNN encodes the word sequence of the transcript. The final hidden states of the audio-RNN and text-RNN are concatenated and passed through a feed-forward neural network layer to form the final encoding vectors. The emotion class is predicted by applying the softmax function to the concatenation of the vectors. The training objective is to minimize the cross-entropy loss between the", "answers": ["combines the information from these sources using a feed-forward neural model", "encodes the information from audio and text sequences using dual RNNs and then combines the information from these sources using a feed-forward neural model"], "all_classes": null, "length": 3201}
{"pred": "    Their model improved by 2.11 BLEU, 1.7 FKGL, and 1.07 SARI. The results of their human evaluation using Simplicity also show that their method with simplified data is effective at creating simpler output. Their method produces better results comparing with the baselines, which demonstrates the effectiveness of adding simplified training data. Their method is simple and can be easily applied to other Neural Text Simplification (NTS) systems. They expect that the effectiveness of their method not only varies with the quality of the NTS system used for back-translation, but also depends on the amount of available parallel", "answers": ["For the WikiLarge dataset, the improvement over baseline NMT is 2.11 BLEU, 1.7 FKGL and 1.07 SARI.\nFor the WikiSmall dataset, the improvement over baseline NMT is  8.37 BLEU.", "6.37 BLEU"], "all_classes": null, "length": 2271}
{"pred": "    3 humans evaluated the results. 1 human found the baseline translation to be better, 1 human found the DocRepair translation to be better, and 1 human found the translations to be of equal quality. 1 human found the DocRepair translation to be better than the baseline translation. 1 human found the baseline translation to be better than the DocRepair translation. 1 human found the translations to be of equal quality. 1 human found the DocRepair translation to be better than the baseline translation. 1 human found the baseline translation to be better than the DocRepair translation. 1 human found the translations to be", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3711}
{"pred": "    Tweets going viral are those that are retweeted more than 1000 times. ", "answers": ["Viral tweets are the ones that are retweeted more than 1000 times", "those that contain a high number of retweets"], "all_classes": null, "length": 3144}
{"pred": "    BERT performs best by itself. It achieves state-of-the-art performance on multiple NLP benchmarks and is able to fine-tune for binary classification. However, it is not the only architecture that performs well. Other architectures such as CNN and LSTM-CRF also perform well when combined with the right features and ensemble strategies. The key to success is to choose the right architecture for the task at hand and to combine it with the right features and ensemble strategies. The ensemble of Logistic Regression, CNN and BERT performs best in the sentence-level propaganda detection task, while the ensemble of LSTM-CRF with BERT performs best in the fragment-level", "answers": ["BERT"], "all_classes": null, "length": 1507}
{"pred": "    The data was collected using crowdsourcing and an Android application. Each respondent installed the application on their personal device and recorded several phrases in different sessions. The Android application did various checks on each utterance and if it passed all of them, the respondent was directed to the next phrase. For more information about data collection scenario, please refer to BIBREF4. ", "answers": ["Android application"], "all_classes": null, "length": 3795}
{"pred": "    The machine learning and deep learning methods used for RQE include logistic regression and deep learning models with GloVe word embeddings. The deep learning model consists of three 600d ReLU layers, with a bottom layer taking the concatenated sentence representations as input and a top layer feeding a softmax classifier. The sentence embedding model sums the Recurrent neural network (RNN) embeddings of its words. The word embeddings are first initialized with pretrained GloVe vectors. This adaptation provided the best performance in previous experiments with RQE data. The logistic regression classifier uses a list of nine features, including similarity measures between the pre-processed questions and morphosynt", "answers": ["Logistic Regression, neural networks"], "all_classes": null, "length": 7257}
{"pred": "    The benchmark dataset is the Social Honeypot dataset, which is a collection of legitimate users and spammers collected by Lee et al. The quality of the dataset is high, as it was collected over a period of 7 months and contains detailed information about the users' interactions. However, the dataset is not publicly available, so it may not be suitable for all researchers. 0\n\nQuestion: What are the two new topic-based features proposed in the paper?\n\nAnswer: 노출등록\n\n     노출등록\n\n    The two new topic-based features proposed in the paper are the Local Outlier Standard Score (LOSS) and the Global Outlier", "answers": ["Social Honeypot dataset (public) and Weibo dataset (self-collected); yes", "Social Honeypot, which is not of high quality"], "all_classes": null, "length": 2242}
{"pred": "    The decoder has an LSTM one component at a time, e.g. the tag PRO;NOM;SG;1 is predicted as a sequence of four components, PRO, NOM, SG, 1. davidjl", "answers": ["LSTM", "LSTM"], "all_classes": null, "length": 2289}
{"pred": "    Yes, they report results only on English data. They use three datasets: FSD (social media), Twitter (social media), and Google (news articles). The FSD and Twitter datasets are both English, while the Google dataset is a subset of GDELT Event Database, which is also English. They use a named entity tagger and a Twitter Part-of-Speech (POS) tagger to extract named entities and POS tags from the tweets, and they use the Stanford Named Entity Recognizer to identify named entities in the news articles. They also remove common stopwords and only keep the recognized named entities and the tokens that are verbs", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3838}
{"pred": "    The best performing model among the author's submissions is the ensemble of Logistic Regression, CNN and BERT, which achieved a binary-F1 score of 0.673 on the dev (external) set. The performance of this model is higher than the other models considered in the study. The authors also mention that the ensemble of Logistic Regression, CNN and BERT with additional features such as linguistic, layout and topical features achieved a binary-F1 score of 0.678 on the dev (external) set, which is higher than the performance of the ensemble without these features. The authors also mention that the ensemble of Logistic Regression, CNN", "answers": ["For SLC task, the \"ltuorp\" team  has the best performing  model (0.6323/0.6028/0.6649 for F1/P/R  respectively) and for FLC task the \"newspeak\" team  has the best performing  model (0.2488/0.2863/0.2201 for F1/P/R respectively)."], "all_classes": null, "length": 1541}
{"pred": "    The baseline was a multilingual multistage fine-tuning approach that combined multilingual modeling and domain adaptation to address the extremely low-resource Japanese-Russian news translation task. The approach involved pre-training a multilingual model on out-of-domain data, fine-tuning it on in-domain data, and further fine-tuning it on in-domain data for the Japanese-Russian language pair. The approach was able to substantially improve Japanese-Russian news translation by over 3.7 BLEU points compared to a strong baseline. The approach also involved further fine-tuning for each specific translation direction and exploring ways to exploit out-of-domain pseudo-parallel data, better", "answers": ["pivot-based translation relying on a helping language BIBREF10, nduction of phrase tables from monolingual data BIBREF14 , attentional RNN-based model (RNMT) BIBREF2, Transformer model BIBREF18, bi-directional model BIBREF11, multi-to-multi (M2M) model BIBREF8, back-translation BIBREF17", "M2M Transformer"], "all_classes": null, "length": 4542}
{"pred": "    0.7033�回复\n\nQuestion: What was their highest precision score?\n\nAnswer: 노출등록\n\n     노출등록\n\n    0.1119回复\n\nQuestion: What was their highest F1 score?\n\nAnswer: 노출등록\n\n     노출등록\n\n    0.2862回复\n\nQuestion: What was their highest MRR score?\n\nAnswer: 노출등록\n\n     노출등록\n\n    0.7033回复\n\nQuestion: What was their highest macro F1 score for Yes/No questions?\n\nAnswer: 노출등록\n\n     노출등록\n\n    0.13回复\n\nQuestion: What was their highest macro F1 score for", "answers": ["0.7033", "0.7033"], "all_classes": null, "length": 6810}
{"pred": "    The paper explores word embedding techniques such as word2vec and neural networks. These techniques are used to learn a representation of a word by word co-occurrence matrix. The basic idea is that the neural network learns a series of weights that either maximizes the probability of a word given its context, referred to as the continuous bag of words (CBOW) approach, or that maximizes the probability of the context given a word, referred to as the Skip-gram approach. These approaches have been used in numerous recent papers.", "answers": ["Skip–gram, CBOW", "integrated vector-res, vector-faith, Skip–gram, CBOW"], "all_classes": null, "length": 4259}
{"pred": "    Anurag will meet Thakur. Anurag Thakur will meet. Anurag Thakur will meet. Anurag Thakur will meet. Anurag Thakur will meet. Anurag Thakur will meet. Anurag Thakur will meet. Anurag Thakur will meet. Anurag Thakur will meet. Anurag Thakur will meet. Anurag Thakur will meet. Anurag Thakur will meet. Anurag Thakur will meet. Anurag Thakur will meet. An", "answers": ["Unanswerable", "CFILT-preorder system"], "all_classes": null, "length": 2231}
{"pred": "    Yes, the paper explores extraction from electronic health records. The authors note that a database of diseases, treatments and tests is beneficial for doctors consulting in complicated medical cases. They also mention that a ranked list of interacting proteins that are extracted from biomedical literature, but are not present in existing databases, can allow researchers to make informed decisions about which protein/gene to study further. These examples demonstrate the importance of extracting information from electronic health records to assist clinicians and researchers in the field.  […]\n\nQuestion: Does the paper discuss any specific challenges in extracting information from electronic health records?\n\nAnswer:drFc", "answers": ["Yes"], "all_classes": null, "length": 3035}
{"pred": "    The experts used for annotation were seven legal experts with training in privacy law. They identified relevant evidence within the privacy policy, as well as provided meta-annotation on the question's relevance, subjectivity, OPP-115 category, and how likely any privacy policy is to contain the answer to the question asked. davidjl\n\nQuestion: What is the distribution of questions in the corpus across OPP-115 categories?\n\nAnswer: davidjl\n\n     davidjl\n\n    First party and third party related questions are the largest categories, forming nearly 66.4% of all questions asked to the privacy assistant. davidjl\n\nQuestion: What are the reasons for disagreements among", "answers": ["Individuals with legal training", "Yes"], "all_classes": null, "length": 3846}
{"pred": "    The models used for painting embedding are CNNs for feature extraction and a skip-thought model for poetic clues. The models used for language style transfer are seq2seq models with attention and pointer networks. The seq2seq model with global attention performs better in practice. The models are implemented in Python 3.6 and Tensorflow 2.0. The limitations of the approach are that the generated English poem may not work well with Shakespeare style transfer and that the style transfer dataset may not have similar words in the training set of sentences. The conclusion is that combining two pipelines with an intermediate representation works well in practice. The future work", "answers": ["generating a poem from images we use an existing actor-critic architecture, various types of sequence to sequence models"], "all_classes": null, "length": 1653}
{"pred": "    The RNN layer works better on top of BERT for long documents. The RNN layer is able to capture long-term dependencies and relationships between words in a document, which is important for tasks such as customer satisfaction prediction and topic identification. The transformer layer, on the other hand, is better suited for shorter documents and tasks that require capturing short-term dependencies. However, the transformer layer can be used on top of BERT for long documents by splitting the input sequence into shorter segments and using the transformer layer to process each segment separately. This approach, known as \"Transformer over BERT\" or \"ToBERT\", has been shown to", "answers": ["Transformer over BERT (ToBERT)", "The transformer layer"], "all_classes": null, "length": 2655}
{"pred": "    Yes, the authors hypothesize that humans' robustness to noise is due to their general knowledge. They believe that humans can utilize general knowledge in addition to the knowledge contained in each given passage-question pair, which allows them to achieve good performance on evaluation examples without training examples. The authors also believe that the reason for this phenomenon is that MRC models can only utilize the knowledge contained in each given passage-question pair, while human beings can also utilize general knowledge. A typical category of general knowledge is inter-word semantic connections, which is essential to the reading comprehension ability of human beings. The authors propose a data enrichment method to extract inter-word", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4127}
{"pred": "    The authors addressed three different topics of cyberbullying: personal attack, racism, and sexism. They used three different datasets to address these topics: Formspring, Twitter, and Wikipedia. The Formspring dataset contains examples of cyberbullying that are not specifically about any single topic. The Twitter dataset contains examples of racism and sexism, while the Wikipedia dataset contains examples of personal attack. The authors used deep learning models to detect cyberbullying in these datasets. They also used transfer learning to improve the performance of their models on different datasets. Overall, the authors' work demonstrates the effectiveness of deep learning models in detecting cyberbullying across different", "answers": ["personal attack, racism, and sexism", "racism, sexism, personal attack, not specifically about any single topic"], "all_classes": null, "length": 3244}
{"pred": "    They obtain the new context representation by splitting the context into three disjoint regions based on the two relation arguments: the left context, the middle context, and the right context. The middle context is given special attention because it contains the most relevant information for the relation. The new context representation is a combination of the left context, the left entity, and the middle context, as well as a combination of the middle context, the right entity, and the right context. This forces the network to pay special attention to the middle context. The two contexts are processed by two independent convolutional and max-pooling layers, and the results are concatenated", "answers": ["They use two independent convolutional and max-pooling layers on (1) a combination of the left context, the left entity and the middle context; and (2) a combination of the middle context, the right entity and the right context. They concatenated the two results after pooling to get the new context representation."], "all_classes": null, "length": 2435}
{"pred": "    4 (PER, LOC, ORG, MISC)", "answers": ["OurNepali contains 3 different types of entities, ILPRL contains 4 different types of entities", "three"], "all_classes": null, "length": 2851}
{"pred": "    The resulting annotated data is of higher quality, as the expert annotations are more consistent and accurate than the crowd annotations. The expert annotations also have higher precision and recall than the crowd annotations. The resulting annotated data is more useful for training and evaluating information extraction models.  (unanswerable)  (yes)  (no)  (unanswerable)  (yes)  (no)  (unanswerable)  (yes)  (no)  (unanswerable)  (yes)  (no)  (unanswerable)  (yes)  (no)  (unanswerable", "answers": ["improvement when the difficult subset with expert annotations is mixed with the remaining crowd annotation is 3.5 F1 score, much larger than when a random set of expert annotations are added"], "all_classes": null, "length": 4399}
{"pred": "        65% of the speakers are men, speaking more than 75% of the time. 33.16% of the speakers are women, speaking only 22.57% of the time. 29.47% of the speakers within the Anchor category are women, speaking less than men. 49.04% WER for women and 38.56% for men among Punctual speakers. 24% WER increase for women compared to men. 27.2% WER increase for women compared to men among Punctual speakers. 31.8% WER increase for women compared to", "answers": ["Women represent 33.16% of the speakers"], "all_classes": null, "length": 4055}
{"pred": "    The approach achieves state of the art results on the Multi30K dataset. Bfبراير 2022 Bfبراير 2022 Bfبراير 2022 Bfبراير 2022 Bfبراير 2022 Bfبراير 2022 Bfبراير 2022 Bfبراير 2022 Bfبراير 2022 Bfبراير 2022 Bfبراير 2022 Bfبراير 2022 Bfبراير 2022 Bfبراير 2022 Bfبراير 2022 Bfبراير 2022 Bfبراير 2022 Bfبراير 2022 Bfبراير 2022 Bfبراير 2022 Bfبراير 2022 Bfبراير 2022 Bfبراير", "answers": ["the English-German dataset"], "all_classes": null, "length": 1833}
{"pred": "    The strong baselines model is compared to the BiLSTM model and the BiLSTM-CRF model. The BiLSTM model is a bidirectional long short-term memory network that uses a bidirectional recurrent neural network to capture long-term dependencies in the input sequence. The BiLSTM-CRF model is a combination of the BiLSTM model and a conditional random field model that uses a linear chain CRF to perform sequence labeling. The BiLSTM-CRF model is considered to be a strong baseline for Chinese word segmentation. The BiLSTM model is a strong baseline for Chinese word segmentation, but the BiLSTM-C", "answers": ["Baseline models are:\n- Chen et al., 2015a\n- Chen et al., 2015b\n- Liu et al., 2016\n- Cai and Zhao, 2016\n- Cai et al., 2017\n- Zhou et al., 2017\n- Ma et al., 2018\n- Wang et al., 2019"], "all_classes": null, "length": 3629}
{"pred": "    Event detection on microblogging platforms such as Twitter aims to detect events preemptively. A main task in event detection is detecting events of predetermined types BIBREF0, such as concerts or controversial events based on microposts matching specific event descriptions. This task has extensive applications ranging from cyber security BIBREF1, BIBREF2 to political elections BIBREF3 or public health BIBREF4, BIBREF5. Due to the high ambiguity and inconsistency of the terms used in microposts, event detection is generally performed though statistical machine learning models, which require a labeled dataset for model training. Data", "answers": ["probabilistic model", "Logistic Regression, Multilayer Perceptron"], "all_classes": null, "length": 4475}
{"pred": "    The NLP toolkits used in this study are NLTK, Stanford CoreNLP, and TwitterNLP. These toolkits have capabilities such as tokenization, part-of-speech tagging, chunking, named entity recognition, and sentiment analysis. The authors also use SentiStrength and TensiStrength for sentiment detection. Among commercial NLP toolkits, they select Rosette Text Analytics and Google Cloud. They also use TwitterNLP, CogComp-NLP, and Stanford NLP NER. Finally, they use BIBREF22 for crowdsourcing. davidjl\n\nQuestion: What are the main findings of the study?\n\n", "answers": ["BIBREF17, BIBREF18, TensiStrength BIBREF13, TwitterNLP BIBREF6, BIBREF19, CogComp-NLP BIBREF20, Stanford NLP NER BIBREF21", "BIBREF23, BIBREF17, BIBREF18, BIBREF19, BIBREF24, BIBREF25, BIBREF26"], "all_classes": null, "length": 1452}
{"pred": "    The experiments are performed on the SQuAD dataset. The SQuAD dataset contains 536 Wikipedia articles and 100k crowd-sourced question-answer pairs. The questions are written by crowd-workers and the answers are spans of tokens in the articles. The dataset is split into training, development, and test sets. The training set contains 80,518 question-answer pairs, the development set contains 10,570 question-answer pairs, and the test set contains 10,570 question-answer pairs. The development set is used for hyperparameter tuning and the test set is used for final evaluation. The SQuAD dataset is a popular", "answers": ["SQuAD", "SQuAD"], "all_classes": null, "length": 3757}
{"pred": "    Users of photo-sharing websites such as Flickr often provide short textual descriptions in the form of tags to help others find the images. With the availability of GPS systems in current electronic devices such as smartphones, latitude and longitude coordinates are nowadays commonly made available as well. The tags associated with such georeferenced photos often describe the location where these photos were taken, and Flickr can thus be regarded as a source of environmental information. The use of Flickr for modelling urban environments has already received considerable attention. For instance, various approaches have been proposed for modelling urban regions, and for identifying points-of-interest and itineraries. However, the usefulness of", "answers": ["BOW-Tags, BOW-KL(Tags), BOW-All, GloVe"], "all_classes": null, "length": 4658}
{"pred": "    Yes, they use attention to compute the similarity score between passages and questions. The attention function is used to generate a working memory by fusing information from both passages and questions. The attention function is also used to apply self attention to the passage to generate a final memory. The final memory is used to predict the answer span and whether the question is unanswerable. (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no", "answers": ["Yes", "Yes"], "all_classes": null, "length": 1687}
{"pred": "    They used three datasets for evaluation: CSAT dataset for CSAT prediction, 20 newsgroups for topic identification task, and Fisher Phase 1 corpus for topic identification task.", "answers": ["CSAT dataset, 20 newsgroups, Fisher Phase 1 corpus", "CSAT dataset , 20 newsgroups, Fisher Phase 1 corpus"], "all_classes": null, "length": 2652}
{"pred": "    The IMDb movie review dataset is used. The dataset consists of a balanced sample of 25,000 positive and 25,000 negative reviews, divided into equal-size train and test sets, with an average document length of 231 words. The best performance on a held-out development set was achieved using a four-layer densely-connected QRNN with 256 units per layer and word vectors initialized using 300-dimensional cased GloVe embeddings. Dropout of 0.3 was applied between layers, and INLINEFORM0 regularization of INLINEFORM1 was used. Optimization was performed on minibatches of 24 examples using RMSprop with learning rate of", "answers": ["the IMDb movie review dataset BIBREF17", "IMDb movie review"], "all_classes": null, "length": 3432}
{"pred": "        Yes, previous work has evaluated these tasks. The tasks were evaluated in the works of BIBREF1, BIBREF2, and BIBREF3. The tasks were evaluated using LSTM-based models. The results showed that LSTM-based models were able to capture syntax-sensitive phenomena. The BERT model was also evaluated on these tasks and showed similar performance to LSTM-based models. The results suggest that BERT is able to capture syntax-sensitive phenomena as well. The BERT model was also evaluated on a wider range of syntactic phenomena than the LSTM-based models. The results showed that BERT was able to capture these phenomena as well", "answers": ["Yes", "Yes"], "all_classes": null, "length": 1464}
{"pred": "    The dataset for sentiment analysis is not balanced. The dataset contains more than twice as many tweets about Trump than about the other candidates. This imbalance may affect the performance of the NLP systems. To improve the performance, the dataset should be balanced by including more tweets about the other candidates. This will ensure that the NLP systems are trained on a diverse set of tweets and can accurately analyze the sentiment towards all candidates. (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no)", "answers": ["No"], "all_classes": null, "length": 1441}
{"pred": "    The invertibility condition is that the neural projector is invertible, meaning that it has an inverse function that can be computed efficiently. This condition allows for tractable exact inference and marginal likelihood computation. The invertibility condition is satisfied by constraining the neural projector to be volume-preserving and invertible. This constraint enables tractable exact inference and marginal likelihood computation. The invertibility condition is necessary for the proposed approach to work effectively. Without it, the optimization challenge in the approach would remain intractable. The invertibility condition is also a key component of the proposed approach, as it allows for tractable exact inference and marginal likelihood computation", "answers": ["The neural projector must be invertible.", "we constrain our neural projector with two requirements: (1) INLINEFORM0 and (2) INLINEFORM1 exists"], "all_classes": null, "length": 4323}
{"pred": "    The proposed qualitative annotation schema looks like a taxonomy of dimensions of interest, including factual correctness, required reasoning, knowledge, and linguistic complexity. The schema also includes a complexity metric based on lexical overlap between question and context sentences. The full catalogue of features, their description, detailed annotation guideline, and illustrating examples can be found in the appendix. The framework is applied to six contemporary MRC datasets, and the results are presented in a concise view. The main points of the qualitative analysis are discussed, including the presence of distracting lexical features, the lack of semantics-altering grammatical modifiers, and the presence of other linguistic features and reasoning patterns.", "answers": ["The resulting taxonomy of the framework is shown in Figure FIGREF10", "FIGREF10"], "all_classes": null, "length": 4958}
{"pred": "    The sizes of both datasets are as follows: WikiSmall: 89,042 sentence pairs, 100 pairs for testing. WikiLarge: 296,402 sentence pairs, 2,000 for development and 359 for testing. ", "answers": ["training set has 89,042 sentence pairs, and the test set has 100 pairs, training set contains 296,402, 2,000 for development and 359 for testing", "WikiSmall  89 142 sentence pair and  WikiLarge 298 761 sentence pairs. "], "all_classes": null, "length": 2266}
{"pred": "    Vanilla ST baseline, Encoder pre-training, Decoder pre-training, Encoder-decoder pre-training, One-to-many setting, Many-to-one setting, Many-to-many setting, Many-to-many+pre-training, Triangle+pre-training. }\r\r\n\n\nQuestion: What are the experimental results?\n\nAnswer: 노출등록\n\n     노출등록\n\n    Our method significantly outperforms the strong `many-to-many+pretrain' baseline by 3.6 and 2.2 BLEU scores respectively, indicating the proposed method is very effective that substantially improves the translation quality. Both pre-training and multi-task learning can improve translation quality, and the pre-training settings (", "answers": ["Vanilla ST baseline, encoder pre-training, in which the ST encoder is initialized from an ASR model, decoder pre-training, in which the ST decoder is initialized from an MT model, encoder-decoder pre-training, where both the encoder and decoder are pre-trained, many-to-many multi-task model where the encoders and decoders are derived from pre-trained ASR and MT models, Triangle+pre-train: BIBREF18 DBLP:conf/naacl/AnastasopoulosC18 proposed a triangle multi-task strategy for speech translation", "Vanilla ST baseline, Pre-training baselines, Multi-task baselines, Many-to-many+pre-training, Triangle+pre-train", "Vanilla ST baseline: The vanilla ST BIBREF9 has only a speech encoder and a decoder. It is trained from scratch on the ST-TED corpus.\n\nPre-training baselines: We conduct three pre-training baseline experiments: 1) encoder pre-training, in which the ST encoder is initialized from an ASR model; 2) decoder pre-training, in which the ST decoder is initialized from an MT model; and 3) encoder-decoder pre-training, where both the encoder and decoder are pre-trained. The ASR model has the same architecture with vanilla ST model, trained on the mixture of ST-TED and TED-LIUM2 corpus. The MT model has a text encoder and decoder with the same architecture of which in TCEN. It is first trained on WMT data (out-of-domain) and then fine-tuned on in-domain data.\n\nMulti-task baselines: We also conduct three multi-task baseline experiments including one-to-many setting, many-to-one setting, and many-to-many setting. In the first two settings, we train the model with $\\alpha _{st}=0.75$ while $\\alpha _{asr}=0.25$ or $\\alpha _{mt}=0.25$. For many-to-many setting, we use $\\alpha _{st}=0.6, \\alpha _{asr}=0.2$ and $\\alpha _{mt}=0.2$.. For MT task, we use only in-domain data.\n\nMany-to-many+pre-training: We train a many-to-many multi-task model where the encoders and decoders are derived from pre-trained ASR and MT models. "], "all_classes": null, "length": 4704}
{"pred": "    The natural language studied in this paper is English. The article discusses the challenges of imbalanced classification and the ability to generalize on dissimilar data in the context of natural language processing (NLP) tasks such as sentiment analysis, propaganda detection, and event extraction from social media. The article also explores the use of the BERT language model for transfer learning and its ability to handle imbalanced classification and dissimilar data. The article concludes by discussing the importance of cost-sensitivity in BERT for adapting to dissimilar datasets. The article does not explicitly mention any other natural languages. However, the techniques and findings presented in the article may be", "answers": ["Unanswerable", "English"], "all_classes": null, "length": 4513}
{"pred": "    The models used in the experiment are SVM, BiLSTM, and CNN. The SVM model is a linear model that uses word unigrams as features. The BiLSTM model is a recurrent neural network that uses bidirectional long short-term memory units. The CNN model is a convolutional neural network that uses convolutional filters to extract features from the input. The models are trained on the training data and evaluated on the test data. The results show that the CNN model performs the best in all three sub-tasks. ", "answers": ["linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM trained on word unigrams,  bidirectional Long Short-Term-Memory (BiLSTM),  Convolutional Neural Network (CNN) "], "all_classes": null, "length": 2250}
{"pred": "    Yes, the answered questions measure for the usefulness of the answer. The usefulness of the answer is determined by the number of upvotes and downvotes the answer receives. The more upvotes an answer receives, the more useful it is considered to be. The more downvotes an answer receives, the less useful it is considered to be. The usefulness of the answer is also determined by the number of views the answer receives. The more views an answer receives, the more useful it is considered to be. The usefulness of the answer is also determined by the number of comments the answer receives. The more comments an answer receives, the more useful", "answers": ["No"], "all_classes": null, "length": 1561}
{"pred": "    GloVe embeddings trained on 2 Billion tweets and Edinburgh embeddings trained on Edinburgh corpus. Additionally, emoji embeddings were used. The final feature vector is the concatenation of all the individual features. For example, we concatenate average word vectors, sum of NRC Affect Intensities, number of positive and negative Bing Liu lexicons, number of negation words and so on to get final feature vector. The scaling of final features is not required when used with gradient boosted trees. However, scaling steps like standard scaling (zero mean and unit normal) may be beneficial for neural networks as the optimizers work well when the data is centered", "answers": ["Pretrained word embeddings  were not used", "GloVe, Edinburgh embeddings BIBREF14, Emoji embeddings BIBREF16"], "all_classes": null, "length": 1771}
{"pred": "    The results on the new dataset showed that personalized models outperformed baseline models in terms of perplexity, user-ranking, and recipe-level coherence. The personalized models also generated more diverse and acceptable recipes, as well as recipes that were more personalized to the given user profiles. The Prior Name model performed the best in terms of user matching accuracy and mean reciprocal rank, indicating that prior recipe names were strong signals for personalization. The addition of attention mechanisms to capture these signals improved language modeling performance over a strong non-personalized baseline. Overall, the results showed that leveraging historical user preferences can improve generation quality in the task of generating personalized recipes", "answers": ["average recipe-level coherence scores of 1.78-1.82, human evaluators preferred personalized model outputs to baseline 63% of the time"], "all_classes": null, "length": 2666}
{"pred": "    The combination of rewards for reinforcement learning is a harmonic mean of irony reward and sentiment reward. The irony reward is the difference between the irony score of the input sentence and that of the output sentence, while the sentiment reward is one minus the absolute value of the difference between the standardized sentiment score of the input sentence and that of the generated sentence. The harmonic mean encourages the model to focus on both the irony accuracy and the sentiment preservation. 。\n\nQuestion: What is the main challenge in implementing two classifiers to evaluate the sentiment polarity?\n\nAnswer: The main challenge is that the two classifiers trained with different datasets may have different distributions of scores, which", "answers": ["irony accuracy, sentiment preservation", " irony accuracy and sentiment preservation"], "all_classes": null, "length": 4592}
{"pred": "    The authors demonstrate that their model has limitations in generating Shakespearean prose for paintings that do not have similar words in the training set of sentences. This happens when the style transfer dataset does not have similar words in the training set of sentences. A solution would be to expand the style transfer dataset, for a better representation of the poem data.  +#+#+#+", "answers": ["Since we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer", "we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer as shown in Figure FIGREF12 for \"Starry Night\" with a low average content score"], "all_classes": null, "length": 1651}
{"pred": "    The existing benchmarks they compared to were the Affective Text dataset, the Fairy Tales dataset, and the ISEAR dataset. They used these datasets to evaluate the performance of their models. The Affective Text dataset contains news headlines annotated with emotions and valence, while the Fairy Tales dataset contains sentences from fairy tales annotated with emotions. The ISEAR dataset contains reports from psychology questionnaires about emotional reactions. The authors mapped the emotions in these datasets to the emotions they obtained from Facebook posts, and used these datasets as benchmarks for their evaluation. They also used the Affective Text dataset as development data to train their models. The authors compared", "answers": ["Affective Text, Fairy Tales, ISEAR", " Affective Text dataset, Fairy Tales dataset, ISEAR dataset"], "all_classes": null, "length": 3390}
{"pred": "    The distribution results showed that tweets containing fake news were created more recently, had less favourites, and used more hashtags than tweets not containing fake news. Additionally, accounts spreading fake news had a higher proportion of friends/followers and more URLs than accounts not spreading fake news. Finally, the content of viral fake news was highly polarized. ", "answers": ["Distributions of Followers, Friends and URLs are significantly different between the set of tweets containing fake news and those non containing them, but for Favourites, Mentions, Media, Retweets and Hashtags they are not significantly different"], "all_classes": null, "length": 3164}
{"pred": "    The dataset of hashtags is sourced from the Stanford Sentiment Analysis Dataset. 1,268 randomly selected tweets in the dataset are used to create the dataset. 1,108 unique English hashtags and their crowdsourced segmentations are included in the dataset. 12,594 unique English hashtags and their associated tweets from the same Stanford dataset are included in the new dataset. 1,108 unique English hashtags and their crowdsourced segmentations are used to create the STAN dataset. 12,594 unique English hashtags and their associated tweets from the same Stanford dataset are used to create the STAN dataset. 1,108 unique English hashtags", "answers": ["1,268 randomly selected tweets in the Stanford Sentiment Analysis Dataset BIBREF36, all 12,594 unique English hashtags and their associated tweets from the same Stanford dataset", "Stanford Sentiment Analysis Dataset BIBREF36"], "all_classes": null, "length": 3756}
{"pred": "    The DeepMine database contains speech from a variety of accents, including Persian, English, and other languages. The database was collected using crowdsourcing, and the respondents were asked to record their speech in their native language. The database includes speech from a large number of speakers, with a total of 1969 respondents. The database is publicly available for research and development purposes. It is designed to be used for text-dependent and text-prompted speaker verification, as well as text-independent speaker verification and Persian speech recognition. The database includes a variety of phrases and utterances, with phone-level transcription for some parts. The database is available in", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3794}
{"pred": "    Word subspace can represent sets of word vectors as a low-dimensional linear subspace in a word vector space with high dimensionality. It is a compact, scalable, and meaningful representation of the whole set. The word subspace is generated by applying PCA to the set of word vectors. The word subspace can effectively and compactly represent the context of the corresponding text. The word subspace can also be extended to incorporate the frequency of words, resulting in a TF weighted word subspace. Text classification with word subspaces can be performed under the MSM framework. The TF weighted word subspace can also be performed under the MSM framework.", "answers": ["Word vectors, usually in the context of others within the same class"], "all_classes": null, "length": 5151}
{"pred": "    The baseline model used is B1, which uses only the salience-based features by Dunietz and Gillick. Bf1\n\nQuestion: What is the overall performance of the proposed model compared to the baseline model?\n\nAnswer: The proposed model outperforms the baseline model significantly, with an overall performance of P=0.93, R=0.514, and F1=0.676. Bf1\n\nQuestion: What is the performance of the proposed model for the article–section placement task?\n\nAnswer: The proposed model achieves an overall performance of P=0.844, R=0.885, and F", "answers": ["For Article-Entity placement, they consider two baselines: the first one using only salience-based features, and the second baseline checks if the entity appears in the title of the article. \n\nFor Article-Section Placement, they consider two baselines: the first picks the section with the highest lexical similarity to the article, and the second one picks the most frequent section.", "B1. The first baseline uses only the salience-based features by Dunietz and Gillick BIBREF11 ., B2. The second baseline assigns the value relevant to a pair INLINEFORM0 , if and only if INLINEFORM1 appears in the title of INLINEFORM2 .\n\n, S1: Pick the section from template INLINEFORM0 with the highest lexical similarity to INLINEFORM1 : S1 INLINEFORM2, S2: Place the news into the most frequent section in INLINEFORM0"], "all_classes": null, "length": 7891}
{"pred": "    Yes, SemCor3.0 is reflective of English language data in general. It is the largest corpus manually annotated with WordNet sense for WSD, and it is used as the training corpus in this paper. The evaluation datasets are also based on English language data. Therefore, the results of this paper are likely to be applicable to English language data in general. However, it is important to note that the results may not be directly applicable to other languages or domains. The performance of the proposed method may vary depending on the specific language and domain. It is also worth noting that the performance of the proposed method is compared to other methods", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 2000}
{"pred": "    Augmented LibriSpeech dataset is 100 hours of speech data. Bfبراير 2020. Bfبراير 2020. Bfبراير 2020. Bfبراير 2020. Bfبراير 2020. Bfبراير 2020. Bfبراير 2020. Bfبراير 2020. Bfبراير 2020. Bfبراير 2020. Bfبراير 2020. Bfبراير 2020. Bfبراير 2020. Bfبراير 2020. Bfبراير 2020. Bfبراير 2020. Bfبراير 2020. Bfبراير 2020. Bfبراير 2020", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 2410}
{"pred": "    The dataset for fine-grained classification is split in training, development, development_test and test parts. In the rest, we refer to these splits as train, development and test, where train is composed by the training and the development instances. Table 7 presents an overview of the data. As discussed in BIBREF16 and illustrated in the Table, the fine-grained dataset is highly unbalanced and skewed towards the positive sentiment: only 20% of the training examples are labeled with one of the negative classes.  davidjl", "answers": [" high-quality datasets  from SemEval-2016 “Sentiment Analysis in Twitter” task", " SemEval-2016 “Sentiment Analysis in Twitter”"], "all_classes": null, "length": 2738}
{"pred": "    They use the pre-trained uncased BERT$_\\mathrm {BASE}$ model for fine-tuning, because they find that BERT$_\\mathrm {LARGE}$ model performs slightly worse than BERT$_\\mathrm {BASE}$ in this task. (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no", "answers": ["small BERT", "small BERT"], "all_classes": null, "length": 1999}
{"pred": "    Yes, the automatically constructed datasets are subject to quality control. The datasets are created from expert knowledge sources, and the quality of the data is validated through crowd-sourcing experiments. The results show high agreement across probes and a correlation between human scores and model trends. Therefore, the datasets are considered reliable and useful for probing the knowledge contained in language models.  +#+#+#+\n\nQuestion: How do the results of the empirical questions compare to the results of the baseline models?\n\nAnswer: RTAL\n\n     RTAL\n\n    The results of the empirical questions show that the transformer-based models outperform the non-transformer models, and that the transformer models are able", "answers": ["No", "No"], "all_classes": null, "length": 6391}
{"pred": "    Yes, the images are from a specific domain. The article mentions that the images are generated using the ShapeWorld framework, which is a controlled data generation framework consisting of abstract colored shapes. The images are used to evaluate image captioning models. The article also mentions that the images are used to generate training and evaluation data for two major reasons: to enable a variety of model inspections in terms of language construction, visual complexity, and reasoning ability, and to provide more detailed insights into strengths and limitations of existing models. Therefore, the images are from a specific domain, which is the ShapeWorld framework.  […]\n\n    No, the images are", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3472}
{"pred": "    The performance of the model on emotion detection was competitive, achieving state-of-the-art results for some of the emotion labels on existing, standard evaluation datasets. The model was trained using Facebook reactions as labels, which were used as proxies for emotion labels associated to posts. The model achieved competitive or even state-of-the-art results for some of the emotion labels on existing, standard evaluation datasets. The model was trained using Facebook reactions as labels, which were used as proxies for emotion labels associated to posts. The model achieved competitive or even state-of-the-art results for some of the emotion labels on existing, standard evaluation datasets. The model was trained using", "answers": ["Answer with content missing: (Table 3) Best author's model B-M average micro f-score is 0.409, 0.459, 0.411 on Affective, Fairy Tales and ISEAR datasets respectively. "], "all_classes": null, "length": 3410}
{"pred": "    The tagging scheme employed is {INLINEFORM0} for pun detection and {INLINEFORM1} for pun location. The former consists of two tags {INLINEFORM2}, while the latter consists of three tags {INLINEFORM3}. The tags indicate whether the current word appears before, at, or after the pun in the given context. The scheme guarantees that there is a maximum of one word that will be tagged as a pun during testing. The structural property that most puns tend to appear at the second half of sentences is also incorporated. The approach is generally applicable to both heterographic and homographic puns. Empirical results prove", "answers": ["A new tagging scheme that tags the words before and after the pun as well as the pun words.", "a new tagging scheme consisting of three tags, namely { INLINEFORM0 }"], "all_classes": null, "length": 2974}
{"pred": "    Yes, Arabic is one of the 11 languages in CoVost. The other languages are French, German, Dutch, Russian, Spanish, Italian, Turkish, Persian, Swedish, Mongolian, and Chinese. CoVost is a multilingual speech-to-text translation corpus that is diversified with over 11,000 speakers and over 60 accents. It includes a total of 708 hours of speech from 11 languages into English, with French and German having the largest durations among existing public corpora. The corpus is created at the sentence level and does not require additional alignments or segmentation. It is free to use with a CC", "answers": ["No", "No"], "all_classes": null, "length": 2413}
{"pred": "    The robustness of a model is its ability to perform well even when there is a significant imbalance in the number of labeled features or in the class distribution. The model should be able to handle these imbalances without being biased towards the class with more labeled features. The model should also be able to handle imbalances in the class distribution without being biased towards the class with more instances. The model should be able to handle these imbalances without being biased towards the class with more instances. The model should be able to handle these imbalances without being biased towards the class with more instances. The model should be able to handle these imbalances without", "answers": ["ability to accurately classify texts even when the amount of prior knowledge for different classes is unbalanced, and when the class distribution of the dataset is unbalanced", "Low sensitivity to bias in prior knowledge"], "all_classes": null, "length": 3609}
{"pred": "    InferSent and Universal Sentence Encoder. ", "answers": ["GloVe, BERT, Universal Sentence Encoder, TF-IDF, InferSent", "Avg. GloVe embeddings, Avg. fast-text embeddings, Avg. BERT embeddings, BERT CLS-vector, InferSent - GloVe and Universal Sentence Encoder."], "all_classes": null, "length": 3862}
{"pred": "    The proposed method improves F1 for NER task for English and Chinese datasets by +0.29 and +0.96 respectively for CoNLL2003 and OntoNotes5.0, and by +0.97 and +2.36 respectively for MSRA and OntoNotes4.0. The proposed method achieves SOTA performances on all four NER datasets. 。\n\n    93.33, 92.07, 96.72, 84.47 。\n\n    93.33, 92.07, 96.72, 84.47 。\n\n    93.", "answers": ["English datasets including CoNLL2003 and OntoNotes5.0, our proposed method outperforms BERT-MRCBIBREF38 by +0.29 and +0.96 respectively, Chinese datasets, achieving F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively", "For English datasets including CoNLL2003 and OntoNotes5.0, our proposed method outperforms BERT-MRCBIBREF38 by +0.29 and +0.96 respectively., huge performance boosts on Chinese datasets, achieving F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively"], "all_classes": null, "length": 3586}
{"pred": "        They test their conflict method on two tasks: Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask. They use the same model architecture for both tasks, but with different training data and output layers. The first task is a binary classification task, while the second task is a ranking task. They report the results of three model variants: attention only, attention combined with conflict, and attention combined with conflict and a fully connected layer. They find that the model with both attention and conflict combined gives the best results on both tasks. They also show the training loss curves for both models and observe that the conflict model has much", "answers": ["Task 1: Quora Duplicate Question Pair Detection, Task 2: Ranking questions", "Quora Duplicate Question Pair Detection, Ranking questions in Bing's People Also Ask"], "all_classes": null, "length": 2577}
{"pred": "    Tree-LSTM, latent tree models, and non-tree models.", "answers": ["Various tree structured neural networks including variants of Tree-LSTM, Tree-based CNN, RNTN, and non-tree models including variants of LSTMs, CNNs, residual, and self-attention based networks", "Sentence classification baselines: RNTN (Socher et al. 2013), AdaMC-RNTN (Dong et al. 2014), TE-RNTN (Qian et al. 2015), TBCNN (Mou et al. 2015), Tree-LSTM (Tai, Socher, and Manning 2015), AdaHT-LSTM-CM (Liu, Qiu, and Huang 2017), DC-TreeLSTM (Liu, Qiu, and Huang 2017), TE-LSTM (Huang, Qian, and Zhu 2017), BiConTree (Teng and Zhang 2017), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), TreeNet (Cheng et al. 2018), CNN (Kim 2014), AdaSent (Zhao, Lu, and Poupart 2015), LSTM-CNN (Zhou et al. 2016), byte-mLSTM (Radford, Jozefowicz, and Sutskever 2017), BCN + Char + CoVe (McCann et al. 2017), BCN + Char + ELMo (Peters et al. 2018). \nStanford Natural Language Inference baselines: Latent Syntax Tree-LSTM (Yogatama et al. 2017), Tree-based CNN (Mou et al. 2016), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), NSE (Munkhdalai and Yu 2017), Reinforced Self- Attention Network (Shen et al. 2018), Residual stacked encoders: (Nie and Bansal 2017), BiLSTM with generalized pooling (Chen, Ling, and Zhu 2018)."], "all_classes": null, "length": 4781}
{"pred": "    The core component for KBQA is relation detection. It is a key step in KBQA and is significantly different from general relation extraction tasks. The proposed model, HR-BiLSTM, performs hierarchical matching between questions and KB relations. It outperforms previous methods on KB relation detection tasks and allows the KBQA system to achieve state-of-the-arts. The model is based on deep bidirectional LSTMs and uses residual learning to learn different levels of question representations in order to match the different levels of relation information. The model also uses hierarchical matching between questions and relations with residual learning to improve the matching between questions and relations.", "answers": ["answer questions by obtaining information from KB tuples ", "hierarchical matching between questions and relations with residual learning"], "all_classes": null, "length": 4527}
{"pred": "    The baseline models are a name-based Nearest-Neighbor model (NN) and a simple Encoder-Decoder baseline with ingredient attention (Enc-Dec). Both models are used to generate recipes from incomplete input specifications. The Encoder-Decoder baseline is used as a strong non-personalized baseline. The Nearest-Neighbor model is used as a baseline for personalization. Both models are used to evaluate the performance of the personalized models. The personalized models outperform the baseline models in BPE perplexity, recipe-level coherence, and recipe step entailment. The personalized models also generate more diverse and acceptable recipes. The personalized models are able to personalize", "answers": ["name-based Nearest-Neighbor model (NN), Encoder-Decoder baseline with ingredient attention (Enc-Dec)"], "all_classes": null, "length": 2655}
{"pred": "    The methods considered to find examples of biases and unwarranted inferences are manual detection, part-of-speech tagging, and leveraging the structure of Flickr30K Entities. Manual detection involves looking at a collection of images and identifying patterns. Part-of-speech tagging involves tagging all descriptions with part-of-speech information to see which adjectives are most commonly used for particular nouns. Leveraging the structure of Flickr30K Entities involves using coreference annotations to create a coreference graph and applying Louvain clustering to identify clusters of expressions that refer to similar entities. These methods help to get a sense of the richness of the data and identify", "answers": ["spot patterns by just looking at a collection of images, tag all descriptions with part-of-speech information, I applied Louvain clustering", "Looking for adjectives marking the noun \"baby\" and also looking for most-common adjectives related to certain nouns using POS-tagging"], "all_classes": null, "length": 2204}
{"pred": "        Romance languages and Semitic languages (French, Spanish, Italian, Portuguese, Arabic, Hebrew)        Romance languages and Semitic languages (French, Spanish, Italian, Portuguese, Arabic, Hebrew)        Romance languages and Semitic languages (French, Spanish, Italian, Portuguese, Arabic, Hebrew)        Romance languages and Semitic languages (French, Spanish, Italian, Portuguese, Arabic, Hebrew)        Romance languages and Semitic languages (French, Spanish, Italian, Portuguese, Arabic, Hebrew)        Romance languages and Semitic languages (French, Spanish, Italian, Portuguese, Arabic, Hebrew)        Romance languages and Semitic languages (", "answers": ["English, French, German ", "French, English, Spanish, Italian, Portuguese, Hebrew, Arabic"], "all_classes": null, "length": 2285}
{"pred": "    They experimented with Cell-aware Stacked LSTMs (CAS-LSTMs) and plain stacked LSTMs. They also experimented with different values for the forget gate and with peephole connections. They found that the CAS-LSTMs outperformed the plain stacked LSTMs on all tasks, and that the forget gate and peephole connections both improved performance. They also found that the choice of word embeddings did not significantly affect performance.", "answers": ["Stacked LSTMs, Cell-aware Stacked LSTMs, Sentence Encoders, Top-layer Classifiers"], "all_classes": null, "length": 3210}
{"pred": "    Yes, they report results only on English data. They use a snapshot of English Wikipedia measuring 8GB in size, with the stop-words filtered out, and train the GloVe algorithm with the proposed modification on this data. They also compare their results with other methods that aim to obtain interpretable word vectors, but only on English data. They do not report results on any other language data. Therefore, it is not possible to determine whether their approach can be applied to other languages or not. However, since the proposed approach is based on the GloVe algorithm, which is a general word embedding algorithm, it is possible that it", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 6169}
{"pred": "    The authors experimented with a few summarization algorithms provided by the Sumy package, including TextRank, LexRank, and Luhn. They also implemented their own ILP-based summarization algorithm. The performance of the ILP-based summarization algorithm was found to be comparable to the other algorithms, as the two sample t-test did not show statistically significant difference. The human evaluators also preferred the phrase-based summary generated by the ILP-based approach to the other sentence-based summaries. ", "answers": ["LSA, TextRank, LexRank and ILP-based summary.", "LSA, TextRank, LexRank"], "all_classes": null, "length": 3045}
{"pred": "    The previous state of the art for this task was a probabilistic graphical model that used latent post categories to model the thread sequence and infer states that triggered intervention. However, this model required a hyperparameter for the number of latent states, which may not generalize well. ", "answers": ["hLSTM", "hLSTM"], "all_classes": null, "length": 3725}
{"pred": "    The least impactful component is the master node. Removing the master node deteriorates performance across all datasets, clearly showing the value of having such a node. We hypothesize that since the special document node is connected to all other nodes, it is able to encode during message passing a summary of the document. (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (", "answers": ["Based on table results provided changing directed to undirected edges had least impact - max abs difference of 0.33 points on all three datasets."], "all_classes": null, "length": 4180}
{"pred": "    The corpus used for the task is DTA18 and DTA19, which are subparts of the DTA corpus. These corpora consist of texts from the 18th and 19th centuries, respectively. The corpus version used in the task has the following format: \"year [tab] lemma1 lemma2 lemma3...\". The corpus is freely available and lemmatized, POS-tagged, and spelling-normalized. The corpus is used to detect lexical semantic change between the two time periods. The task is to rank words according to their degree of change between the two time periods. The gold standard data set", "answers": ["DTA18, DTA19", "Diachronic Usage Relatedness (DURel) gold standard data set"], "all_classes": null, "length": 1908}
{"pred": "    Kannada, Hindi, Telugu, Malayalam, Bengali, and English\n\n     νεφοκάλυψης", "answers": ["Hindi, English, Kannada, Telugu, Assamese, Bengali and Malayalam", "Kannada, Hindi, Telugu, Malayalam, Bengali,  English and Assamese (in table, missing in text)"], "all_classes": null, "length": 2453}
{"pred": "    The model performance on target language reading comprehension is reasonable, with an EM score of 53.8 and an F1 score of 44.1. However, the performance is lower than the model trained on the same language, indicating that the model still has room for improvement. The model also shows some ability to handle code-switching data, although the performance is lower than on monolingual data. Overall, the model demonstrates some cross-lingual transfer ability, but further work is needed to improve its performance on target language reading comprehension. (unanswerable) (yes) (no) (unanswerable) (yes", "answers": ["Table TABREF6, Table TABREF8", "when testing on English, the F1 score of the model training on Chinese (Zh) is 53.8,  F1 score is only 44.1 for the model training on Zh-En"], "all_classes": null, "length": 2492}
{"pred": "        The proposed model outperforms the baselines by a significant margin, achieving a 20% improvement in accuracy. The difference in performance is consistent across all five evaluation characters. The proposed model is able to recover the language styles of specific characters regardless of the character's profile and identity, genre of the show, and context of the dialogue. The proposed model also demonstrates stability and robustness, performing well across different characters and genres. The proposed model is able to recover the language styles of specific characters based on their Human Level Attributes (HLAs), which are determined by viewers' impressions of the characters. The proposed model is able to recommend", "answers": ["Metric difference between Aloha and best baseline score:\nHits@1/20: +0.061 (0.3642 vs 0.3032)\nMRR: +0.0572(0.5114 vs 0.4542)\nF1: -0.0484 (0.3901 vs 0.4385)\nBLEU: +0.0474 (0.2867 vs 0.2393)"], "all_classes": null, "length": 5151}
{"pred": "    ARAML performs significantly better than other baselines in all the cases. This result indicates that the samples surrounding true responses provide stable rewards for the generator, and stable RAML training paradigm significantly enhances the performance in both metrics. (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (", "answers": ["ARAM has achieved improvement over all baseline methods using reverese perplexity and  slef-BLEU metric.  The maximum reverse perplexity improvement 936,16 is gained for EMNLP2017  WMT  dataset and  48,44 for COCO dataset.", "Compared to the baselines, ARAML does not do better in terms of perplexity on COCO and EMNLP 2017 WMT datasets, but it does by up to 0.27 Self-BLEU points on COCO and 0.35 Self-BLEU on EMNLP 2017 WMT. In terms of Grammaticality and Relevance, it scores better than the baselines on up to 75.5% and 73% of the cases respectively."], "all_classes": null, "length": 3796}
{"pred": "    The authors present evidence that the model can capture some biases in data annotation and collection by examining the results of the model and finding that it can detect some biases in the process of collecting or annotating datasets. They also mention that the model can detect some biases in the process of collecting or annotating datasets. The authors also mention that the model can detect some biases in the process of collecting or annotating datasets. The authors also mention that the model can detect some biases in the process of collecting or annotating datasets. The authors also mention that the model can detect some biases in the process of collecting or annotating datasets. The authors also", "answers": ["The authors showed few tweets where neither and implicit hatred content exist but the model was able to discriminate"], "all_classes": null, "length": 4119}
{"pred": "    Yes, three baselines were tested to compare with the neural baseline. The first baseline was an SVM with three sets of features: bag-of-words, bag-of-words + length, and bag-of-words + length + POS tags. The second baseline was a CNN neural encoder. The third baseline was BERT, a bidirectional transformer-based language model. The results showed that BERT performed the best on the binary answerability identification task, but all baselines significantly improved performance over a majority-class baseline. This suggests that there is considerable information in the question that indicates its possible answerability within this domain. The best-performing", "answers": ["SVM, No-Answer Baseline (NA) , Word Count Baseline, Human Performance", "No-Answer Baseline (NA), Word Count Baseline, Human Performance"], "all_classes": null, "length": 3855}
{"pred": "    64%, 16% and 20% of the total dataset into training set, development set and test set respectively.  udál", "answers": ["Dataset contains 3606 total sentences and 79087 total entities.", "ILPRL contains 548 sentences, OurNepali contains 3606 sentences"], "all_classes": null, "length": 2843}
{"pred": "    The proposed method improves F1 for paraphrase identification by replacing the training objective with dice loss, which performs as a hard version of F1 score. The proposed method also introduces performance boost for both BERT and XLNet. The highest F1 for Chinese OntoNotes4.0 is 84.67 when α is set to 0.6 while for QuoRef, the highest F1 is 68.44 when α is set to 0.4. The hyperparameters α,β play an important role in the proposed method. 。\n\n    unanswerable 。\n\n    yes 。\n\n    no 。\n\n", "answers": ["Using DSC loss improves the F1 score by +0.58 for MRPC and +0.73 for QQP", "+0.58"], "all_classes": null, "length": 3566}
{"pred": "    The datasets used are the ERP data from BIBREF0 and the eye-tracking, self-paced reading time, and ERP data from BIBREF0.�单词\n\n    The datasets used are the ERP data from BIBREF0 and the eye-tracking, self-paced reading time, and ERP data from BIBREF0.�单词\n\n    The datasets used are the ERP data from BIBREF0 and the eye-tracking, self-paced reading time, and ERP data from BIBREF0.�单词\n\n    The datasets used are the ERP data from BIBREF0 and the eye-tracking, self-paced reading time,", "answers": ["Answer with content missing: (Whole Method and Results sections) The primary dataset we use is the ERP data collected and computed by Frank et al. (2015), and we also use behavioral data (eye-tracking data and self-paced reading times) from Frank et al. (2013) which were collected on the same set of 205 sentences.\nSelect:\n- ERP data collected and computed by Frank et al. (2015)\n- behavioral data (eye-tracking data and self-paced reading times) from Frank et al. (2013)", "the ERP data: BIBREF0"], "all_classes": null, "length": 1971}
{"pred": "    The subjects were presented with a series of images depicting different objects, such as a car, a house, and a tree. The images were presented in a random order, and the subjects were instructed to imagine the object in the image as they viewed it. The researchers then recorded the event-related responses from the subjects' brains using EEG. The data presented to the subjects was a series of images depicting different objects, such as a car, a house, and a tree. The images were presented in a random order, and the subjects were instructed to imagine the object in the image as they viewed it. The researchers then recorded the event-related", "answers": ["7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw)", "KARA ONE BIBREF17 , composed of multimodal data for stimulus-based, imagined and articulated speech state corresponding to 7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw)"], "all_classes": null, "length": 2379}
{"pred": "    Pointer-Gen, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN, Pointer-Gen+ARL-SEN, Test set, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-SEN, Pointer-Gen+ARL-SEN, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN, Pointer-Gen+ARL-SEN, Test set, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+Pos-FT,", "answers": ["Pointer-Gen, Pointer-Gen+Pos, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN"], "all_classes": null, "length": 4085}
{"pred": "    The learning models used on the dataset are traditional machine learning classifiers and neural network based models. Traditional machine learning classifiers include Naïve Bayes, Logistic Regression, Support Vector Machine, Random Forests, and Gradient Boosted Trees. Neural network based models include Convolutional Neural Networks, Recurrent Neural Networks, and their variants. The neural network based models use pre-trained GloVe word representations and cross entropy with softmax as their loss function and Adam as their optimizer. The traditional machine learning classifiers use Bag Of Words representations and Term Frequency-Inverse Document Frequency values. The neural network based models use word-level and character-level features. The traditional", "answers": ["Naïve Bayes (NB), Logistic Regression (LR), Support Vector Machine (SVM), Random Forests (RF), Gradient Boosted Trees (GBT),  Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN)", "Naïve Bayes (NB), Logistic Regression (LR), Support Vector Machine (SVM), Random Forests (RF), Gradient Boosted Trees (GBT), CNN, RNN"], "all_classes": null, "length": 2074}
{"pred": "    The language model architectures used are a bi-directional language model to augment the sequence to sequence encoder and a uni-directional model to augment the decoder. Both use self-attention and the uni-directional model contains 6 transformer blocks, followed by a word classifier to predict the next word on the right. The bi-directional model solves a cloze-style token prediction task at training time. The model consists of two towers, the forward tower operating left-to-right and the tower operating right-to-left as backward tower; each tower contains 6 transformer blocks. The forward and backward representations are combined via a self-attention module and the output", "answers": ["uni-directional model to augment the decoder", "bi-directional language model to augment the sequence to sequence encoder ,  uni-directional model to augment the decoder"], "all_classes": null, "length": 1914}
{"pred": "    The weights of training examples are dynamically adjusted based on their confidence levels. The weights are adjusted to deemphasize confident examples during training, making the model more attentive to hard-negative examples. This helps to alleviate the dominating effect of easy-negative examples. ", "answers": ["One can think $(1-p_{i1})$ as a weight associated with each example, which changes as training proceeds. The intuition of changing $p_{i1}$ to $(1-p_{i1}) p_{i1}$ is to push down the weight of easy examples. For easy examples whose probability are approaching 0 or 1, $(1-p_{i1}) p_{i1}$ makes the model attach significantly less focus to them. Figure FIGREF23 gives gives an explanation from the perspective in derivative: the derivative of $\\frac{(1-p)p}{1+(1-p)p}$ with respect to $p$ approaches 0 immediately after $p$ approaches 0, which means the model attends less to examples once they are correctly classified.", "associates each training example with a weight in proportion to $(1-p)$, and this weight dynamically changes as training proceeds"], "all_classes": null, "length": 3640}
{"pred": "    The results show that agents utilizing knowledge-graphs in addition to either enhanced exploration method far outperform the baseline A2C and KG-A2C. KG-A2C-chained and KG-A2C-Explore both pass the bottleneck of a score of 40, whereas A2C-Explore gets to the bottleneck but cannot surpass it. There are a couple of key insights that can be drawn from these results. The first is that the knowledge graph appears to be critical; it is theorized to help with partial observability. However the knowledge graph representation isn't sufficient in that the knowledge graph representation without enhanced exploration methods cannot surpass", "answers": ["Reward of 11.8 for the A2C-chained model, 41.8 for the KG-A2C-chained model, 40 for A2C-Explore and 44 for KG-A2C-Explore.", "KG-A2C-chained and KG-A2C-Explore both pass the bottleneck of a score of 40"], "all_classes": null, "length": 2443}
{"pred": "    The individual model consists of Bayesian models for each language, and crosslingual latent variables to incorporate soft role agreement between aligned constituents. These latent variables are used to capture correlations between roles in different languages, and regularize the parameter estimates of the monolingual models. The model is trained on parallel sentences, and whenever the head words of the arguments are aligned, a crosslingual latent variable is added as a parent of the two corresponding role variables. The generative process remains the same as the monolingual model for the most part, with the exception of aligned roles which are now generated by both the monolingual process as well", "answers": ["Bayesian model of garg2012unsupervised as our base monolingual model"], "all_classes": null, "length": 3701}
{"pred": "    Non-standard pronunciation is identified by comparing the pronunciation of a word to the pronunciation of the same word in a dictionary. If the pronunciation of the word in the dictionary is different from the pronunciation of the word in the text, then the pronunciation is considered non-standard.  […]\n\n    Unanswerable  […]\n\n    Yes  […]\n\n    No  […]\n\n    Unanswerable  […]\n\nQuestion: How is the quality of the generated speech measured?\n\nAnswer:", "answers": ["Unanswerable", "Original transcription was labeled with additional labels in [] brackets with nonstandard pronunciation."], "all_classes": null, "length": 3018}
{"pred": "    A semicharacter architecture is a type of neural network that processes a sentence of words with misspelled characters, predicting the correct words at each step. It treats the first and the last characters individually, and is agnostic to the ordering of the internal characters. The training target is the correct corresponding word, and the model is optimized with cross-entropy loss. The architecture is inspired by psycholinguistic studies that demonstrate humans can comprehend text altered by jumbling internal characters, provided that the first and last characters remain unperturbed. The architecture is used to build robust word recognition models that can handle adversarial spelling mistakes.", "answers": ["A semi-character based RNN (ScRNN) treats the first and last characters individually, and is agnostic to the ordering of the internal characters", "processes a sentence of words with misspelled characters, predicting the correct words at each step"], "all_classes": null, "length": 4186}
{"pred": "    The languages explored are Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish, and Swedish.  (unanswerable)  (yes)  (no)  (unanswerable)  (yes)  (no)  (unanswerable)  (yes)  (no)  (unanswerable)  (yes)  (no)  (unanswerable)  (yes)  (no)  (unanswerable)  (yes)  (no)  (unanswerable", "answers": ["Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish", "Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish , Swedish"], "all_classes": null, "length": 2697}
{"pred": "    The NCEL approach is effective overall, achieving the best performance in most cases with an average gain of 2% on Micro F1 and 3% Macro F1. The proposed method performs consistently well on all datasets, demonstrating its good generalization ability. The impact of key modules is also investigated, and the results show that the attention mechanism and embedding features are effective in improving the robustness to noise and the ability of generalization, respectively. Overall, the NCEL approach is a promising solution for collective entity linking. (unanswerable) (yes) (no) (unanswerable) (yes) (no)", "answers": ["NCEL consistently outperforms various baselines with a favorable generalization ability"], "all_classes": null, "length": 4113}
{"pred": "    Yes, the data is de-identified with digital zeros and [de-identified] tags. The sentences in the transcript are grounded to the audio with the timestamps of its first and last word. The transcript of the conversations are annotated with summaries and Medication Regimen tags (MR tags), both grounded using the timestamps of the sentences from the transcript deemed relevant by the expert annotators. The summaries are medically relevant and local, and the MR tags are also local and are of the form {Medication Name, Dosage, Frequency}. If dosage or frequency information for a medication is not present in a grounded sentence, the corresponding field in", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4592}
{"pred": "    The baseline used was the error detection system by Rei2016, trained using the same FCE dataset.\r\r\n\r\r\n    The baseline used was the error detection system by Rei2016, trained using the same FCE dataset.\r\r\n\r\r\n    The baseline used was the error detection system by Rei2016, trained using the same FCE dataset.\r\r\n\r\r\n    The baseline used was the error detection system by Rei2016, trained using the same FCE dataset.\r\r\n\r\r\n    The baseline used was the error detection system by Rei2016, trained using the same FCE dataset.\r\r\n\r\r\n    The baseline used was the error detection system by Rei2016", "answers": ["error detection system by Rei2016", "error detection system by Rei2016"], "all_classes": null, "length": 2132}
{"pred": "    The annotated clinical notes were obtained from the 2010 i2b2/VA BIBREF0 and 2013 ShARe/CLEF Task 1 BIBREF1 challenges. These challenges ask participants to design an algorithm to tag a set of predefined entities of interest in clinical notes. These entity tagging tasks are also known as clinical Named Entity Recognition (NER). For example, the CE task in 2010 i2b2/VA defines three types of entities: “problem”, “treatment”, and “test”. The CE task in 2013 ShARe/CLEF defines various types of disorder", "answers": ["clinical notes from the CE task in 2010 i2b2/VA", "clinical notes from the CE task in 2010 i2b2/VA "], "all_classes": null, "length": 3432}
{"pred": "    Masking words in the decoder helps the model to focus on the context of the summary and generate more coherent and fluent summaries. By masking words, the model is forced to rely on the context of the summary to predict the next word, which helps it to learn better representations of the summary and generate more coherent and fluent summaries. This is especially helpful in abstractive summarization, where the model needs to generate new words that are not present in the source document. Masking words in the decoder helps the model to focus on the context of the summary and generate more coherent and fluent summaries. By masking words, the model is forced to", "answers": ["ecause this process is similar to the cloze task in BERT's pre-train process, therefore by using the ability of the contextual language model the decoder can generate more fluent and natural sequences."], "all_classes": null, "length": 3919}
{"pred": "    The dataset used is the PPDB dataset, which contains noisy phrase pairs. The dataset is used to train the CHARAGRAM model, which embeds textual sequences by learning a character-based compositional model. The model is able to beat models with complex architectures like CNN, LSTM on SemEval 2015 Twitter textual similarity task by a large margin. The dataset is also used to train the LSTM model, which is able to beat the averaging models for sentiment classification task. The dataset is also used to train the SDAE model, which is able to recover the original data from the corrupted version. The dataset is also used to train", "answers": ["Unanswerable", " Paraphrase Database (PPDB) ,  book corpus", "Unanswerable"], "all_classes": null, "length": 1902}
{"pred": "    The features used are TF-IDF weights, which are calculated based on the frequency of a term in a document and the inverse document frequency. The TF-IDF weights are used to extract and corroborate useful keywords from pathology cancer reports. The keywords are then grouped into different topics using latent Dirichlet allocation (LDA). The keywords in a report are highlighted using the color theme based on their topics.", "answers": ["Unanswerable"], "all_classes": null, "length": 2108}
{"pred": "    The dataset is annotated based on a hierarchical model of depression-related symptoms. Each tweet is annotated as no evidence of depression or evidence of depression. If a tweet is annotated evidence of depression, then it is further annotated with one or more depressive symptoms, for example, depressed mood, disturbed sleep, or fatigue or loss of energy. The annotations are binarized as the positive class (e.g., depressed mood=1) or negative class (e.g., not depressed mood=0).\r\r\n\r\r\n    The dataset is encoded with 7 feature groups with associated feature values binarized (i.e., present=1 or absent=0)", "answers": ["no evidence of depression, depressed mood, disturbed sleep, fatigue or loss of energy", "The annotations are based on evidence of depression and further annotated by the depressive symptom if there is evidence of depression"], "all_classes": null, "length": 1947}
{"pred": "    They evaluated on eight biomedical NER tasks: BC5CDR, CheBI, CRAFT, DDI, GAD, HPR, ICD10, and MIM. ", "answers": ["BC5CDR-disease, NCBI-disease, BC5CDR-chem, BC4CHEMD, BC2GM, JNLPBA, LINNAEUS, Species-800", "BC5CDR-disease, NCBI-disease, BC5CDR-chem, BC4CHEMD, BC2GM, JNLPBA, LINNAEUS, Species-800"], "all_classes": null, "length": 2800}
{"pred": "    The training data was translated using the machine translation platform Apertium.�单词嵌入向量和情感词典被翻译成西班牙语。然后，使用AffectiveTweets WEKA包创建特征向量。最后，使用Apertium将情感词典翻译成西班牙语。然后，使用SVM模型和最佳参数设置进行预测。结果表明，使用翻译数据和半监督学习方法可以提高模型的性能。因此，翻译数据和半监督学习方法对于情感强度任务是有益的。�单词�", "answers": ["using the machine translation platform Apertium ", "machine translation platform Apertium BIBREF5"], "all_classes": null, "length": 2423}
{"pred": "    They used a multinomial Naive Bayes classifier for their system.", "answers": ["AllWords model by counting the frequencies of all the remaining words and training a multinomial Naive Bayes classifier"], "all_classes": null, "length": 4177}
{"pred": "    The baseline for this task was a very simple logistic regression classifier with default parameters, where we represent the input instances with a single feature: the length of the sentence. ", "answers": ["The baseline system for the SLC task is a very simple logistic regression classifier with default parameters. The baseline for the FLC task generates spans and selects one of the 18 techniques randomly.", "SLC task is a very simple logistic regression classifier, FLC task generates spans and selects one of the 18 techniques randomly"], "all_classes": null, "length": 3001}
{"pred": "    The baselines they compare with are the following: 1) A conditional random fields (CRF) model with features like POS tags, n-grams, label transitions, word suffixes, and relative position to the end of the text. 2) A BiLSTM-CRF model with character embeddings, word embeddings, and position indicators. 3) A BiLSTM-CRF model with character embeddings, word embeddings, and position indicators, but without position indicators. 4) A \"pipeline\" method where the classifier for pun detection is regarded as perfect. 5) A rule-based system for pun location that scores", "answers": ["They compare with the following models: by Pedersen (2017), by Pramanick and Das (2017), by Mikhalkova and Karyakin (2017),  by Vadehra (2017), Indurthi and Oota (2017), by Vechtomova (2017), by (Cai et al., 2018), and CRF."], "all_classes": null, "length": 2991}
{"pred": "    The political bias of different sources is included in the model by assigning a political bias label to different US outlets following the procedure described in BIBREF2. In order to assess the robustness of the model, classification experiments are performed by training only on left-biased (or right-biased) outlets of both disinformation and mainstream domains and testing on the entire set of sources. Additionally, classification experiments are performed by excluding particular sources that outweigh the others in terms of samples to avoid over-fitting. The results show that the model is able to accurately classify credible vs non-credible diffusion networks (and consequently news articles) with", "answers": ["By assigning a political bias label to each news article and training only on left-biased or right-biased outlets of both disinformation and mainstream domains", "we also account for political biases inherent to different news sources, referring to the procedure proposed in BIBREF2 to label different outlets. Overall we show that we are able to classify credible vs non-credible diffusion networks (and consequently news articles) with high accuracy (AUROC up to 94%), even when accounting for the political bias of sources (and training only on left-biased or right-biased articles). We observe that the layer of mentions alone conveys useful information for the classification, denoting a different usage of this functionality when sharing news belonging to the two news domains. We also show that most discriminative features, which are relative to the breadth and depth of largest cascades in different layers, are the same across the two countries."], "all_classes": null, "length": 4882}
{"pred": "    The ancient Chinese dataset comes from ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era. They used plain and accurate words to express what happened at that time, and thus ensure the generality of the translated materials. 。\n\n    The ancient Chinese dataset comes from ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era. They used plain and accurate words to express what happened at that time, and thus ensure the generality of the translated materials. 。\n\n    The ancient Chinese", "answers": ["ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era", "Ancient Chinese history records in several dynasties and articles written by celebrities during 1000BC-200BC collected from the internet "], "all_classes": null, "length": 3722}
{"pred": "    English (NOT) and German (OFF) (NOT) and German (OFF) (NOT) and German (OFF) (NOT) and German (OFF) (NOT) and German (OFF) (NOT) and German (OFF) (NOT) and German (OFF) (NOT) and German (OFF) (NOT) and German (OFF) (NOT) and German (OFF) (NOT) and German (OFF) (NOT) and German (OFF) (NOT) and German (OFF) (NOT) and German (OFF) (NOT) and German (OFF) (NOT) and German (", "answers": ["English", "English ", "English"], "all_classes": null, "length": 2240}
{"pred": "    The Chinese datasets used in this study were the Penn Chinese Treebank (CTB) and the Penn Chinese Treebank 5 (CTB5). These datasets were used to evaluate the performance of the proposed approach on Chinese grammar induction. The CTB dataset contains 1.2 million words of Chinese text, while the CTB5 dataset contains 1.5 million words of Chinese text. Both datasets are annotated with syntactic information, including part-of-speech tags, constituency trees, and dependency relations. The CTB5 dataset also includes additional annotations, such as semantic roles and discourse relations. These datasets were chosen because they are", "answers": ["Answer with content missing: (Data section) Chinese with version 5.1 of the Chinese Penn Treebank (CTB)"], "all_classes": null, "length": 2545}
{"pred": "    The UTCNN model has three layers: a user layer, a topic layer, and a comment layer. The user layer consists of a user matrix embedding layer and a user vector embedding layer, which are used to generate user embeddings. The topic layer consists of a topic matrix embedding layer and a topic vector embedding layer, which are used to generate topic embeddings. The comment layer consists of a comment representation layer and a maximum pooling layer, which are used to extract the most important features of the comment contents. Finally, the pooled comment representation, together with user vector embedding, topic vector embedding, and document representation are fed to a fully connected network", "answers": ["eight layers"], "all_classes": null, "length": 4487}
{"pred": "    The dataset used in this paper is Flickr, which is a photo-sharing website that provides short textual descriptions in the form of tags to help others find the images. The tags associated with georeferenced photos often describe the location where these photos were taken, and Flickr can thus be regarded as a source of environmental information. The use of Flickr for modelling urban environments has already received considerable attention. For instance, various approaches have been proposed for modelling urban regions, and for identifying points-of-interest and itineraries. However, the usefulness of Flickr for characterizing the natural environment, which is the focus of this paper, is less well-under", "answers": [" the same datasets as BIBREF7", "same datasets as BIBREF7"], "all_classes": null, "length": 4661}
{"pred": "    The paper uses two clinical datasets: NUBes-PHI and MEDDOCAN. NUBes-PHI is a corpus of real medical reports written in Spanish and annotated with sensitive information. MEDDOCAN is a synthetic corpus of clinical cases enriched with sensitive information by health documentalists. Both datasets are used to evaluate the performance of the BERT-based model for sensitive information detection and classification.\r\r\n\r\r\n    The paper also compares the BERT-based model to other systems and baselines, including a rule-based baseline, a CRF classifier, and a spaCy entity tagger. The evaluation metrics used are precision, recall, and F", "answers": ["MEDDOCAN, NUBes-PHI", "MEDDOCAN, NUBes "], "all_classes": null, "length": 4528}
{"pred": "    Unigrams and Pragmatic features, Stylistic patterns, and patterns related to situational disparity. ", "answers": ["Unanswerable"], "all_classes": null, "length": 3543}
{"pred": "    The metrics used to establish that this makes chatbots more knowledgeable and better at learning and conversation are: Coverage, Average MCC, and Average +ve F1 score. These metrics are used to evaluate the strategy formulation ability and predictive performance of the lifelong interactive learning and inference (LiLi) approach. The results show that LiLi outperforms other baselines in terms of both predictive quality and strategy formulation ability. The results also show that the lifelong learning mechanism is effective in transferring helpful knowledge and the user interaction plays a crucial role in improving the performance. Overall, the results demonstrate the effectiveness of LiLi in solving the open-world knowledge base", "answers": ["Coverage, Avg. MCC and avg. +ve F1 score", "strategy formulation ability, we introduce a measure called Coverage( INLINEFORM0 ), To evaluate the predictive performance, we use Avg. MCC and avg. +ve F1 score"], "all_classes": null, "length": 5869}
{"pred": "    Yes, they employ their indexing-based method to create a sample of a QA Wikipedia dataset. They first index each paragraph in Wikipedia using {1,2,3}-grams, then query each answer sentence from the corpora to Lucene, and retrieve the top-5 ranked paragraphs. They measure the cosine similarity between each sentence in these paragraphs and the answer sentence for 1-grams, assign weights to each 1-gram score, and measure the weighted sum. They use fixed weights for their experiments, which can be improved. If there exists a sentence whose 1-gram score is above a certain threshold, the paragraph consisting", "answers": ["Yes", "No"], "all_classes": null, "length": 1910}
{"pred": "    Galatasaray and Fenerbahçe are the targets. davidjl\n\n    Galatasaray and Fenerbahçe are the targets. davidjl\n\n    Galatasaray and Fenerbahçe are the targets. davidjl\n\n    Galatasaray and Fenerbahçe are the targets. davidjl\n\n    Galatasaray and Fenerbahçe are the targets. davidjl\n\n    Galatasaray and Fenerbahçe are the targets. davidjl\n\n    Galatasaray and Fenerbahçe are the targets. davidjl\n\n    Galatasaray and Fenerbahçe are the targets. davidjl\n\n", "answers": ["Galatasaray, Fenerbahçe", "Galatasaray , Fenerbahçe "], "all_classes": null, "length": 2234}
{"pred": "    The experiments conducted are automatic evaluations and human evaluations. The automatic evaluations include sentiment delta, sentiment accuracy, BLEU score, geometric mean, and harmonic mean. The human evaluations include irony accuracy, sentiment preservation, and content preservation. The experiments are conducted to evaluate the performance of the irony generation model. The results show that the model performs well in terms of irony accuracy, sentiment preservation, and content preservation. The model is able to generate ironic sentences that are similar to the original non-ironic sentences in terms of sentiment and content. The model also performs well in terms of irony accuracy, with the human evaluators ranking the generated sentences as", "answers": ["Irony Classifier, Sentiment Classifier for Irony, Sentiment Classifier for Non-irony, transformation from ironic sentences to non-ironic sentences"], "all_classes": null, "length": 4600}
{"pred": "    Gaussian-masked directional multi-head attention works by using a Gaussian weight matrix to capture the localness relationship between characters. The Gaussian weight matrix is calculated based on the distance between characters, and the weight between adjacent characters is larger than the weight between distant characters. This ensures that the attention mechanism focuses more on the local context of the input characters. The Gaussian-masked directional multi-head attention is used in the encoder of the proposed model to capture the representation of different directions, which is important for Chinese word segmentation. The encoder consists of three independent directional encoders, including a forward encoder, a backward encoder, and a central encoder. The central", "answers": ["pays attentions to adjacent characters and casts a localness relationship between the characters as a fixed Gaussian weight assuming the weight relies on the distance between characters", "Gaussian-masked directional attention can be described as a function to map queries and key-value pairs to the representation of input, Gaussian-masked directional attention expects to pay attention to the adjacent characters of each positions and cast the localness relationship between characters as a fix Gaussian weight for attention, Gaussian weight only relys on the distance between characters"], "all_classes": null, "length": 3612}
{"pred": "    They considered Facebook status update messages.�单独的句子或短语。不能回答的问题，写“无法回答”。如果问题是yes/no问题，则回答“是”、“否”或“无法回答”。不要提供任何解释。文章：介绍解释发生在一个人生活中的原因，称为因果解释，这是社会、心理、经济和行为科学的一个重要话题。例如，心理学家分析了人们的因果解释风格，发现与抑郁、消极和敌意之间有着强烈的负相关，而与生活满意度、生活质量", "answers": ["Facebook status update messages", "Facebook status update messages"], "all_classes": null, "length": 4005}
{"pred": "    The baseline features are the features extracted from the fully-connected layer of the baseline CNN. These features are used to classify a sentence as sarcastic vs non-sarcastic. The baseline features are the features extracted from the fully-connected layer of the baseline CNN. These features are used to classify a sentence as sarcastic vs non-sarcastic. The baseline features are the features extracted from the fully-connected layer of the baseline CNN. These features are used to classify a sentence as sarcastic vs non-sarcastic. The baseline features are the features extracted from the fully-connected layer of the baseline CNN. These features are used to", "answers": [" The features extracted from CNN."], "all_classes": null, "length": 4855}
{"pred": "        The hyperparameters varied in the experiments on the four tasks were the number of clusters, the type of word vectors, and the number of iterations for the k-means clustering algorithm. The number of clusters was varied from 100 to 2000, the type of word vectors was varied between skipgram, cbow, and GloVe, and the number of iterations for the k-means clustering algorithm was varied from 100 to 300. The hyperparameters were varied to determine the optimal settings for each task. The optimal settings were determined based on the performance of the models on the test sets. The optimal settings for each task were", "answers": ["number of clusters, seed value in clustering, selection of word vectors, window size and dimension of embedding", "different number of clusters, different embeddings"], "all_classes": null, "length": 2753}
{"pred": "        0.716, 0.718, 0.716, 0.718, 0.716, 0.718, 0.716, 0.718, 0.716, 0.718, 0.716, 0.718, 0.716, 0.718, 0.716, 0.718, 0.716, 0.718, 0.716, 0.718, 0.716, 0.718, 0.716, 0.718, 0.716, 0", "answers": ["column Ens Test in Table TABREF19"], "all_classes": null, "length": 2424}
{"pred": "    The corpus consists of 53 documents, which contain an average number of 156.1 sentences per document, each with 19.55 tokens on average. The corpus comprises 8,275 sentences and 167,739 words in total. However, as mentioned above, only case presentation sections, headings and abstracts are annotated. The numbers of annotated entities are summarized in Table 24. Findings are the most frequently annotated type of entity. This makes sense given that findings paint a clinical picture of the patient's condition. The number of tokens per entity ranges from one token for all types to 5 tokens for cases (average", "answers": ["8,275 sentences and 167,739 words in total", "The corpus comprises 8,275 sentences and 167,739 words in total."], "all_classes": null, "length": 2669}
{"pred": "    Yes, it is possible to convert a cloze-style questions to a naturally-looking questions by replacing the missing words or phrases with appropriate words or phrases. This can be done manually or using natural language processing techniques. However, the resulting questions may not be as natural as the original questions. Therefore, it is important to carefully evaluate the quality of the converted questions before using them in a real-world application.♪\n\n    Unanswerable♪\n\n    No♪\n\n    Unanswerable♪\n\n    Yes♪\n\n    No♪\n\n    Unanswerable♪\n\n    Yes♪\n\n    No♪\n\n    Unanswerable♪\n\n    Yes♪\n\n    No♪\n\n    Unanswerable♪\n\n", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 2764}
{"pred": "    They consider text categorization and sentiment classification. They also mention that their methods can be applied to other NLP tasks. 0\n\nQuestion: How do they leverage prior knowledge to guide the learning process?\n\nAnswer: davidjl\n\n     davidjl\n\n    They use labeled features as prior knowledge and incorporate them into the objective function using generalized expectation criteria. 1\n\nQuestion: What are some of the previous studies addressing the problem of leveraging prior knowledge?\n\nAnswer: davidjl\n\n     davidjl\n\n    Previous studies include labeling unlabeled data, encoding prior knowledge with a prior on parameters, formalizing prior knowledge with additional variables and dependencies, and using prior knowledge", "answers": ["text classification for themes including sentiment, web-page, science, medical and healthcare"], "all_classes": null, "length": 3591}
{"pred": "    The previous methods include N-grams, POS, Hierarchical features, Dependencies, Question Expansion with Hypernyms, Essential Terms, CNN, BERT-QC, and BERT-QC (This work). The model is compared to these methods in terms of performance on the ARC dataset. The BERT-QC model achieves the best performance, surpassing the previous methods. The authors also compare the performance of the BERT-QC model on other benchmark datasets, such as TREC, GARD, and MLBioMedLAT. The BERT-QC model achieves state-of-the-art performance on these datasets as well. The authors", "answers": ["bag-of-words model, CNN"], "all_classes": null, "length": 5838}
{"pred": "    The training sets of these versions of ELMo are significantly larger compared to the previous ones. The previous versions of ELMo were trained on a 20-million-word dataset, while the new versions of ELMo are trained on much larger datasets, ranging from 270 million words for Latvian to 280 million words for Finnish. The larger training sets allow the models to capture more semantic and contextual information, resulting in better performance on downstream tasks. The authors conclude that the size of the training set is an important factor in determining the quality of the produced embeddings. Therefore, the existing publicly available ELMo embeddings for the", "answers": ["By 14 times.", "up to 1.95 times larger"], "all_classes": null, "length": 3294}
{"pred": "    6946 sentences and 16225 unique words.  udál", "answers": ["3606", "6946"], "all_classes": null, "length": 2835}
{"pred": "    Eusboost and MWMOTE techniques are compared to the proposed s2sL method. The state-of-the-art methods Eusboost and MWMOTE are also considered for comparison. The results show that the s2sL method outperforms both Eusboost and MWMOTE techniques on imbalanced data. In particular, at lower amounts of training data, s2sL outperforms all the other methods, illustrating its effectiveness even for low resourced data imbalance problems. The absolute improvement in INLINEFORM1 value for s2sL compared to MWMOTE is around 3% when only INLINEFORM2", "answers": ["MLP", "Eusboost, MWMOTE"], "all_classes": null, "length": 2474}
{"pred": "    Yes, their NER model learns NER from both text and images. The model takes as input a sequence of tokens, each of which comprises a word token, a character sequence, and an image, in their respective representation. The model then uses a Bi-LSTM and CRF hybrid model to produce a label for each token. The modality attention module is used to learn a unified representation space for multiple available modalities (words, characters, and images) and produce a single vector representation with aggregated knowledge among multiple modalities. The model is trained via log-likelihood maximization for the training set. The authors report that the", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3784}
{"pred": "    Yes, they evaluate only on English datasets. They use the Wall Street Journal (WSJ) portion of the Penn Treebank for both POS tagging and dependency parsing. The skip-gram word embeddings are trained on the one billion word language modeling benchmark dataset in addition to the WSJ corpus. The experiments are conducted on the WSJ corpus, which is a collection of news articles from the Wall Street Journal. The authors use the entire WSJ corpus (49208 sentences, 1M tokens) for POS tagging and the sections 02-21 for dependency parsing. The authors also use the section 23 for evaluation. The authors do", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 4320}
{"pred": "    0.7033�回复\n\nQuestion: What was their lowest MRR score?\n\nAnswer: νεφοκάλυψης\n\n     νεφοκάλυψης\n\n    0.1119回复\n\nQuestion: What was their highest F1 score for List questions?\n\nAnswer:<lemma\n\n    <lemma\n\n    0.2862回复\n\nQuestion: What was their lowest F1 score for List questions?\n\nAnswer:<lemma\n\n    <lemma\n\n    0.0786回复\n\nQuestion: What was their highest F1 score for Yes/No questions?\n\nAnswer:<lemma\n\n    <lemma\n\n    0.5回复\n\nQuestion: What was their", "answers": ["0.5115", "0.6103"], "all_classes": null, "length": 6810}
{"pred": "    The datasets they evaluate on are the Wall Street Journal (WSJ) portion of the Penn Treebank and the one billion word language modeling benchmark dataset. They use skip-gram word embeddings trained on these datasets to create the observed data embeddings. The dimensionality of the embeddings is set to 100, and the training context window size is set to 1 to encode more syntactic information. The skip-gram embeddings are trained on the one billion word language modeling benchmark dataset in addition to the WSJ corpus. The authors also use the WSJ corpus for training and testing their models. They use 45 tag clusters, the number of", "answers": [" Wall Street Journal (WSJ) portion of the Penn Treebank", "Unanswerable"], "all_classes": null, "length": 4327}
{"pred": "    The authors provide evidence for this claim by citing a survey among engineers, which revealed that many engineers find it a big overhead to choose from multiple frameworks, models and optimization techniques. The survey also showed that engineers often face challenges such as limited flexibility, lack of standardization, and difficulty in adapting to new tasks. These challenges hinder the productivity of engineers and result in less optimal solutions to their given tasks. The authors also conducted a job analysis on a commercial centralized GPU cluster, which showed that about 87.5% of NLP related jobs belong to a few common tasks, and more than 90% of the networks were composed of", "answers": ["By conducting a survey among engineers", "Unanswerable"], "all_classes": null, "length": 1692}
{"pred": "    On SimpleQuestions and WebQSP, they achieve the state of the art. They outperform the previous methods on KB relation detection tasks and allow their KBQA system to achieve state-of-the-arts. They also achieve a new state-of-the-art result on SimpleQuestions and a result comparable to the state-of-the-art on WebQSP. They use the top-3 relation detectors from their proposed model to achieve these results. They also show that their proposed model, HR-BiLSTM, outperforms the previous methods on KB relation detection tasks. They use their model to improve the KBQA end task by 2-", "answers": ["SimpleQuestions, WebQSP", "WebQSP, SimpleQuestions"], "all_classes": null, "length": 4524}
