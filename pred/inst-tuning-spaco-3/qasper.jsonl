{"pred": "    The ground truth for fake news is established by a single person who manually inspects the text field within the tweets to label them as containing fake news, or not containing them. This annotated dataset is publicly available and can be freely reused. (10 points) 10pt\n\nQuestion: What is the main finding regarding the exposure of fake news?\n\nAnswer: 노출등록\n\n    The main finding regarding the exposure of fake news is that viral tweets containing fake news are shorter-lived than those containing other type of content. (5 points) 5pt\n\nQuestion: What is the main finding regarding the characteristics of accounts spreading fake news?\n\nAnswer:", "answers": ["Ground truth is not established in the paper"], "all_classes": null, "length": 3141}
{"pred": "    The GhostVLAD approach is a pooling strategy that uses both NetVLAD and Ghost clusters to aggregate frame-level features into a single utterance-level feature. It adds Ghost clusters along with the NetVLAD clusters to map any noisy or irrelevant content into ghost clusters, which are not included during the feature aggregation stage. This helps to improve the performance of language identification by reducing the impact of noisy or irrelevant features. The GhostVLAD approach has been shown to outperform other pooling strategies for language identification, particularly for Indian languages. It has also been found to work well even when the input during training contains smaller durations. The embeddings generated by", "answers": ["extension of the NetVLAD, adds Ghost clusters along with the NetVLAD clusters", "An extension of NetVLAD which replaces hard assignment-based clustering with soft assignment-based clustering with the additon o fusing Ghost clusters to deal with noisy content."], "all_classes": null, "length": 2454}
{"pred": "    68.8% to 71.8% when applied to the IEMOCAP dataset. {\r\r\n）\n\n    68.8% to 71.8% when applied to the IEMOCAP dataset. {\r\r\n）\n\n    68.8% to 71.8% when applied to the IEMOCAP dataset. {\r\r\n）\n\n    68.8% to 71.8% when applied to the IEMOCAP dataset. {\r\r\n）\n\n    68.8% to 71.8% when applied to the IEMOCAP dataset. {\r\r\n）\n\n    68.8% to ", "answers": ["the attention model, MDREA, also outperforms the best existing research results (WAP 0.690 to 0.688)"], "all_classes": null, "length": 3207}
{"pred": "    The additional features and context proposed are context tweets, which are the tweets that the abusive tweet has replied to or quoted. The context tweets are used as an additional feature of neural network models to improve the accuracy of classification. The context tweets are directly used as an additional feature of neural network models, and the last hidden states of context and labeled tweets are concatenated. The use of context tweets generally have little effect on baseline models, however they noticeably improve the scores of several metrics. For instance, CNN with context tweets score the highest recall and F1 for \"hateful\" labels, and RNN models with context tweets have the highest recall", "answers": ["using tweets that one has replied or quoted to as contextual information", "text sequences of context tweets"], "all_classes": null, "length": 2060}
{"pred": "    FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney.", "answers": ["FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney", "FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney."], "all_classes": null, "length": 3411}
{"pred": "    Yes, the hashtag and SemEval datasets contain only English data. The article states that the hashtags are English and the SemEval dataset is in English. Therefore, the answer is yes. The article does not mention any other languages. The article also states that the hashtags are segmented into words, which is a process that is specific to English. Therefore, the answer is yes. The article does not mention any other languages. The article also states that the hashtags are segmented into words, which is a process that is specific to English. Therefore, the answer is yes. The article does not mention any other languages. The article also states that", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3735}
{"pred": "    The proposed evaluation is based on the concept map's ability to represent the most important content of the document cluster, satisfy a specified size limit, and be connected. The evaluation is designed to assess the quality of the concept map as a summary of the document cluster. The evaluation is not based on the quality of the individual concepts or relations in the concept map. The evaluation is not based on the ability of the concept map to be used for navigation in a document collection. The evaluation is not based on the ability of the concept map to be used for writing assistance. The evaluation is not based on the ability of the concept map to be used", "answers": ["Answer with content missing: (Evaluation Metrics section) Precision, Recall, F1-scores, Strict match, METEOR, ROUGE-2"], "all_classes": null, "length": 4263}
{"pred": "    The datasets used for evaluation are the CNN/DailyMail news highlights dataset, the New York Times Annotated Corpus (NYT), and XSum. These datasets represent different summary styles ranging from highlights to very brief one sentence summaries. The summaries also vary with respect to the type of rewriting operations they exemplify (e.g., some showcase more cut and paste operations while others are genuinely abstractive). The CNN/DailyMail dataset contains news articles and associated highlights, i.e., a few bullet points giving a brief overview of the article. The NYT dataset contains 110,540 articles with abstractive summaries. The XSum dataset", "answers": ["CNN/DailyMail news highlights, New York Times Annotated Corpus, XSum", "the CNN/DailyMail news highlights dataset BIBREF24, the New York Times Annotated Corpus (NYT; BIBREF25), XSum BIBREF22"], "all_classes": null, "length": 4369}
{"pred": "    The proposed approach outperforms other WSD approaches employing word embeddings in terms of accuracy and computational efficiency. It also provides a more interpretable representation of word meanings, allowing for better understanding of the underlying semantic relationships between words. Additionally, the approach is able to handle polysemy and other semantic challenges more effectively than previous methods. Overall, the proposed approach represents a significant improvement over existing WSD approaches. 】\n\n    Unanswerable 】\n\n    Yes 】\n\n    No 】\n\n    Unanswerable 】\n\n    Yes 】\n\n    No 】\n\n    Unanswerable 】\n\n    Yes 】\n\n    No 】\n\n    Un", "answers": ["GM$\\_$KL achieves better correlation than existing approaches for various metrics on SCWS dataset."], "all_classes": null, "length": 2189}
{"pred": "    The ensemble method works by simply averaging the predictions from the constituent single models. These single models are selected using a greedy algorithm that starts with the best performing model according to validation performance and then adds the best performing model that has not been previously tried. The algorithm keeps the model in the ensemble if it improves the validation performance and discards it otherwise. This process is repeated until all models have been tried once. The resulting model is called a greedy ensemble. The algorithm is offered 10 models and selects 5 of them for the final ensemble. 。\n\n    The ensemble method works by simply averaging the predictions from the constituent single models. These", "answers": ["simply averaging the predictions from the constituent single models"], "all_classes": null, "length": 4212}
{"pred": "    The sources of the datasets are Friends TV sitcom and Facebook messenger chats. The dataset consists of two subsets, Friends and EmotionPush, according to the source of the dialogues. The former comes from the scripts of the Friends TV sitcom, while the latter is made up of Facebook messenger chats. Each subset includes 1000 English dialogues, and each dialogue can be further divided into a few consecutive utterances. All the utterances are annotated by five annotators on a crowd-sourcing platform (Amazon Mechanical Turk), and the labeling work is only based on the textual content. Annotator votes for one of the seven emotions,", "answers": ["Friends TV sitcom, Facebook messenger chats"], "all_classes": null, "length": 3185}
{"pred": "    English (EN) (EN) (EN)\n\n    English (EN) (EN) (EN)\n\n    English (EN) (EN) (EN)\n\n    English (EN) (EN) (EN)\n\n    English (EN) (EN) (EN)\n\n    English (EN) (EN) (EN)\n\n    English (EN) (EN) (EN)\n\n    English (EN) (EN) (EN)\n\n    English (EN) (EN) (EN)\n\n    English (EN) (EN) (EN)\n\n    English (EN) (EN) (EN)\n\n    English (EN) (EN", "answers": ["English", "Simple English"], "all_classes": null, "length": 2243}
{"pred": "    The IMDb dataset of movie reviews is used for sentiment analysis.", "answers": ["IMDb dataset of movie reviews", "IMDb"], "all_classes": null, "length": 2327}
{"pred": "    91.2% on the test set.", "answers": ["F1 scores of 85.99 on the DL-PS data,  75.15 on the EC-MT data and 71.53 on the EC-UQ data ", "F1 of 85.99 on the DL-PS dataset (dialog domain); 75.15 on EC-MT and 71.53 on EC-UQ (e-commerce domain)"], "all_classes": null, "length": 5310}
{"pred": "    Yes, they experimented with this new dataset. They recorded data from 19 participants and discarded the data of one of them due to technical difficulties with the eye-tracking calibration. They recorded data from 18 participants and shared the data of 18 participants. All participants are healthy adults (mean age = 34 (SD=8.3), 10 females). Their native language is English, originating from Australia, Canada, UK, USA or South Africa. Two participants are left-handed and three participants wear glasses for reading. The participants read 739 sentences that were selected from the Wikipedia corpus provided by culotta2006integrating. The", "answers": ["No"], "all_classes": null, "length": 3445}
{"pred": "        The datasets used are the Wizard of Oz method and crowdsourcing. The Wizard of Oz method involves applying the Wizard of Oz method to a set of potential users of the system, and from this study, a set of questions that the users asked is collected. These questions are manually classified into a set of intent classes, and used to train the first version of the system. Next, this set is increased both in terms of number of classes and samples per class. The crowdsourcing approach involves posting the questions to the \"fake\" system and collecting the answers from the users. These answers are also manually classified into a set of intent classes,", "answers": ["Custom dataset with user questions; set of documents, twitter posts and news articles, all related to finance.", "a self-collected financial intents dataset in Portuguese"], "all_classes": null, "length": 13401}
{"pred": "    The Energy sector achieved the best performance. The accuracy of the GARCH(1,1) model ranged from 0.15 to 0.44 for the HealthCare and Energy sectors, respectively. The proposed model outperformed GARCH(1,1) for all analyzed sectors, demonstrating the effectiveness of combining price and news for short-term volatility forecasting. The fact that the model outperformed GARCH(1,1) for all analyzed sectors confirms the robustness of the proposed architecture and evidences that the global model approach generalizes well. The Energy sector achieved the best performance. The accuracy of the GARCH(", "answers": ["Energy with accuracy of 0.538", "Energy"], "all_classes": null, "length": 10349}
{"pred": "    They compared the performance of the SMT and various NMT models on their built dataset. The models to be tested and their configurations are as follows: SMT: The state-of-art Moses toolkit was used to train SMT model. RNN-based NMT: The basic RNN-based NMT model is based on BIBREF0 which is introduced above. Transformer-NMT: They also trained the Transformer model BIBREF4 which is a strong baseline of NMT on both augmented and unaugmented parallel corpus. The training configuration of the Transformer model is shown in Table TABREF32. The hyper-parameters are set based", "answers": ["RNN-based NMT model, Transformer-NMT"], "all_classes": null, "length": 3708}
{"pred": "    The three regularization terms are: (1) a regularization term associated with neutral features; (2) the maximum entropy of class distribution regularization term; and (3) the KL divergence between reference and predicted class distribution.  (Eq. 9), (Eq. 11), (Eq. 13)  (Eq. 9), (Eq. 11), (Eq. 13)  (Eq. 9), (Eq. 11), (Eq. 13)  (Eq. 9), (Eq. 11), (Eq. 13)  (Eq. 9), (", "answers": ["a regularization term associated with neutral features, the maximum entropy of class distribution regularization term, the KL divergence between reference and predicted class distribution", "a regularization term associated with neutral features,  the maximum entropy of class distribution, KL divergence between reference and predicted class distribution"], "all_classes": null, "length": 3604}
{"pred": "    1) SVM with unigram, bigram, and trigram features, which is a standard yet rather strong classifier for text features; 2) SVM with average word embedding, where a document is represented as a continuous representation by averaging the embeddings of the composite words; 3) SVM with average transformed word embeddings (the INLINEFORM0 in equation EQREF6), where a document is represented as a continuous representation by averaging the transformed embeddings of the composite words; 4) two mature deep learning models on text classification, CNN and Recurrent Convolutional Neural Networks (RCNN), where the hyperparameters are based on their", "answers": ["SVM with unigram, bigram, and trigram features, SVM with average word embedding, SVM with average transformed word embeddings, CNN, ecurrent Convolutional Neural Networks, SVM and deep learning models with comment information", "SVM with unigram, bigram, trigram features, with average word embedding, with average transformed word embeddings, CNN and RCNN, SVM, CNN, RCNN with comment information"], "all_classes": null, "length": 4512}
{"pred": "    5.5 points in the INLINEFORM1 measure and 3.5 points in the INLINEFORM2 measure. davidjl\n\n    5.5 points in the INLINEFORM1 measure and 3.5 points in the INLINEFORM2 measure. davidjl\n\n    5.5 points in the INLINEFORM1 measure and 3.5 points in the INLINEFORM2 measure. davidjl\n\n    5.5 points in the INLINEFORM1 measure and 3.5 points in the INLINEFORM2 measure. davidjl\n\n    5.5 points in the INLINEFORM1 measure and 3.5 points in the", "answers": ["They decrease MAE in 0.34"], "all_classes": null, "length": 2735}
{"pred": "    The proposed model uses sparse attention mechanisms, which can lead to crisper examples of attention head behavior and novel behaviors unraveled thanks to the sparsity and adaptivity of the model. Additionally, the model can learn different sparsity patterns at each layer, leading to more variance in individual head behavior, and overall to different tendencies compared to the fixed case of α=1.5. The model also learns to combine sparse and dense attention, and distinguishes between the encoder self-attention and the other two modules, which exhibit more uniform background distributions. The model also learns to identify specific roles played by different heads, such as positional heads,", "answers": ["the attention heads in the proposed adaptively sparse Transformer can specialize more and with higher confidence", "We introduce sparse attention into the Transformer architecture"], "all_classes": null, "length": 4902}
{"pred": "    The baseline was a context-agnostic MT system. The DocRepair model was trained to correct inconsistencies between sentence-level translations of the baseline system. The DocRepair model was trained on monolingual document-level data and was able to improve translation quality as measured by BLEU, targeted contrastive evaluation of several discourse phenomena, and human evaluation. The DocRepair model was able to correct inconsistencies in translations caused by VP ellipsis, lexical cohesion, and deixis. The DocRepair model was able to improve translation quality by 0.7 BLEU, and its sentence-level repair version by 0.5 BLEU. The DocRepair model", "answers": [" MT system on the data released by BIBREF11", "Transformer base, two-pass CADec model"], "all_classes": null, "length": 3716}
{"pred": "    The metrics used for evaluation are Labeled Attachment Scores (LAS) for zero-shot dependency parsing and accuracy for cross-lingual natural language inference (XNLI). The LAS score measures the percentage of correctly labeled dependencies in a treebank, while the accuracy score measures the percentage of correctly classified sentences in the XNLI dataset. Both metrics are used to evaluate the performance of the proposed bilingual language models (RAMEN) in zero-shot cross-lingual transfer tasks. The RAMEN models are evaluated on six target languages: French, Russian, Arabic, Chinese, Hindi, and Vietnamese. The results show that the RAMEN models offer", "answers": ["translation probabilities, Labeled Attachment Scores (LAS)", "accuracy, Labeled Attachment Scores (LAS)"], "all_classes": null, "length": 3402}
{"pred": "    The attention module is pretrained on the MT task.", "answers": ["the model is pre-trained on CTC-based ASR task and MT task in the pre-training stage."], "all_classes": null, "length": 4656}
{"pred": "    Unigrams and Pragmatic features, Stylistic patterns, and patterns related to situational disparity.  […]\n\n    BIBREF0, BIBREF1, BIBREF2, BIBREF3, BIBREF4, BIBREF5, BIBREF6, BIBREF7.  […]\n\n    BIBREF8, BIBREF9, BIBREF10, BIBREF11, BIBREF12, BIBREF13, BIBREF14, BIBREF15, BIBREF16, BIBREF17, BIBREF18, BIBREF19.", "answers": ["Unanswerable"], "all_classes": null, "length": 3543}
{"pred": "    The encoder has an LSTM architecture.", "answers": ["LSTM", "LSTM"], "all_classes": null, "length": 2289}
{"pred": "    Yes, WordNet is useful for taxonomic reasoning for this task. It provides a set of triples that represent semantic relations between concepts, which can be used to generate gold questions and answers. The use of WordNet allows for the construction of natural language questions that contextualize the types of concepts being probed. Moreover, the availability of glosses and example sentences in WordNet allows for the creation of natural language questions that contextualize the types of concepts being probed. The use of WordNet also allows for the evaluation of the extent to which models are able to handle complex forms of reasoning that require several inferential steps or hops.", "answers": ["Unanswerable", "Yes"], "all_classes": null, "length": 6391}
{"pred": "    The baselines were conventional automatic speech recognition (ASR) systems, hybrid systems, and end-to-end (E2E) systems. The conventional ASR systems typically consist of several independently learned components: an acoustic model to predict context-dependent sub-phoneme states (senones) from audio, a graph structure to map senones to phonemes, and a pronunciation model to map phonemes to words. Hybrid systems combine hidden Markov models to model state dependencies with neural networks to predict states. E2E systems reduce the overall complexity of the final system. The baselines were also wav2letter, which uses 1D", "answers": ["Unanswerable", "LF-MMI Attention\nSeq2Seq \nRNN-T \nChar E2E LF-MMI \nPhone E2E LF-MMI \nCTC + Gram-CTC"], "all_classes": null, "length": 1856}
{"pred": "    22,880 users. davidjluser", "answers": ["22,880 users", "20,000"], "all_classes": null, "length": 4160}
{"pred": "    BLEU-1/4, ROUGE-L, Distinct-1/2, and recipe-level coherence. 1/2, and recipe-level coherence. 1/2, and recipe-level coherence. 1/2, and recipe-level coherence. 1/2, and recipe-level coherence. 1/2, and recipe-level coherence. 1/2, and recipe-level coherence. 1/2, and recipe-level coherence. 1/2, and recipe-level coherence. 1/2, and recipe-level coherence. 1/2, and recipe-level coherence. 1/2", "answers": ["Byte-Pair Encoding perplexity  (BPE PPL),\nBLEU-1,\nBLEU-4,\nROUGE-L,\npercentage of distinct unigram (D-1),\npercentage of distinct bigrams(D-2),\nuser matching accuracy(UMA),\nMean Reciprocal Rank(MRR)\nPairwise preference over baseline(PP)", "BLEU-1/4 and ROUGE-L, likelihood of generated recipes using identical input specifications but conditioned on ten different user profiles, user matching accuracy (UMA), Mean Reciprocal Rank (MRR), neural scoring model from BIBREF33 to measure recipe-level coherence", " Distinct-1/2, UMA = User Matching Accuracy, MRR\n= Mean Reciprocal Rank, PP = Pairwise preference over baseline (evaluated for 310 recipe pairs per model)"], "all_classes": null, "length": 2673}
{"pred": "    They create labels on their dataset based on the linguistic characteristics of the real-world human-human symptom checking dialogues. The labels include open-ended inquiries, detailed inquiries, multi-intent inquiries, reconfirmation inquiries, inquiries with transitional clauses, yes/no responses, detailed responses, responses with revision, responses with topic drift, and responses with transitional clauses. These labels are used to categorize the utterances in the seed data and then abstracted into templates by replacing entity phrases like cough and often with respective placeholders. The templates are refined through verifying logical correctness and injecting expression diversity by linguistically trained researchers. The labels are also used to construct expression pools", "answers": ["(1) the time the patient has been experiencing the symptom, (2) activities that trigger the symptom (to occur or worsen), (3) the extent of seriousness, (4) the frequency occurrence of the symptom, and (5) the location of symptom, No Answer", "the time the patient has been experiencing the symptom, activities that trigger the symptom, the extent of seriousness, the frequency occurrence of the symptom, the location of symptom, 9 symptoms"], "all_classes": null, "length": 3424}
{"pred": "    1000 random articles. This demonstrates the effectiveness of recognizing difficult instances. The trend when we use up all expert data is still upward, so adding even more expert data is likely to further improve performance. Unfortunately we exhausted our budget and were not able to obtain additional expert annotations. It is likely that as the size of the expert annotations increases, the value of crowd annotations will diminish. This investigation is left for future work. {\r\r\n\n\n    Unanswerable. The article does not provide enough information to answer this question. {\r\r\n\n\n    No. The article does not provide enough information to answer this question. {\r\r\n\n\n    Unanswerable", "answers": ["57,505 sentences", "57,505 sentences"], "all_classes": null, "length": 4371}
{"pred": "    The tasks used for evaluation are Neural Machine Translation, and analyzing the behavior of the learned attention heads.ยนตร\n\nQuestion: What are the main contributions of the paper?\n\nAnswer: The main contributions of the paper are the introduction of sparse attention into the Transformer architecture, showing that it eases interpretability and leads to slight accuracy gains, and the proposal of an adaptive version of sparse attention, where the shape of each attention head is learnable and can vary continuously and dynamically between the dense limit case of softmax and the sparse, piecewise-linear sparsemax case. Moreover, the paper makes an extensive analysis of the added interpretability of these models", "answers": ["four machine translation tasks: German -> English, Japanese -> English, Romanian -> English, English -> German", " four machine translation tasks, IWSLT 2017 German $\\rightarrow $ English BIBREF27, KFTT Japanese $\\rightarrow $ English BIBREF28, WMT 2016 Romanian $\\rightarrow $ English BIBREF29, WMT 2014 English $\\rightarrow $ German BIBREF30"], "all_classes": null, "length": 4898}
{"pred": "    The performance for Estonian in the NER task improved by 3.5% compared to the fastText baseline. The performance for other languages also improved, with the largest improvement for Croatian and Lithuanian. The performance for Slovenian was slightly worse than the fastText baseline, but still improved for all other EMBEDDIA languages. The original English ELMo model performed poorly in syntactic categories and only outperformed Latvian in semantic categories. The low score can be partially explained by English model scoring 0.00 in one syntactic category. Overall, the newly produced contextual embeddings produced substantially better results compared", "answers": ["5 percent points.", "0.05 F1"], "all_classes": null, "length": 3290}
{"pred": "    The authors have backgrounds in political science, computational linguistics, and digital humanities. They have experience in analyzing text as social and cultural data. They have worked on projects related to hate speech, rumors, and political traits. They have also worked on projects related to Darwin's reading decisions, media frames in news stories, and the use of language in social media. They have experience in developing computational models and using them to explore and answer research questions. They have experience in validating and analyzing the output of these models. They have experience in working across disciplines and negotiating about what approaches to analysis are appropriate and feasible. They have experience in making use", "answers": ["Unanswerable"], "all_classes": null, "length": 8506}
{"pred": "    No, the paper introduces a supervised approach to spam detection. The LDA model is used to extract topic-based features, which are then used in a supervised classification model to detect spammers. The paper does not discuss unsupervised methods for spam detection. Therefore, the answer to the question is \"no\". The paper introduces a supervised approach to spam detection, using topic-based features extracted from the LDA model. The paper does not discuss unsupervised methods for spam detection. Therefore, the answer to the question is \"no\". The paper introduces a supervised approach to spam detection, using topic-based features extracted from the LDA model", "answers": ["No", "No"], "all_classes": null, "length": 2239}
{"pred": "    The Nguni languages are similar to each other and harder to distinguish. The same is true of the Sotho languages. 。\n\n    The Nguni languages are similar to each other and harder to distinguish. The same is true of the Sotho languages. 。\n\n    The Nguni languages are similar to each other and harder to distinguish. The same is true of the Sotho languages. 。\n\n    The Nguni languages are similar to each other and harder to distinguish. The same is true of the Sotho languages. 。\n\n    The Nguni languages are similar to each other and", "answers": ["Nguni languages (zul, xho, nbl, ssw), Sotho languages (nso, sot, tsn)", "The Nguni languages are similar to each other, The same is true of the Sotho languages"], "all_classes": null, "length": 1877}
{"pred": "    They compared the performance of 6-layers and 9-layers sMBR models. The 3 additional layers of layer-wise training brought a relative 12.6% decrease in CER. The averaged CER of sMBR models with different layers decreased by about 0.73% compared with CE models, indicating the effectiveness of sequence discriminative learning. Additionally, they distilled a 2-layers model from a 9-layers model, achieving a 14% reduction in CER compared with a 2-layers regular trained model. Finally, they demonstrated the effectiveness of transfer learning with sMBR,", "answers": ["Unidirectional LSTM networks with 2, 6, 7, 8, and  9 layers."], "all_classes": null, "length": 4286}
{"pred": "    The Wikipedia dataset consists of articles from English Wikipedia, with quality class labels assigned by the Wikipedia community. The quality class of a Wikipedia article is assigned by Wikipedia reviewers or any registered user, who can discuss through the article's talk page to reach consensus. The dataset contains around 5K FA, 28K GA, 212K B, 533K C, 2.6M Start, and 3.2M Stub articles. The arXiv dataset consists of three subsets of academic articles under the arXiv repository of Computer Science (cs), from the three subject areas of: Artificial Intelligence (cs.ai),", "answers": ["a sample of  29,794 wikipedia articles and 2,794 arXiv papers "], "all_classes": null, "length": 4187}
{"pred": "    A group of 50 native people who were well-versed in both English and Tamil languages acted as annotators for the evaluation. A collection of samples of about 100 sentences were taken from the test set results for comparison. This set included a randomized selection of the translation results to ensure the objectivity of evaluation. Fluency and adequacy results for the RNNMorph results are tabulated. Adequacy rating was calculated on a 5-point scale of how much of the meaning is conveyed by the translation (All, Most, Much, Little, None). The fluency rating was calculated based on grammatical correctness on a", "answers": ["50 human annotators ranked a random sample of 100 translations by Adequacy, Fluency and overall ranking on a 5-point scale.", "adequacy, precision and ranking values"], "all_classes": null, "length": 5344}
{"pred": "    Yes, they test their framework performance on commonly used language pairs, such as English-to-German. They achieve considerable improvements in BLEU scores compared to the baseline NMT system. They also show that incorporating additional parallel data from other language pairs can help reduce ambiguities in the translation process. Their approach is able to accommodate the attention mechanism seamlessly and easily, with a conventional attention mechanism and a standard training procedure. They also demonstrate the effectiveness of their approach in an under-resourced translation task and in a zero-resourced translation task. Overall, their proposed approach is a promising direction for future research in multilingual NMT. However, there", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4472}
{"pred": "    The models are evaluated based on their efficiency and accuracy. Efficiency is measured by the retention rate of tokens, which is the fraction of tokens that are kept in the keywords. Accuracy is measured as the fraction of sentences generated by greedily decoding the model that exactly matches the target sentence. The models are also evaluated based on their stability and robustness. The stability of the models is evaluated by comparing the achievable accuracy and efficiency tradeoffs for different objectives. The robustness of the models is evaluated by analyzing the correlation between the retention rates of tokens and their properties. Finally, the models are evaluated based on their interpretability, which is ensured by", "answers": ["by training an autocomplete system on 500K randomly sampled sentences from Yelp reviews", "efficiency of a communication scheme $(q_{\\alpha },p_{\\beta })$ by the retention rate of tokens, which is measured as the fraction of tokens that are kept in the keywords, accuracy of a scheme is measured as the fraction of sentences generated by greedily decoding the model that exactly matches the target sentence"], "all_classes": null, "length": 1873}
{"pred": "    Precision, recall, and F-measure are looked at for classification tasks. These metrics are used to evaluate the performance of a classifier in identifying the correct class for a given input. Precision measures the proportion of true positives to all predicted positives, recall measures the proportion of true positives to all actual positives, and F-measure is a weighted average of precision and recall. These metrics provide a way to assess the accuracy and effectiveness of a classifier in classifying data. 0\n\nQuestion: What is the best approach for sentence classification in the given article?\n\nAnswer: 노출등록\n\n    The best approach for sentence classification in the given article is using", "answers": ["Precision, Recall, F-measure, accuracy", "Precision, Recall and F-measure"], "all_classes": null, "length": 3044}
{"pred": "    The source domain is the existing domain with sufficient labeled data, and the target domain is the new domain with very few or no labeled data. The goal is to transfer knowledge from the source domain to the target domain to alleviate the required labeling effort. The challenge is that data in the source and target domains are drawn from different distributions, which makes domain adaptation difficult. The key intuition is that domain-specific features can be aligned with the help of domain-invariant features (pivot features). The proposed method jointly performs feature adaptation and semi-supervised learning in a multi-task learning setting. The feature adaptation component explicitly minimizes the distance between the encoded representations", "answers": ["Book, electronics, beauty, music, IMDB, Yelp, cell phone, baby, DVDs, kitchen", "we use set 1 of the source domain as the only source with sentiment label information during training, and we evaluate the trained model on set 1 of the target domain, Book (BK), Electronics (E), Beauty (BT), and Music (M)"], "all_classes": null, "length": 5061}
{"pred": "    They compare with LSTM, QRNN, RAN, and NAS.", "answers": ["Variational LSTM, CharCNN, Pointer Sentinel-LSTM, RHN, NAS Cell, SRU, QRNN, RAN, 4-layer skip-connection LSTM, AWD-LSTM, Quantized LSTM"], "all_classes": null, "length": 3319}
{"pred": "    Embedding Layer, Neural Network Layers, Loss Function, Metrics.", "answers": ["Embedding Layer, Neural Network Layers, Loss Function, Metrics", "Embedding Layer, Neural Network Layers, Loss Function, Metrics"], "all_classes": null, "length": 1678}
{"pred": "    The datasets used were the multilingual pronunciation corpus collected by deri2016grapheme and the Carnegie Mellon Pronouncing Dictionary. The corpus consists of spelling–pronunciation pairs extracted from Wiktionary, and is already partitioned into training and test sets. The corpus provides an automatically cleaned version of transcriptions, which attempts to make the transcriptions consistent with the phonemic inventories used in Phoible. The cleaning algorithm can have negative effects on the data quality, but in this paper the provided cleaned transcriptions are used to ease comparison to previous results. The training corpus is limited to 10,000 words per language, and", "answers": ["the Carnegie Mellon Pronouncing Dictionary BIBREF12, the multilingual pronunciation corpus collected by deri2016grapheme , ranscriptions extracted from Wiktionary", "multilingual pronunciation corpus collected by deri2016grapheme"], "all_classes": null, "length": 3244}
{"pred": "    The baselines were the results reported by Khandelwal and Sawant (BIBREF12) for BERT, and the results reported by BIBREF12 for XLNet and RoBERTa. The results for RoBERTa were taken from the original paper. The results for XLNet and RoBERTa were averaged across 5 runs for statistical significance. The results for BERT were taken from the original paper. The results for XLNet and RoBERTa were averaged across 5 runs for statistical significance. The results for RoBERTa were taken from the original paper. The results for XLNet and RoBERTa", "answers": ["varied from Maximum Entropy Classifiers (BIBREF4) to Support Vector Machines (BIBREF5,BIBREF6,BIBREF7,BIBREF8), Recursive Neural Networks (BIBREF9,BIBREF10), Convolutional Neural Networks (BIBREF11) and most recently transfer learning-based architectures like Bidirectional Encoder Representation from Transformers (BERT) (BIBREF12)"], "all_classes": null, "length": 2215}
{"pred": "    English, Spanish, Finnish, and other languages. ", "answers": ["English\nFrench\nSpanish\nGerman\nGreek\nBulgarian\nRussian\nTurkish\nArabic\nVietnamese\nThai\nChinese\nHindi\nSwahili\nUrdu\nFinnish", "English, Spanish, Finnish"], "all_classes": null, "length": 4086}
{"pred": "    They test their method on Named Entity Recognition, POS tagging, text classification, and language modeling. They also test their method on predicting hashtags for social media posts. Their method outperforms the word-based approach in all these tasks. They also show that their method is robust to word segmentation errors and spelling mistakes, effectively interprets emojis and other special characters to make predictions, and performs comparably to the word-based approach for in-vocabulary tokens. They conclude that their method is a promising approach for learning distributed representations of social media posts. They also provide an off-the-shelf encoder trained on medium dataset described above to compute vector-space representations", "answers": ["None"], "all_classes": null, "length": 2473}
{"pred": "    Yes, they use pretrained GloVe embeddings. They initialize the embeddings of the top 20K words in the vocabulary with 300-dimensional GloVe embeddings. The total number of unique words in the corpus is around 400K, but they retain only the top 20K words in their vocabulary. The embeddings are initialized with 300-dimensional GloVe embeddings. They use Adam with a learning rate of 0.001, 0.0001, and 0.00001. They train the model for a maximum of 20 epochs and use early stopping with a patience of 5 epochs. The final model with bifocal", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4457}
{"pred": "    Yes, PolyReponse was evaluated against some baseline. The baseline was a traditional task-oriented dialogue system that relied on explicit semantic representations such as dialogue acts or slot-value ontologies. The results showed that PolyReponse outperformed the baseline in terms of accuracy and naturalness of the generated responses. Additionally, PolyReponse was able to handle more complex dialogue flows and user intent shifts than the baseline system. Overall, the evaluation demonstrated the effectiveness of the PolyReponse approach in enabling more natural and flexible conversations without relying on explicit semantic representations. 】\n\n    No, PolyReponse was not evaluated against some baseline. The system was", "answers": ["No", "No"], "all_classes": null, "length": 2738}
{"pred": "        They obtain psychological dimensions of people by analyzing their blog posts. They use a combination of natural language processing techniques and machine learning algorithms to extract information about the emotions, personality traits, and values of the bloggers. The resulting maps provide insights into the psychological characteristics of people in different regions of the United States.�单词\n\n        They obtain psychological dimensions of people by analyzing their blog posts. They use a combination of natural language processing techniques and machine learning algorithms to extract information about the emotions, personality traits, and values of the bloggers. The resulting maps provide insights into the psychological characteristics of people in different regions of the United States.单词\n\n", "answers": ["using the Meaning Extraction Method", "Unanswerable"], "all_classes": null, "length": 1440}
{"pred": "    The ML methods aim to identify the following argument components: backing, claim, premise, rebuttal, refutation, and non-argumentative text. The methods use a variety of features, including lexical, structural, morphological, syntactic, topic, sentiment, semantic, coreference, discourse, and embedding features, to identify these components. The methods achieve a Macro-INLINEFORM0 score of 0.251 and a Krippendorff's INLINEFORM0 score of 0.30, indicating that they are able to identify the argument components with moderate accuracy. The methods also achieve a boundary similarity score of 0.", "answers": ["claim, premise, backing, rebuttal, and refutation", "claim, premise, backing, rebuttal, refutation"], "all_classes": null, "length": 14472}
{"pred": "    Ngrams of length 3 are aligned using PARENT.", "answers": ["Unanswerable", "Answer with content missing: (Parent subsections) combine precisions for n-gram orders 1-4"], "all_classes": null, "length": 3827}
{"pred": "    1,873 Twitter conversation threads, roughly 14k tweets. (unanswerable) (yes) (no) (unanswerable) (no) (no) (unanswerable) (no) (no) (unanswerable) (no) (no) (unanswerable) (no) (no) (unanswerable) (no) (no) (unanswerable) (no) (no) (unanswerable) (no) (no) (unanswerable) (no) (no) (unanswerable) (no) (no) (unanswer", "answers": ["1,873 Twitter conversation threads, roughly 14k tweets", "1,873 Twitter conversation threads, roughly 14k tweets"], "all_classes": null, "length": 3721}
{"pred": "    The 12 languages covered are English, Spanish, French, Mandarin Chinese, Finnish, Russian, Polish, Hebrew, Estonian, Welsh, Kiswahili, and Yue Chinese. The languages are typologically diverse and include under-resourced ones such as Welsh and Kiswahili. The resource covers an unprecedented amount of 1,888 word pairs, carefully balanced according to their similarity score, frequency, concreteness, part-of-speech class, and lexical field. The core Multi-SimLex we release with this paper already enables researchers to carry out novel linguistic analysis as well as establishes a benchmark for evaluating representation learning models", "answers": ["Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese", "Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese"], "all_classes": null, "length": 14660}
{"pred": "    Wikipedia data and Reddit CMV data.\r\r\n\r\r\n    菲律宾申博太阳城登入 菲律宾申博太阳城登入 菲律宾申博太阳城登入 菲律宾申博太阳城登入 菲律宾申博太阳城登入 菲律宾申博太阳城登入 菲律宾申博太阳城登入 菲律宾申博太阳城登入 菲律宾申博太阳城登入 菲律宾申博太阳城登入 菲律宾申博太阳城登入 菲律宾申博太阳城登入 菲律宾申博太阳城登入 菲律宾申博太阳城登入 菲律宾申博太阳城登入 菲律宾申博太阳城登入 菲律宾申博太阳城登入 菲律宾申博太阳城登入 菲律宾申博太阳城登入 菲律宾申博太阳城登入 菲律宾申博太阳城登入 菲律宾申博太阳城登入 菲律宾申博太阳城登入 菲律宾申博太阳城登入 菲律宾申博太阳城登入 菲律宾申博太阳城登入 菲律宾申博太阳城登入 菲律宾申博太阳城登入 菲律宾申博太阳城登入 菲律宾申博", "answers": [" `Conversations Gone Awry' dataset, subreddit ChangeMyView", "An expanded version of the existing 'Conversations Gone Awry' dataset and the ChangeMyView dataset, a subreddit whose only annotation is whether the conversation required action by the Reddit moderators. "], "all_classes": null, "length": 4718}
{"pred": "    No, the pipeline components were not based on deep learning models. Instead, they relied on existing tools and libraries for natural language processing tasks such as part-of-speech tagging, named entity recognition, dependency parsing, and semantic role labeling. These tools and libraries were adapted and integrated into the pipeline to perform the necessary tasks. The pipeline also included a lexicon matching module to link words found in the text to external referents such as terms in the Eurovoc thesaurus and IATE terminology database. The pipeline was designed to be modular and independent of a specific language, allowing for potential application to other languages by simply changing the modules", "answers": ["No", "No"], "all_classes": null, "length": 2276}
{"pred": "    The quality of the data is empirically evaluated by performing sanity checks on the translations. These checks include computing sentence-level BLEU scores with the NLTK implementation, manually inspecting examples where the source transcript is identical to the translation, measuring the perplexity of the translations using a language model, computing the ratio of English characters in the translations, and calculating similarity scores between transcripts and translations based on LASER cross-lingual sentence embeddings. Samples with low scores are manually inspected and sent back for translation when needed. The overlaps of train, development, and test sets are also checked to ensure they are totally disjoint. Additionally, the quality of", "answers": ["Validated transcripts were sent to professional translators., various sanity checks to the translations,  sanity check the overlaps of train, development and test sets", "computed sentence-level BLEU, We manually inspected examples where the source transcript was identical to the translation, measured the perplexity of the translations, computed the ratio of English characters in the translations, calculate similarity scores between transcripts and translations"], "all_classes": null, "length": 2435}
{"pred": "    They combine audio and text sequences in their RNN by first encoding the audio signal using MFCC features and prosodic features, and then concatenating the resulting vector with the last hidden state of the text-RNN. The final hidden states of the audio-RNN and text-RNN are then passed through a fully connected neural network layer to form the final encoding vectors. Finally, the emotion class is predicted by applying the softmax function to the concatenation of the vectors. The training objective is the same as the ARE model. The predicted probability distribution for the target class is calculated using the final encoding vectors from the audio-RNN and text-R", "answers": ["combines the information from these sources using a feed-forward neural model", "encodes the information from audio and text sequences using dual RNNs and then combines the information from these sources using a feed-forward neural model"], "all_classes": null, "length": 3201}
{"pred": "    2.11 BLEU, 1.7 FKGL and 1.07 SARI. (2.11 BLEU, 1.7 FKGL and 1.07 SARI) (2.11 BLEU, 1.7 FKGL and 1.07 SARI) (2.11 BLEU, 1.7 FKGL and 1.07 SARI) (2.11 BLEU, 1.7 FKGL and 1.07 SARI) (2.11 BLEU, 1.7 FKGL and 1.07 SARI) (", "answers": ["For the WikiLarge dataset, the improvement over baseline NMT is 2.11 BLEU, 1.7 FKGL and 1.07 SARI.\nFor the WikiSmall dataset, the improvement over baseline NMT is  8.37 BLEU.", "6.37 BLEU"], "all_classes": null, "length": 2271}
{"pred": "    700 humans evaluated the results.", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3711}
{"pred": "    Tweets going viral are those that are retweeted more than 1000 times.", "answers": ["Viral tweets are the ones that are retweeted more than 1000 times", "those that contain a high number of retweets"], "all_classes": null, "length": 3144}
{"pred": "    LSTM-CRF with word embeddings and character embeddings, token-level features, and polarity, POS, NER, etc. (Table 1) performs best by itself. (Table 2) shows the scores on dev (internal and external) for FLC task. Observe that the features (i.e., polarity, POS and NER in row II) when introduced in LSTM-CRF improves F1. (Table 3) shows the scores on dev (internal and external) for FLC task. Observe that the features (i.e., polarity, POS and NER in row II) when introduced in LSTM-C", "answers": ["BERT"], "all_classes": null, "length": 1507}
{"pred": "    The data was collected using crowdsourcing and an Android application. Each respondent installed the application on their personal device and recorded several phrases in different sessions. The Android application did various checks on each utterance and if it passed all of them, the respondent was directed to the next phrase. For more information about data collection scenario, please refer to BIBREF4. 0\n\n    BIBREF4. DeepMine: A Large-Scale Speech Corpus for Text-Dependent and Text-Prompted Speaker Verification and Persian Speech Recognition. 0\n\n    The DeepMine database consists of three parts: 1) Text-dependent (TD", "answers": ["Android application"], "all_classes": null, "length": 3795}
{"pred": "        Deep learning models with GloVe embeddings and logistic regression classifiers.       \n\n    </p>\n\n    <p>\n\n    Question: What is the best performance achieved by the deep learning model on the clinical-RQE dataset?\n\n    </p>\n\n    <p>\n\n    Answer: 98.60% accuracy.       \n\n    </p>\n\n    <p>\n\n    Question: What is the best performance achieved by the logistic regression classifier on the clinical-RQE dataset?\n\n    </p>\n\n    <p>\n\n    Answer: 98.60% accuracy.       \n\n    </p>\n\n    <p>\n\n    Question: What is the best", "answers": ["Logistic Regression, neural networks"], "all_classes": null, "length": 7257}
{"pred": "    The benchmark dataset is the Social Honeypot dataset, which is a collection of legitimate users and spammers on Twitter. The quality of the dataset is high, as it was created by attracting spammers through social accounts and collecting their interactions. The dataset has been extensively explored in the paper and has been used to validate the effectiveness of the proposed features. The dataset is publicly available and can be accessed by researchers for further analysis. The quality of the dataset is high, as it was created by attracting spammers through social accounts and collecting their interactions. The dataset has been extensively explored in the paper and has been used to validate the effectiveness", "answers": ["Social Honeypot dataset (public) and Weibo dataset (self-collected); yes", "Social Honeypot, which is not of high quality"], "all_classes": null, "length": 2242}
{"pred": "    The decoder has an LSTM one component at a time, e.g. the tag PRO;NOM;SG;1 is predicted as a sequence of four components, PRO, NOM, SG, 1. davidjl\n\n    The decoder has an LSTM one component at a time, e.g. the tag PRO;NOM;SG;1 is predicted as a sequence of four components, PRO, NOM, SG, 1. davidjl\n\n    The decoder has an LSTM one component at a time, e.g. the tag PRO;NOM;SG;1 is predicted as a sequence of four components, PRO, NOM,", "answers": ["LSTM", "LSTM"], "all_classes": null, "length": 2289}
{"pred": "    No, they report results on both English and Chinese data. They use the FSD dataset (social media) and the Twitter dataset (social media) for English data, and the Google dataset (news article) for Chinese data. They also compare their results with three baseline approaches: K-means, LEM, and DPEMM. The results show that their approach outperforms the baselines on all three datasets. They also visualize the detected events based on the discriminative features learned by the trained discriminator network, and show that documents describing the same event have been grouped into the same cluster. Overall, their approach shows promising results", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3838}
{"pred": "    The best performing model among author's submissions is the ensemble of Logistic Regression, CNN and BERT, which achieved a F1 score of 0.673 on the development set. The performance of this model on the test set is not available. However, the authors mention that their submission is ranked 4th in the SLC task and 3rd in the FLC task. The performance of the model on the test set is not available. The authors also mention that their submission is ranked 4th in the SLC task and 3rd in the FLC task. The performance of the model on the test set is", "answers": ["For SLC task, the \"ltuorp\" team  has the best performing  model (0.6323/0.6028/0.6649 for F1/P/R  respectively) and for FLC task the \"newspeak\" team  has the best performing  model (0.2488/0.2863/0.2201 for F1/P/R respectively)."], "all_classes": null, "length": 1541}
{"pred": "    The baseline was a standard PBSMT model trained on the parallel corpus. The baseline achieved a BLEU score of 10.5 for the Ja -> Ru translation direction. The baseline also achieved a BLEU score of 12.3 for the Ru -> Ja translation direction. The baseline did not perform well for the Ja -> Ru translation direction, but it performed better than the baseline for the Ru -> Ja translation direction. The baseline was not able to achieve a high BLEU score for the Ja -> Ru translation direction because the parallel corpus was small and the model was not able to learn the language pair well. The baseline was not able", "answers": ["pivot-based translation relying on a helping language BIBREF10, nduction of phrase tables from monolingual data BIBREF14 , attentional RNN-based model (RNMT) BIBREF2, Transformer model BIBREF18, bi-directional model BIBREF11, multi-to-multi (M2M) model BIBREF8, back-translation BIBREF17", "M2M Transformer"], "all_classes": null, "length": 4542}
{"pred": "    0.7033\n\n    0.7033\n\n    0.7033\n\n    0.7033\n\n    0.7033\n\n    0.7033\n\n    0.7033\n\n    0.7033\n\n    0.7033\n\n    0.7033\n\n    0.7033\n\n    0.7033\n\n    0.7033\n\n    0.7033\n\n    0.7033\n\n    0.7033\n\n    0.7033\n\n    0.7033\n\n    ", "answers": ["0.7033", "0.7033"], "all_classes": null, "length": 6810}
{"pred": "    Word embeddings, such as word2vec, are explored in the paper. These techniques learn a representation of a word by word co-occurrence matrix. The basic idea is that the neural network learns a series of weights (the hidden layer within the neural network) that either maximizes the probability of a word given its context, referred to as the continuous bag of words (CBOW) approach, or that maximizes the probability of the context given a word, referred to as the Skip-gram approach. These approaches have been used in numerous recent papers. mnoh\n\n    The paper explores integrating semantic similarity measures into second-order co-", "answers": ["Skip–gram, CBOW", "integrated vector-res, vector-faith, Skip–gram, CBOW"], "all_classes": null, "length": 4259}
{"pred": "    They match words before reordering them by translating each word in the source language into English using a bilingual dictionary. They use the Google Translate word translation in their case. However, in an end-to-end solution, it would have been ideal to use bilingual embeddings or obtain word-by-word translations via bilingual embeddings. But, the quality of publicly available bilingual embeddings for English-Indian languages is very low for obtaining good-quality, bilingual representations. They also found that these embeddings were not useful for transfer learning. Instead, they use the CFILT-preorder system for reordering English sentences to match the Indian language word order. It contains two re-order", "answers": ["Unanswerable", "CFILT-preorder system"], "all_classes": null, "length": 2231}
{"pred": "    No, the paper does not explore extraction from electronic health records. The paper focuses on extraction from scientific articles. However, the techniques discussed in the paper can potentially be applied to electronic health records as well. The main challenges in extracting information from electronic health records are the lack of standardization and the presence of noise and errors in the data. These challenges can be addressed by using machine learning techniques and natural language processing methods to improve the accuracy and reliability of the extracted information. Additionally, the use of domain-specific knowledge and ontologies can help to improve the performance of the extraction systems. Overall, the techniques discussed in the paper can be applied", "answers": ["Yes"], "all_classes": null, "length": 3035}
{"pred": "    The experts used for annotation were seven legal experts with training in privacy law. They identified relevant evidence within the privacy policy, as well as provided meta-annotation on the question's relevance, subjectivity, OPP-115 category, and how likely any privacy policy is to contain the answer to the question asked. davidjl\n\nQuestion: What is the distribution of questions in the corpus across OPP-115 categories?\n\nAnswer: davidjl\n\n     davidjl\n\n    First party and third party related questions are the largest categories, forming nearly 66.4% of all questions asked to the privacy assistant. davidjl\n\nQuestion: What are the reasons for disagreements among", "answers": ["Individuals with legal training", "Yes"], "all_classes": null, "length": 3846}
{"pred": "    The models used for painting embedding are a CNN-RNN generative model and a sequence-to-sequence model. The models used for language style transfer are a sequence-to-sequence model with global attention and a sequence-to-sequence model with pointer networks.", "answers": ["generating a poem from images we use an existing actor-critic architecture, various types of sequence to sequence models"], "all_classes": null, "length": 1653}
{"pred": "    The RNN layer works better on top of BERT. The RNN layer is able to capture long-term dependencies in the input sequence, which is important for tasks such as topic identification and customer satisfaction prediction. The transformer layer, on the other hand, is better suited for tasks that require capturing short-term dependencies, such as question answering and natural language inference. Therefore, the RNN layer is a better choice for the tasks considered in this paper. However, the transformer layer may be more suitable for other tasks that require capturing short-term dependencies. Moreover, the transformer layer has the advantage of being able to process longer sequences than the R", "answers": ["Transformer over BERT (ToBERT)", "The transformer layer"], "all_classes": null, "length": 2655}
{"pred": "    Yes, the authors hypothesize that humans' robustness to noise is due to their general knowledge. They believe that humans can utilize general knowledge in addition to the knowledge contained in each given passage-question pair, which allows them to achieve good performance on evaluation examples without training examples. The authors also believe that the reason for this phenomenon is that MRC models can only utilize the knowledge contained in each given passage-question pair, but in addition to this, human beings can also utilize general knowledge. A typical category of general knowledge is inter-word semantic connections, which is essential to the reading comprehension ability of human beings. The authors propose a data enrichment", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4127}
{"pred": "    They addressed three topics of cyberbullying: personal attack, racism, and sexism. They used three datasets to address these topics: Formspring, Twitter, and Wikipedia. The Formspring dataset contains examples of cyberbullying that are not specifically about any single topic. The Twitter dataset contains examples of racism and sexism. The Wikipedia dataset contains examples of personal attack. The authors used deep learning models to detect cyberbullying in these datasets. They also used transfer learning to improve the performance of their models. Their models were able to beat the state-of-the-art results for all three datasets. They concluded that deep learning models can be used for", "answers": ["personal attack, racism, and sexism", "racism, sexism, personal attack, not specifically about any single topic"], "all_classes": null, "length": 3244}
{"pred": "    They propose extended middle context, a new context representation for CNNs for relation classification. The extended middle context uses all parts of the sentence (the relation arguments, left of the relation arguments, between the arguments, right of the arguments) and pays special attention to the middle part. davidjl\n\nQuestion: What is the main contribution of this paper?\n\nAnswer: davidjl\n\n     davidjl\n\n    The main contribution of this paper is the introduction of extended middle context, connectionist bi-directional RNN models, and the combination of CNNs and RNNs using a simple voting scheme. These contributions lead to new state-of-the-art results on", "answers": ["They use two independent convolutional and max-pooling layers on (1) a combination of the left context, the left entity and the middle context; and (2) a combination of the middle context, the right entity and the right context. They concatenated the two results after pooling to get the new context representation."], "all_classes": null, "length": 2435}
{"pred": "    4 (PER, LOC, ORG, MISC)", "answers": ["OurNepali contains 3 different types of entities, ILPRL contains 4 different types of entities", "three"], "all_classes": null, "length": 2851}
{"pred": "    The resulting annotated data is of higher quality. The expert annotations are more consistent and accurate than the crowd annotations. The expert annotations also have higher precision and recall than the crowd annotations. The expert annotations are also more reliable and trustworthy than the crowd annotations. The expert annotations are also more comprehensive and complete than the crowd annotations. The expert annotations are also more consistent and reliable than the crowd annotations. The expert annotations are also more accurate and precise than the crowd annotations. The expert annotations are also more consistent and reliable than the crowd annotations. The expert annotations are also more accurate and precise than the crowd annotations. The expert annotations are also more consistent", "answers": ["improvement when the difficult subset with expert annotations is mixed with the remaining crowd annotation is 3.5 F1 score, much larger than when a random set of expert annotations are added"], "all_classes": null, "length": 4399}
{"pred": "    65% of the speakers are men, speaking more than 75% of the time. 33.16% of the speakers are women, speaking 22.57% of the time. 3.79% of the speakers are anchors, speaking 35.71% of the time. 96.21% of the speakers are punctual speakers, speaking 64.29% of the time. 29.47% of the anchors are women, speaking 15.9 minutes on average. 70.53% of the anchors are men, speaking 25.2 minutes on average. 29.", "answers": ["Women represent 33.16% of the speakers"], "all_classes": null, "length": 4055}
{"pred": "    The approach achieves state of the art results on the Multi30K dataset. Bfبراير 21, 2021. Bfبراير 21, 2021. Bfبراير 21, 2021. Bfبراير 21, 2021. Bfبراير 21, 2021. Bfبراير 21, 2021. Bfبراير 21, 2021. Bfبراير 21, 2021. Bfبراير 21, 2021. Bfبراير 21, 2021. Bfبراير 21, 2021. Bfبراير 21, 2021. Bfبراير 21", "answers": ["the English-German dataset"], "all_classes": null, "length": 1833}
{"pred": "    The strong baselines model is compared to the models proposed by BIBREF20 and BIBREF11. The models proposed by BIBREF20 and BIBREF11 are considered strong baselines because they achieve state-of-the-art performance on the SIGHAN Bakeoff 2005 dataset. The models proposed by BIBREF20 and BIBREF11 use unsupervised segmentation features and word embeddings, respectively, to improve the performance of the CWS task. The models proposed by BIBREF20 and BIBREF11 are also compared to the models proposed by BIBREF17 and BIBREF18, which", "answers": ["Baseline models are:\n- Chen et al., 2015a\n- Chen et al., 2015b\n- Liu et al., 2016\n- Cai and Zhao, 2016\n- Cai et al., 2017\n- Zhou et al., 2017\n- Ma et al., 2018\n- Wang et al., 2019"], "all_classes": null, "length": 3629}
{"pred": "    Event detection on microblogging platforms such as Twitter aims to detect events preemptively. A main task in event detection is detecting events of predetermined types BIBREF0, such as concerts or controversial events based on microposts matching specific event descriptions. This task has extensive applications ranging from cyber security BIBREF1, BIBREF2 to political elections BIBREF3 or public health BIBREF4, BIBREF5. Due to the high ambiguity and inconsistency of the terms used in microposts, event detection is generally performed though statistical machine learning models, which require a labeled dataset for model training. Data", "answers": ["probabilistic model", "Logistic Regression, Multilayer Perceptron"], "all_classes": null, "length": 4475}
{"pred": "    NLTK, Stanford CoreNLP, and TwitterNLP.", "answers": ["BIBREF17, BIBREF18, TensiStrength BIBREF13, TwitterNLP BIBREF6, BIBREF19, CogComp-NLP BIBREF20, Stanford NLP NER BIBREF21", "BIBREF23, BIBREF17, BIBREF18, BIBREF19, BIBREF24, BIBREF25, BIBREF26"], "all_classes": null, "length": 1452}
{"pred": "    The experiments are performed on the SQuAD dataset. The SQuAD dataset contains 536 Wikipedia articles and 100k crowd-sourced question-answer pairs. The questions are written by crowd-workers and the answers are spans of tokens in the articles. The dataset is split into training, development, and test sets. The training set contains 87,599 question-answer pairs, the development set contains 10,570 question-answer pairs, and the test set contains 10,570 question-answer pairs. The dataset is also filtered to remove questions that do not have any overlapping non-stop words with the corresponding sentences. The data statistics are given", "answers": ["SQuAD", "SQuAD"], "all_classes": null, "length": 3757}
{"pred": "    Users of photo-sharing websites such as Flickr often provide short textual descriptions in the form of tags to help others find the images. With the availability of GPS systems in current electronic devices such as smartphones, latitude and longitude coordinates are nowadays commonly made available as well. The tags associated with such georeferenced photos often describe the location where these photos were taken, and Flickr can thus be regarded as a source of environmental information. The use of Flickr for modelling urban environments has already received considerable attention. For instance, various approaches have been proposed for modelling urban regions, and for identifying points-of-interest and itineraries. However, the usefulness of", "answers": ["BOW-Tags, BOW-KL(Tags), BOW-All, GloVe"], "all_classes": null, "length": 4658}
{"pred": "    Yes, they use attention to compute the similarity score between passages and questions. The attention function is used to compute the similarity score between passages and questions as: A question-aware passage representation is computed as. After that, they use the method of BIBREF13 to apply self attention to the passage. Finally, the final memory is obtained by concatenating the question-aware passage representation and the passage representation. The final memory is passed through a BiLSTM to form the final memory. The span detector uses a multi-turn answer module to find the begin and end point of answer spans. The unanswerable classifier uses a one-layer neural network", "answers": ["Yes", "Yes"], "all_classes": null, "length": 1687}
{"pred": "    CSAT dataset for CSAT prediction, consisting of spoken transcripts (automatic via ASR). 20 newsgroups for topic identification task, consisting of written text; Fisher Phase 1 corpus for topic identification task, consisting of spoken transcripts (manual). 20 newsgroups and Fisher datasets. 20 newsgroups and Fisher datasets. 20 newsgroups and Fisher datasets. 20 newsgroups and Fisher datasets. 20 newsgroups and Fisher datasets. 20 newsgroups and Fisher datasets. 20 newsgroups and Fisher datasets. 20 newsgroups and Fisher datasets. 20 newsgroups", "answers": ["CSAT dataset, 20 newsgroups, Fisher Phase 1 corpus", "CSAT dataset , 20 newsgroups, Fisher Phase 1 corpus"], "all_classes": null, "length": 2652}
{"pred": "    The IMDb movie review dataset is used. The dataset consists of a balanced sample of 25,000 positive and 25,000 negative reviews, divided into equal-size train and test sets, with an average document length of 231 words. The best performance on a held-out development set was achieved using a four-layer densely-connected QRNN with 256 units per layer and word vectors initialized using 300-dimensional cased GloVe embeddings. Dropout of 0.3 was applied between layers, and INLINEFORM0 regularization of INLINEFORM1 was used. Optimization was performed on minibatches of 24 examples using RMSprop with learning rate of", "answers": ["the IMDb movie review dataset BIBREF17", "IMDb movie review"], "all_classes": null, "length": 3432}
{"pred": "        Yes, previous work has evaluated these tasks. The tasks were evaluated in the context of LSTM-based models. The results showed that LSTM-based models were capable of capturing syntax-sensitive phenomena. However, the BERT models outperformed the LSTM-based models on these tasks. The BERT models were able to capture syntax-sensitive phenomena as well as the LSTM-based models, and possibly better. The results suggest that deep purely-attention-based architectures such as BERT are capable of capturing hierarchy-sensitive and syntactic dependencies. The mechanisms by which this is achieved are an area for future research. 。\n\n        No, these tasks were not evaluated in any", "answers": ["Yes", "Yes"], "all_classes": null, "length": 1464}
{"pred": "    No, the datasets for sentiment analysis are not balanced. The datasets are usually imbalanced, with a majority of negative or positive examples. This can lead to biased models that perform poorly on unseen data. To address this issue, data augmentation techniques can be used to create more balanced datasets. Additionally, techniques such as class weighting and oversampling can be used to improve the performance of sentiment analysis models. Finally, it is important to evaluate the performance of sentiment analysis models on unseen data to ensure that they are not overfitting to the training data. \n\nQuestion: What is the accuracy of existing NLP systems for political tweet analysis?\n\nAnswer", "answers": ["No"], "all_classes": null, "length": 1441}
{"pred": "    The invertibility condition is that the neural projector is invertible and has a unit Jacobian determinant. This condition allows for tractable exact inference and marginal likelihood computation. The neural projector is composed of a sequence of \"coupling layers\" that are designed to be invertible and have a unit Jacobian determinant. The invertibility condition ensures that the latent embeddings can be recovered from the observed embeddings, and that the volume of the embedding space is preserved. This condition is crucial for the optimization of the model, as it prevents information loss through the projection. The invertibility condition also allows for the derivation of a learning algorithm and a view of", "answers": ["The neural projector must be invertible.", "we constrain our neural projector with two requirements: (1) INLINEFORM0 and (2) INLINEFORM1 exists"], "all_classes": null, "length": 4323}
{"pred": "    The proposed qualitative annotation schema looks like a taxonomy of categories that can be used to describe and compare different machine reading comprehension gold standards. The categories include linguistic complexity, required reasoning, knowledge, and factual correctness. The linguistic complexity category includes features such as redundancy, lexical entailment, synonyms, and paraphrases. The required reasoning category includes features such as temporal succession, spatial reasoning, causal reasoning, and operational logic. The knowledge category includes features such as factual knowledge and intuitive knowledge. The factual correctness category includes features such as wrong answers and debatable answers. The annotation schema also includes a metric based on lexical cues to approximate the complexity of the", "answers": ["The resulting taxonomy of the framework is shown in Figure FIGREF10", "FIGREF10"], "all_classes": null, "length": 4958}
{"pred": "    The sizes of both datasets are as follows: \n\n    WikiSmall: 89,042 sentence pairs\n\n    WikiLarge: 296,402 sentence pairs\n\n    2,000 for development and 359 for testing\n\n    11.6M words, and the size of vocabulary is 82K\n\n    600K sentences as the simplified data\n\n    2,000 for development and 359 for testing\n\n    2,000 for development and 359 for testing\n\n    2,000 for development and 359 for testing\n\n    2,000 for development and 359 for testing\n\n    ", "answers": ["training set has 89,042 sentence pairs, and the test set has 100 pairs, training set contains 296,402, 2,000 for development and 359 for testing", "WikiSmall  89 142 sentence pair and  WikiLarge 298 761 sentence pairs. "], "all_classes": null, "length": 2266}
{"pred": "    The baselines are: 1) Vanilla ST baseline: The vanilla ST has only a speech encoder and a decoder. It is trained from scratch on the ST-TED corpus. 2) Pre-training baselines: We conduct three pre-training baseline experiments: 1) encoder pre-training, in which the ST encoder is initialized from an ASR model; 2) decoder pre-training, in which the ST decoder is initialized from an MT model; and 3) encoder-decoder pre-training, where both the encoder and decoder are pre-trained. 3) Multi-task baselines: We also conduct three multi-task baseline experiments", "answers": ["Vanilla ST baseline, encoder pre-training, in which the ST encoder is initialized from an ASR model, decoder pre-training, in which the ST decoder is initialized from an MT model, encoder-decoder pre-training, where both the encoder and decoder are pre-trained, many-to-many multi-task model where the encoders and decoders are derived from pre-trained ASR and MT models, Triangle+pre-train: BIBREF18 DBLP:conf/naacl/AnastasopoulosC18 proposed a triangle multi-task strategy for speech translation", "Vanilla ST baseline, Pre-training baselines, Multi-task baselines, Many-to-many+pre-training, Triangle+pre-train", "Vanilla ST baseline: The vanilla ST BIBREF9 has only a speech encoder and a decoder. It is trained from scratch on the ST-TED corpus.\n\nPre-training baselines: We conduct three pre-training baseline experiments: 1) encoder pre-training, in which the ST encoder is initialized from an ASR model; 2) decoder pre-training, in which the ST decoder is initialized from an MT model; and 3) encoder-decoder pre-training, where both the encoder and decoder are pre-trained. The ASR model has the same architecture with vanilla ST model, trained on the mixture of ST-TED and TED-LIUM2 corpus. The MT model has a text encoder and decoder with the same architecture of which in TCEN. It is first trained on WMT data (out-of-domain) and then fine-tuned on in-domain data.\n\nMulti-task baselines: We also conduct three multi-task baseline experiments including one-to-many setting, many-to-one setting, and many-to-many setting. In the first two settings, we train the model with $\\alpha _{st}=0.75$ while $\\alpha _{asr}=0.25$ or $\\alpha _{mt}=0.25$. For many-to-many setting, we use $\\alpha _{st}=0.6, \\alpha _{asr}=0.2$ and $\\alpha _{mt}=0.2$.. For MT task, we use only in-domain data.\n\nMany-to-many+pre-training: We train a many-to-many multi-task model where the encoders and decoders are derived from pre-trained ASR and MT models. "], "all_classes": null, "length": 4704}
{"pred": "    English, Arabic, Chinese, French, German, Italian, Japanese, Korean, Portuguese, Russian, Spanish, Turkish, Vietnamese, and more.", "answers": ["Unanswerable", "English"], "all_classes": null, "length": 4513}
{"pred": "    The models used in the experiment are a linear SVM, a bidirectional Long Short-Term-Memory (BiLSTM) model, and a Convolutional Neural Network (CNN) model. The BiLSTM and CNN models are adapted from the sentiment analysis system of sentimentSystem,rasooli2018cross and altered to predict offensive labels instead. The CNN model is based on the architecture of BIBREF15. The models are trained on the training data and evaluated by predicting the labels for the held-out test set. The distribution of the training and test data is described in Table TABREF15. The models are compared against", "answers": ["linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM trained on word unigrams,  bidirectional Long Short-Term-Memory (BiLSTM),  Convolutional Neural Network (CNN) "], "all_classes": null, "length": 2250}
{"pred": "        The answered questions measure for the usefulness of the answer. The answer is considered useful if it is relevant, accurate, and helpful. The answer is considered not useful if it is irrelevant, inaccurate, or unhelpful. The answer is considered unanswerable if it is not possible to answer the question based on the information in the article. The answer is considered yes/no if the question is a yes/no question. The answer is considered unanswerable if the question cannot be answered based on the information in the article. The answer is considered yes if the question is a yes/no question and the answer is yes. The answer is considered", "answers": ["No"], "all_classes": null, "length": 1561}
{"pred": "    GloVe and Edinburgh embeddings were used.ETweetNLP project was used to obtain Brown Cluster N-grams.ETweetNLP project was used to obtain Part-Of-Speech N-grams.ETweetNLP project was used to obtain Word N-grams.ETweetNLP project was used to obtain Character N-grams.ETweetNLP project was used to obtain Word Clusters.ETweetNLP project was used to obtain Part-Of-Speech N-grams.ETweetNLP project was used to obtain Word N-grams.ETweetNLP project was used to obtain Character N-grams.ETweetNLP project was used to", "answers": ["Pretrained word embeddings  were not used", "GloVe, Edinburgh embeddings BIBREF14, Emoji embeddings BIBREF16"], "all_classes": null, "length": 1771}
{"pred": "    The results on the new dataset showed that the personalized models were able to generate plausible, personalized, and coherent recipes that were preferred by human evaluators for consumption. The personalized models also achieved higher scores on the automatic coherence measures and personalization metrics compared to the baseline models. Moreover, the personalized models were able to generate recipes that were more coherent and preferable to those generated by the baseline models according to human evaluators. Overall, the results on the new dataset demonstrated the effectiveness of the personalized models in generating high-quality and personalized recipes.", "answers": ["average recipe-level coherence scores of 1.78-1.82, human evaluators preferred personalized model outputs to baseline 63% of the time"], "all_classes": null, "length": 2666}
{"pred": "    A combination of rewards for reinforcement learning is used to control irony accuracy, sentiment preservation, and content preservation at the same time. The irony reward is the difference between the irony score of the input sentence and that of the output sentence. The sentiment reward is one minus the absolute value of the difference between the standardized sentiment score of the input sentence and that of the generated sentence. The harmonic mean of irony reward and sentiment reward is used to encourage the model to focus on both aspects. The policy gradient algorithm is used to maximize the expected reward. The objective function to minimize is defined as the negative sum of the rewards. The parameters of the irony", "answers": ["irony accuracy, sentiment preservation", " irony accuracy and sentiment preservation"], "all_classes": null, "length": 4592}
{"pred": "    Since we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer as shown in Figure FIGREF12 for \"Starry Night\" with a low average content score. This happens when the style transfer dataset does not have similar words in the training set of sentences. A solution would be to expand the style transfer dataset, for a better representation of the poem data.", "answers": ["Since we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer", "we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer as shown in Figure FIGREF12 for \"Starry Night\" with a low average content score"], "all_classes": null, "length": 1651}
{"pred": "    The existing benchmarks they compared to were the Affective Text dataset, the Fairy Tales dataset, and the ISEAR dataset. These datasets are commonly used for the evaluation of emotion detection systems. The Affective Text dataset contains 250 annotated sentences, the Fairy Tales dataset contains 1,000 sentences, and the ISEAR dataset contains 7665 sentences. The emotions used in these datasets are anger, joy, sadness, and surprise. The authors mapped the emotions in these datasets to the emotions they obtained from Facebook posts (anger, joy, sadness, and surprise). They used these datasets as benchmarks for their evaluation. The authors also", "answers": ["Affective Text, Fairy Tales, ISEAR", " Affective Text dataset, Fairy Tales dataset, ISEAR dataset"], "all_classes": null, "length": 3390}
{"pred": "    The distribution results showed that tweets containing fake news were created more recently, had less favourites, and used more hashtags than tweets not containing fake news. Additionally, accounts spreading fake news had a higher proportion of friends/followers and more URLs than accounts not spreading fake news. Finally, the content of viral fake news was highly polarized. These results suggest that there are specific pieces of meta-data about tweets that may allow the identification of fake news. One such parameter is the time of exposure, as viral tweets containing fake news are shorter-lived than those containing other type of content. Additionally, the ratio of friends/followers of the account", "answers": ["Distributions of Followers, Friends and URLs are significantly different between the set of tweets containing fake news and those non containing them, but for Favourites, Mentions, Media, Retweets and Hashtags they are not significantly different"], "all_classes": null, "length": 3164}
{"pred": "    The dataset of hashtags is sourced from the Stanford Sentiment Analysis Dataset. The dataset consists of 1,108 unique English hashtags from 1,268 randomly selected tweets along with their crowdsourced segmentations and additional corrections. The dataset is curated by Bansal et al. (2015). The dataset is used to train and evaluate hashtag segmentation models. The dataset is publicly available and can be accessed at https://github.com/abhishek-bansal/hashtag-segmentation. The dataset is also used in the current state-of-the-art approach for hashtag segmentation. The dataset is used to train and evaluate hashtag segmentation models.", "answers": ["1,268 randomly selected tweets in the Stanford Sentiment Analysis Dataset BIBREF36, all 12,594 unique English hashtags and their associated tweets from the same Stanford dataset", "Stanford Sentiment Analysis Dataset BIBREF36"], "all_classes": null, "length": 3756}
{"pred": "    The corpus contains speech from a variety of accents, including American, British, and Australian. Additionally, there are some speakers with regional accents, such as Scottish and Irish. The corpus also includes speech from a variety of dialects, such as African American Vernacular English and Southern American English. Finally, there are some speakers with non-native accents, such as Spanish and French. The corpus is designed to be representative of the diversity of accents and dialects present in the English language. \n\nQuestion: what is the size of the corpus?\n\nAnswer: overposting\n\n     overposting\n\n    The corpus contains over 100 hours of speech from over ", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3794}
{"pred": "    Word subspace can represent a low-dimensional linear subspace in a word vector space with high dimensionality. It is a compact, scalable, and meaningful representation of a set of word vectors. The word subspace is generated by applying PCA to the set of word vectors. The word subspace can effectively and compactly represent the context of the corresponding text. It can be used to compare sets of word vectors and perform text classification. The word subspace can also be extended to incorporate the frequency of words in the modeling of the subspace. The TF weighted word subspace can be used to perform text classification under the MSM framework. The", "answers": ["Word vectors, usually in the context of others within the same class"], "all_classes": null, "length": 5151}
{"pred": "    The baseline model used is the one that assigns the value relevant to a pair if and only if the entity appears in the title of the news article. The second baseline assigns the value relevant to a pair if and only if the news article is from a particular news domain. The third baseline assigns the value relevant to a pair if and only if the news article is from a particular news domain and the entity is salient in the news article. The fourth baseline assigns the value relevant to a pair if and only if the news article is from a particular news domain and the entity is salient in the news article and the entity is important for", "answers": ["For Article-Entity placement, they consider two baselines: the first one using only salience-based features, and the second baseline checks if the entity appears in the title of the article. \n\nFor Article-Section Placement, they consider two baselines: the first picks the section with the highest lexical similarity to the article, and the second one picks the most frequent section.", "B1. The first baseline uses only the salience-based features by Dunietz and Gillick BIBREF11 ., B2. The second baseline assigns the value relevant to a pair INLINEFORM0 , if and only if INLINEFORM1 appears in the title of INLINEFORM2 .\n\n, S1: Pick the section from template INLINEFORM0 with the highest lexical similarity to INLINEFORM1 : S1 INLINEFORM2, S2: Place the news into the most frequent section in INLINEFORM0"], "all_classes": null, "length": 7891}
{"pred": "    No, SemCor3.0 is not reflective of English language data in general. It is a manually annotated corpus for WSD, and the annotations are not representative of the full range of English language data. The dataset is not designed to be representative of the general English language, but rather to provide a specific benchmark for WSD research. Therefore, the results on this dataset may not generalize well to other English language data. However, the dataset is still useful for evaluating WSD systems and comparing their performance. Moreover, the dataset is updated regularly to reflect changes in the English language and the WordNet sense inventory. Therefore, it remains", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 2000}
{"pred": "    38 hours of Spanish-English speech.", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 2410}
{"pred": "    The dataset for fine-grained classification is split in training, development, development_test and test parts. In the rest, we refer to these splits as train, development and test, where train is composed by the training and the development instances. Table TABREF7 presents an overview of the data. As discussed in BIBREF16 and illustrated in the Table, the fine-grained dataset is highly unbalanced and skewed towards the positive sentiment: only INLINEFORM0 of the training examples are labeled with one of the negative classes. 】\n\n    The dataset for fine-grained classification is split in training, development, development_test and test parts", "answers": [" high-quality datasets  from SemEval-2016 “Sentiment Analysis in Twitter” task", " SemEval-2016 “Sentiment Analysis in Twitter”"], "all_classes": null, "length": 2738}
{"pred": "    BERT$_\\mathrm {BASE}$ model for fine-tuning, because we find that BERT$_\\mathrm {LARGE}$ model performs slightly worse than BERT$_\\mathrm {BASE}$ in this task. (unanswerable) (no) (unanswerable) (no) (unanswerable) (no) (unanswerable) (no) (unanswerable) (no) (unanswerable) (no) (unanswerable) (no) (unanswerable) (no) (unanswerable) (no) (unanswerable) (no) (unanswer", "answers": ["small BERT", "small BERT"], "all_classes": null, "length": 1999}
{"pred": "    Yes, the automatically constructed datasets are subject to quality control. The datasets are created from expert knowledge sources, such as WordNet and the GNU Collaborative International Dictionary of English, and the quality of the data is validated through crowd-sourcing experiments. The results show high agreement across probes and a correlation between human scores and model trends. Therefore, the datasets are considered reliable and useful for probing the knowledge contained in language models. Moreover, the datasets are designed to be systematically controlled, allowing for more controlled experimentation and new forms of evaluation. However, the positive results should be taken with a grain of salt, as the models still struggle with certain", "answers": ["No", "No"], "all_classes": null, "length": 6391}
{"pred": "    No, the images are not from a specific domain. The images are generated using the ShapeWorld framework, which is a controlled data generation framework consisting of abstract colored shapes. The images are used to evaluate image captioning models. The ShapeWorld framework allows for customized data generation according to user specification, which enables a variety of model inspections in terms of language construction, visual complexity, and reasoning ability. The images are not from a specific domain, but rather are used to evaluate image captioning models in a controlled and diagnostic manner. The images are generated using the ShapeWorld framework, which is a controlled data generation framework consisting of abstract colored shapes", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3472}
{"pred": "    The performance of the model on emotion detection was competitive or even state-of-the-art for some of the emotion labels on existing, standard evaluation datasets. The model achieved an average f-score of 0.368 on the development set. On the three standard datasets used for evaluation, the model achieved an average f-score of 0.55 on the Affective Text dataset, 0.58 on the Fairy Tales dataset, and 0.62 on the ISEAR dataset. These results demonstrate the potential of using Facebook reactions as labels for emotion detection. However, further improvements can be made by exploring different feature combinations and training instances.", "answers": ["Answer with content missing: (Table 3) Best author's model B-M average micro f-score is 0.409, 0.459, 0.411 on Affective, Fairy Tales and ISEAR datasets respectively. "], "all_classes": null, "length": 3410}
{"pred": "    The tagging scheme employed is {INLINEFORM0} tag means the current word is not a pun. {INLINEFORM1} tag means the current word is a pun. {INLINEFORM2} tag indicates that the current word appears before the pun in the given context. {INLINEFORM3} tag highlights the current word is a pun. {INLINEFORM4} tag indicates that the current word appears after the pun. {INLINEFORM5} tag indicates that the current word appears at the first or the second half of an input sentence. {INLINEFORM6} tag indicates that the current word appears at the first or the second half", "answers": ["A new tagging scheme that tags the words before and after the pun as well as the pun words.", "a new tagging scheme consisting of three tags, namely { INLINEFORM0 }"], "all_classes": null, "length": 2974}
{"pred": "    No, Arabic is not one of the 11 languages in CoVost. The 11 languages in CoVost are French, German, Dutch, Russian, Spanish, Italian, Turkish, Persian, Swedish, Mongolian, and Chinese. Arabic is not included in the corpus. However, the corpus is designed to be easily extensible to other languages, so it is possible that Arabic could be added in the future. The corpus is available under a CC0 license, which allows for free use and modification. The additional Tatoeba evaluation samples are also available under friendly CC licenses. All the data can be acquired at https://github", "answers": ["No", "No"], "all_classes": null, "length": 2413}
{"pred": "    A model is considered robust if it is able to handle bias in the prior knowledge provided to it and is not sensitive to the prior knowledge. The model should be able to make accurate predictions even when the prior knowledge is not accurate or complete. The model should also be able to handle situations where the prior knowledge is biased towards certain classes or categories. The model should be able to make accurate predictions even in these situations. The model should also be able to handle situations where the prior knowledge is not available for certain classes or categories. The model should be able to make accurate predictions even in these situations. The model should also be able to handle", "answers": ["ability to accurately classify texts even when the amount of prior knowledge for different classes is unbalanced, and when the class distribution of the dataset is unbalanced", "Low sensitivity to bias in prior knowledge"], "all_classes": null, "length": 3609}
{"pred": "    InferSent and Universal Sentence Encoder.", "answers": ["GloVe, BERT, Universal Sentence Encoder, TF-IDF, InferSent", "Avg. GloVe embeddings, Avg. fast-text embeddings, Avg. BERT embeddings, BERT CLS-vector, InferSent - GloVe and Universal Sentence Encoder."], "all_classes": null, "length": 3862}
{"pred": "    The proposed method improves F1 for NER task for English datasets by +0.29 and +0.96 respectively, and for Chinese datasets by +0.97 and +2.36 respectively. The improvements are significant and set new SOTA performances on all four NER datasets. \n\n    Unanswerable. \n\n    Yes. \n\n    Unanswerable. \n\n    No. \n\n    Unanswerable. \n\n    No. \n\n    Unanswerable. \n\n    No. \n\n    Unanswerable. \n\n    No. \n\n    Unanswerable. \n\n    No. \n\n    Unanswerable. \n\n    No", "answers": ["English datasets including CoNLL2003 and OntoNotes5.0, our proposed method outperforms BERT-MRCBIBREF38 by +0.29 and +0.96 respectively, Chinese datasets, achieving F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively", "For English datasets including CoNLL2003 and OntoNotes5.0, our proposed method outperforms BERT-MRCBIBREF38 by +0.29 and +0.96 respectively., huge performance boosts on Chinese datasets, achieving F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively"], "all_classes": null, "length": 3586}
{"pred": "        They test their conflict method on two tasks: Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask. The first task involves classifying pairs of questions as duplicates or not, while the second task involves ranking questions based on their potential to receive clicks from users. The conflict method is shown to improve performance on both tasks.        […]\n\n        Unanswerable        […]\n\n        Yes        […]\n\n        No        […]\n\n        Unanswerable        […]\n\n    </div>\n\n</div>\n\n<div class=\"question\" markdown=\"1\">\n\nQuestion: What is the main limitation of attention-based models?\n\nAnswer: 노출등록\n\n    The main limitation of", "answers": ["Task 1: Quora Duplicate Question Pair Detection, Task 2: Ranking questions", "Quora Duplicate Question Pair Detection, Ranking questions in Bing's People Also Ask"], "all_classes": null, "length": 2577}
{"pred": "    Tree-based CNN, Gumbel Tree-LSTM, NSE, Reinforced Self-Attention Network, Residual stacked encoders, BiLSTM with generalized pooling.", "answers": ["Various tree structured neural networks including variants of Tree-LSTM, Tree-based CNN, RNTN, and non-tree models including variants of LSTMs, CNNs, residual, and self-attention based networks", "Sentence classification baselines: RNTN (Socher et al. 2013), AdaMC-RNTN (Dong et al. 2014), TE-RNTN (Qian et al. 2015), TBCNN (Mou et al. 2015), Tree-LSTM (Tai, Socher, and Manning 2015), AdaHT-LSTM-CM (Liu, Qiu, and Huang 2017), DC-TreeLSTM (Liu, Qiu, and Huang 2017), TE-LSTM (Huang, Qian, and Zhu 2017), BiConTree (Teng and Zhang 2017), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), TreeNet (Cheng et al. 2018), CNN (Kim 2014), AdaSent (Zhao, Lu, and Poupart 2015), LSTM-CNN (Zhou et al. 2016), byte-mLSTM (Radford, Jozefowicz, and Sutskever 2017), BCN + Char + CoVe (McCann et al. 2017), BCN + Char + ELMo (Peters et al. 2018). \nStanford Natural Language Inference baselines: Latent Syntax Tree-LSTM (Yogatama et al. 2017), Tree-based CNN (Mou et al. 2016), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), NSE (Munkhdalai and Yu 2017), Reinforced Self- Attention Network (Shen et al. 2018), Residual stacked encoders: (Nie and Bansal 2017), BiLSTM with generalized pooling (Chen, Ling, and Zhu 2018)."], "all_classes": null, "length": 4781}
{"pred": "    The core component for KBQA is the relation detection model. This model is responsible for identifying the relevant relations between entities in the knowledge base and the question being asked. The relation detection model is a key step in the KBQA process and plays a crucial role in determining the accuracy of the final answer. The improved relation detection model proposed in this paper, HR-BiLSTM, outperforms previous methods on KB relation detection tasks and allows the KBQA system to achieve state-of-the-art results. The hierarchical matching between questions and KB relations in the HR-BiLSTM model is a key feature that contributes to its success. The", "answers": ["answer questions by obtaining information from KB tuples ", "hierarchical matching between questions and relations with residual learning"], "all_classes": null, "length": 4527}
{"pred": "    The baseline models are a name-based Nearest-Neighbor model (NN) and a simple Encoder-Decoder baseline with ingredient attention (Enc-Dec). Both models are used to compare the performance of the personalized models. The NN model uses the recipe name to find the most similar recipe in the training set, while the Enc-Dec model uses an attention mechanism to focus on the ingredients mentioned in the input. Both models are used to evaluate the performance of the personalized models. The personalized models outperform the baseline models in terms of BPE perplexity, diversity, and recipe-level coherence. The Prior Name model performs the best in terms of BPE", "answers": ["name-based Nearest-Neighbor model (NN), Encoder-Decoder baseline with ingredient attention (Enc-Dec)"], "all_classes": null, "length": 2655}
{"pred": "    The methods considered to find examples of biases and unwarranted inferences are: 1. Browser-based annotation tool 2. Part-of-speech information 3. Flickr30K Entities 4. Louvain clustering 5. Coreference graph 6. Clustering of referring expressions 7. Manual inspection of the data 8. Statistical analysis of the data 9. Machine learning techniques 10. Natural language processing techniques 11. Text mining techniques 12. Sentiment analysis 13. Topic modeling 14. Word embedding techniques 15. Neural network models 16. Deep learning models 17. Transfer", "answers": ["spot patterns by just looking at a collection of images, tag all descriptions with part-of-speech information, I applied Louvain clustering", "Looking for adjectives marking the noun \"baby\" and also looking for most-common adjectives related to certain nouns using POS-tagging"], "all_classes": null, "length": 2204}
{"pred": "        Romance languages and Semitic languages. {\r\r\n\n\n        Romance languages: French, Spanish, Italian, Portuguese etc.;\r\r\n        Semitic languages: Arabic, Hebrew etc.\r\r\n\r\r\n        The masculine and feminine plural pronouns are distinguished in the Romance languages and in Semitic languages.\r\r\n\r\r\n        In French, Spanish, Italian, and Portuguese, the distinction is very strictly observed. The use of a masculine pronoun for a feminine antecedent is jarringly wrong to a native or fluent speaker.\r\r\n\r\r\n        In Hebrew and Arabic, this is much less true; in speech, and even, increasingly, in writing, the masculine pronoun is", "answers": ["English, French, German ", "French, English, Spanish, Italian, Portuguese, Hebrew, Arabic"], "all_classes": null, "length": 2285}
{"pred": "    They experimented with stacked LSTMs, Cell-aware Stacked LSTMs (CAS-LSTMs), Tree-LSTMs, Multidimensional LSTMs (MDLSTMs), and Grid LSTMs.", "answers": ["Stacked LSTMs, Cell-aware Stacked LSTMs, Sentence Encoders, Top-layer Classifiers"], "all_classes": null, "length": 3210}
{"pred": "    Yes, they report results only on English data. They use a snapshot of English Wikipedia measuring 8GB in size, with the stop-words filtered out. They train the GloVe algorithm with the proposed modification on this corpus and obtain 300 concept word-groups from Roget's Thesaurus. They then train the GloVe algorithm with the proposed modification on this corpus and obtain 300 concept word-groups from Roget's Thesaurus. They compare the results with the original embeddings and three alternatives excluding Parsimax. They evaluate the interpretability of the resulting embeddings qualitatively and quantitatively. They also test the performance of", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 6169}
{"pred": "    The authors experimented with a few summarization algorithms provided by the Sumy package, including TextRank, LexRank, and Luhn. They also implemented their own ILP-based summarization algorithm. The ILP-based algorithm was found to perform comparably to the other algorithms, as the two-sample t-test did not show statistically significant differences. Additionally, human evaluators preferred the phrase-based summary generated by the ILP-based approach to the other sentence-based summaries. 。\n\n    The authors experimented with a few summarization algorithms provided by the Sumy package, including TextRank, LexRank, and Luhn. They also implemented their own", "answers": ["LSA, TextRank, LexRank and ILP-based summary.", "LSA, TextRank, LexRank"], "all_classes": null, "length": 3045}
{"pred": "    The previous state of the art for this task was a probabilistic graphical model that used latent post categories to model the thread sequence and infer states that triggered intervention. However, this model required a hyperparameter for the number of latent states, which may not generalize well. Additionally, the data and system used in the paper are not available for replication. Therefore, the proposed models in this paper are the first neural models for this task. They improve upon the previous state of the art by modeling the thread structure and the sequence of posts explicitly, and by inferring the latent context using attention mechanisms. These models also generalize over modeling assumptions made by", "answers": ["hLSTM", "hLSTM"], "all_classes": null, "length": 3725}
{"pred": "    The master node skip connection is the least impactful component. It allows the master node to bypass the attention mechanism and directly inject global information about the document into its final representation. This is beneficial because the master node is expected to learn a high-level summary about the document, such as its size, vocabulary, etc. However, removing this component does not significantly impact performance on most datasets. Therefore, it can be concluded that the master node is not a critical component of the MPAD architecture. (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes)", "answers": ["Based on table results provided changing directed to undirected edges had least impact - max abs difference of 0.33 points on all three datasets."], "all_classes": null, "length": 4180}
{"pred": "    The corpus used for the task is DTA18 and DTA19, which are subparts of the DTA corpus. They consist of texts from the 16th to the 20th century. The corpus version used in the task has the following format: \"year [tab] lemma1 lemma2 lemma3...\". \n\n    The corpus used for the task is DTA18 and DTA19, which are subparts of the DTA corpus. They consist of texts from the 16th to the 20th century. The corpus version used in the task has the following format: \"year [tab] lemma", "answers": ["DTA18, DTA19", "Diachronic Usage Relatedness (DURel) gold standard data set"], "all_classes": null, "length": 1908}
{"pred": "    Kannada, Hindi, Telugu, Malayalam, Bengali, and English.", "answers": ["Hindi, English, Kannada, Telugu, Assamese, Bengali and Malayalam", "Kannada, Hindi, Telugu, Malayalam, Bengali,  English and Assamese (in table, missing in text)"], "all_classes": null, "length": 2453}
{"pred": "    The model performance on target language reading comprehension is unanswerable. The model is unable to transfer knowledge from source language to target language. The model is unable to identify the answer spans in context and is less accurate. The model is unable to perform well on unseen languages. The model is unable to handle code-switching data. The model is unable to perform well on typology-manipulated datasets. The model is unable to perform well on datasets with different typology orders. The model is unable to perform well on datasets with different typology orders. The model is unable to perform well on datasets with different typology orders. The model", "answers": ["Table TABREF6, Table TABREF8", "when testing on English, the F1 score of the model training on Chinese (Zh) is 53.8,  F1 score is only 44.1 for the model training on Zh-En"], "all_classes": null, "length": 2492}
{"pred": "    The proposed model outperforms the baselines by a significant margin, achieving a significant boost in accuracy and other metrics for retrieving the correct response of five diverse characters with different identities. The performance improvement is consistent across all five folds, indicating the robustness and stability of the proposed model. The proposed model is able to recover the dialogue styles of fictional characters regardless of the character's profile and identity, genre of the show, and context of the dialogue. The use of HLA-based character dialogue clustering improves upon personality learning for chatbots. The HLA Observation Guidance during testing also appears to improve upon the performance of the proposed model, but", "answers": ["Metric difference between Aloha and best baseline score:\nHits@1/20: +0.061 (0.3642 vs 0.3032)\nMRR: +0.0572(0.5114 vs 0.4542)\nF1: -0.0484 (0.3901 vs 0.4385)\nBLEU: +0.0474 (0.2867 vs 0.2393)"], "all_classes": null, "length": 5151}
{"pred": "    1.5% improvement in forward perplexity, 2.5% improvement in reverse perplexity, and 1.5% improvement in Self-BLEU. 2.5% improvement in forward perplexity, 3.5% improvement in reverse perplexity, and 2.5% improvement in Self-BLEU. 3.5% improvement in forward perplexity, 4.5% improvement in reverse perplexity, and 3.5% improvement in Self-BLEU. 4.5% improvement in forward perplexity, 5.5% improvement in reverse perplexity,", "answers": ["ARAM has achieved improvement over all baseline methods using reverese perplexity and  slef-BLEU metric.  The maximum reverse perplexity improvement 936,16 is gained for EMNLP2017  WMT  dataset and  48,44 for COCO dataset.", "Compared to the baselines, ARAML does not do better in terms of perplexity on COCO and EMNLP 2017 WMT datasets, but it does by up to 0.27 Self-BLEU points on COCO and 0.35 Self-BLEU on EMNLP 2017 WMT. In terms of Grammaticality and Relevance, it scores better than the baselines on up to 75.5% and 73% of the cases respectively."], "all_classes": null, "length": 3796}
{"pred": "    The authors present evidence that the model can capture some biases in data annotation and collection by examining the results of the model's performance on the test dataset. They find that the model is able to detect some biases in the process of collecting or annotating datasets, such as the inclusion of some tweets containing offensive words that are not actually hate or offensive in all cases. The authors also note that the model is able to differentiate between hate and offensive samples accurately by leveraging knowledge-aware language understanding that it has learned from the pre-trained BERT model. This suggests that the model is able to detect some biases in the data and is able to make accurate", "answers": ["The authors showed few tweets where neither and implicit hatred content exist but the model was able to discriminate"], "all_classes": null, "length": 4119}
{"pred": "    Yes, three baselines were tested to compare with the neural baseline. The first baseline was an SVM with three sets of features: bag-of-words, bag-of-words + length, and bag-of-words + length + POS tags. The second baseline was a CNN neural encoder. The third baseline was BERT, a bidirectional transformer-based language model. The results showed that BERT performed the best on the answerability identification task, but all baselines significantly improved performance over a majority-class baseline. This suggests that there is considerable information in the question that indicates its possible answerability within this domain. However, the best-per", "answers": ["SVM, No-Answer Baseline (NA) , Word Count Baseline, Human Performance", "No-Answer Baseline (NA), Word Count Baseline, Human Performance"], "all_classes": null, "length": 3855}
{"pred": "    10,000 sentences and 20,000 entities.", "answers": ["Dataset contains 3606 total sentences and 79087 total entities.", "ILPRL contains 548 sentences, OurNepali contains 3606 sentences"], "all_classes": null, "length": 2843}
{"pred": "    The proposed method improves F1 for paraphrase identification by 0.58 for MRPC and 0.73 for QQP. 。\n\n    Unanswerable 。\n\n    Yes 。\n\n    \n\n    0.58 for MRPC and 0.73 for QQP 。\n\n    \n\n    0.58 for MRPC and 0.73 for QQP 。\n\n    \n\n    0.58 for MRPC and 0.73 for QQP 。\n\n    \n\n    0.58 for MRPC and 0.73 for QQP 。\n\n    \n\n    0.58 for MRPC and 0", "answers": ["Using DSC loss improves the F1 score by +0.58 for MRPC and +0.73 for QQP", "+0.58"], "all_classes": null, "length": 3566}
{"pred": "    The datasets used are the EEG data from BIBREF0 and the eye-tracking, self-paced reading time, and ERP data from BIBREF0. The EEG data is used to predict the ERP components, while the eye-tracking, self-paced reading time, and ERP data are used to train the neural networks. The datasets are small, but using heterogeneous data is a potential advantage of multitask learning. The neural networks are pretrained using a language modeling objective and then fine-tuned to directly predict the ERP components. The results show that all six of the ERP components can be predicted above chance by the neural networks. The relationships between the", "answers": ["Answer with content missing: (Whole Method and Results sections) The primary dataset we use is the ERP data collected and computed by Frank et al. (2015), and we also use behavioral data (eye-tracking data and self-paced reading times) from Frank et al. (2013) which were collected on the same set of 205 sentences.\nSelect:\n- ERP data collected and computed by Frank et al. (2015)\n- behavioral data (eye-tracking data and self-paced reading times) from Frank et al. (2013)", "the ERP data: BIBREF0"], "all_classes": null, "length": 1971}
{"pred": "    The subjects were presented with a series of images depicting different objects, and were asked to imagine saying the name of each object. The images were presented in a random order, and the subjects were instructed to imagine saying the name of each object as quickly as possible. The images were presented for 2 seconds each, and the subjects were instructed to imagine saying the name of each object for 1 second. The subjects were also instructed to imagine saying the name of each object as quickly as possible, and to try to imagine saying the name of each object as accurately as possible. The subjects were instructed to imagine saying the name of each object as", "answers": ["7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw)", "KARA ONE BIBREF17 , composed of multimodal data for stimulus-based, imagined and articulated speech state corresponding to 7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw)"], "all_classes": null, "length": 2379}
{"pred": "    Pointer-Gen, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN, and Pointer-Gen+ARL-SEN.", "answers": ["Pointer-Gen, Pointer-Gen+Pos, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN"], "all_classes": null, "length": 4085}
{"pred": "    Traditional machine learning models and neural network based models. The traditional machine learning models include Naïve Bayes, Logistic Regression, Support Vector Machine, Random Forests, and Gradient Boosted Trees. The neural network based models include Convolutional Neural Networks, Recurrent Neural Networks, and their variants. The neural network based models use pre-trained GloVe word representations. The HybridCNN model is also used to evaluate the HybridCNN for this dataset. The RNN models use bidirectional GRU cells and self-matching attention mechanisms. The LTC method is also used to extract latent topic information from the hidden states of RNN. The context", "answers": ["Naïve Bayes (NB), Logistic Regression (LR), Support Vector Machine (SVM), Random Forests (RF), Gradient Boosted Trees (GBT),  Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN)", "Naïve Bayes (NB), Logistic Regression (LR), Support Vector Machine (SVM), Random Forests (RF), Gradient Boosted Trees (GBT), CNN, RNN"], "all_classes": null, "length": 2074}
{"pred": "    We use a bi-directional language model to augment the sequence to sequence encoder and a uni-directional model to augment the decoder. Both use self-attention and the uni-directional model contains 6 transformer blocks, followed by a word classifier to predict the next word on the right. The bi-directional model solves a cloze-style token prediction task at training time. The model consists of two towers, the forward tower operates left-to-right and the tower operating right-to-left as backward tower; each tower contains 6 transformer blocks. The forward and backward representations are combined via a self-attention module and the output of this module is", "answers": ["uni-directional model to augment the decoder", "bi-directional language model to augment the sequence to sequence encoder ,  uni-directional model to augment the decoder"], "all_classes": null, "length": 1914}
{"pred": "    The weights are dynamically adjusted by multiplying the soft probability p with a decaying factor (1-p). This decaying factor pushes down the weight of easy examples as training proceeds. The intuition is to push down the weight of easy examples. For easy examples whose probability are approaching 0 or 1, (1-p)p makes the model attach significantly less focus to them. The proposed method mimics the idea of focal loss for object detection in vision, which down-weights the loss assigned to well-classified examples by adding a (1-p)^β factor. The final loss is (1-p)^βlogp. The proposed method", "answers": ["One can think $(1-p_{i1})$ as a weight associated with each example, which changes as training proceeds. The intuition of changing $p_{i1}$ to $(1-p_{i1}) p_{i1}$ is to push down the weight of easy examples. For easy examples whose probability are approaching 0 or 1, $(1-p_{i1}) p_{i1}$ makes the model attach significantly less focus to them. Figure FIGREF23 gives gives an explanation from the perspective in derivative: the derivative of $\\frac{(1-p)p}{1+(1-p)p}$ with respect to $p$ approaches 0 immediately after $p$ approaches 0, which means the model attends less to examples once they are correctly classified.", "associates each training example with a weight in proportion to $(1-p)$, and this weight dynamically changes as training proceeds"], "all_classes": null, "length": 3640}
{"pred": "    The results show that agents utilizing knowledge-graphs in addition to either enhanced exploration method far outperform the baseline A2C and KG-A2C. KG-A2C-chained and KG-A2C-Explore both pass the bottleneck of a score of 40, whereas A2C-Explore gets to the bottleneck but cannot surpass it. There are a couple of key insights that can be drawn from these results. The first is that the knowledge graph appears to be critical; it is theorized to help with partial observability. However, the knowledge graph representation isn't sufficient in that the knowledge graph representation without enhanced exploration methods cannot", "answers": ["Reward of 11.8 for the A2C-chained model, 41.8 for the KG-A2C-chained model, 40 for A2C-Explore and 44 for KG-A2C-Explore.", "KG-A2C-chained and KG-A2C-Explore both pass the bottleneck of a score of 40"], "all_classes": null, "length": 2443}
{"pred": "    A Bayesian model for semantic role induction in multiple languages. The model consists of individual Bayesian models for each language and crosslingual latent variables to incorporate soft role agreement between aligned constituents. 0\n\n    Unanswerable 1\n\n    Yes 2\n\n    No 3\n\n    Unanswerable 4\n\n    Yes 5\n\n    No 6\n\n    Unanswerable 7\n\n    Yes 8\n\n    No 9\n\n    Unanswerable 10\n\n    Yes 11\n\n    No 12\n\n    Unanswerable 13\n\n    Yes 14\n\n    No 15\n\n    Unanswer", "answers": ["Bayesian model of garg2012unsupervised as our base monolingual model"], "all_classes": null, "length": 3701}
{"pred": "    The resource includes annotations for noises and disfluencies including aborted words, mispronunciations, poor intelligibility, repeated and corrected words, false starts, hesitations, undefined sound or pronunciations, non-verbal articulations, and pauses. Foreign words, in this case Spanish words, are also labelled as such. {\r\r\n\n\n    Unanswerable. {\r\r\n\n\n    No. {\r\r\n\n\n    Unanswerable. {\r\r\n\n\n    Yes. {\r\r\n\n\n    No. {\r\r\n\n\n    Unanswerable. {\r\r\n\n\n    No. {\r\r\n\n\n    Unanswerable. {\r\r\n\n\n    Yes. {\r\r\n\n\n    No. {\r\r\n\n\n    Un", "answers": ["Unanswerable", "Original transcription was labeled with additional labels in [] brackets with nonstandard pronunciation."], "all_classes": null, "length": 3018}
{"pred": "    A semicharacter architecture is a type of neural network that processes a sentence of words with misspelled characters, predicting the correct words at each step. It treats the first and the last characters individually, and is agnostic to the ordering of the internal characters. The training target is the correct corresponding word, and the model is optimized with cross-entropy loss. The architecture is inspired by psycholinguistic studies that demonstrate that humans can comprehend text altered by jumbling internal characters, provided that the first and last characters of each word remain unperturbed. The architecture is used to build robust word recognition models that can handle advers", "answers": ["A semi-character based RNN (ScRNN) treats the first and last characters individually, and is agnostic to the ordering of the internal characters", "processes a sentence of words with misspelled characters, predicting the correct words at each step"], "all_classes": null, "length": 4186}
{"pred": "    The languages explored are Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish, and Swedish. These languages represent a diverse range of typological, morphological, and syntactic characteristics. The focus is on languages for which morphosyntactic lexicons are available, and the results are compared to those obtained by the CRF-based system MarMoT and the best bidirectional LSTM models described by BIBREF20. The results show that an optimised enrichment of feature-based models with morphosyntactic lexicon results in significant accuracy gains", "answers": ["Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish", "Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish , Swedish"], "all_classes": null, "length": 2697}
{"pred": "    The NCEL approach is effective overall, achieving the best performance in most cases with an average gain of 2% on Micro F1 and 3% Macro F1. The baseline methods also achieve competitive results on some datasets but fail to adapt to the others. The proposed method performs consistently well on all datasets, demonstrating its good generalization ability. The impact of key modules is also investigated, and the results show that the prior probability performs well in the \"easy\" case but poorly in the \"hard\" case, while the global module in NCEL brings more improvements in the \"hard\" case than in the \"easy\" dataset", "answers": ["NCEL consistently outperforms various baselines with a favorable generalization ability"], "all_classes": null, "length": 4113}
{"pred": "    Yes, the data is de-identified with digital zeros and [de-identified] tags. The sentences in the transcript are grounded to the audio with the timestamps of its first and last word. The summaries and Medication Regimen tags are also grounded using the timestamps of the sentences from the transcript deemed relevant by the expert annotators. The transcript for a typical conversation can be quite long, and not easy for many of the high performing deep learning models to act on. Moreover, the medical information about a concept/condition/entity can change during the conversation after a significant time gap. For example, dosage of a medication can be different when", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4592}
{"pred": "    The baseline used was the error detection system by Rei2016, trained using the same FCE dataset. The performance of this system is reported in Table 4. The results show that error detection performance is substantially improved by making use of artificially generated data, created by any of the described methods. When comparing the error generation system by Felice2014a (FY14) with our pattern-based (PAT) and machine translation (MT) approaches, we see that the latter methods covering all error types consistently improve performance. While the added error types tend to be less frequent and more complicated to capture, the added coverage is indeed beneficial for", "answers": ["error detection system by Rei2016", "error detection system by Rei2016"], "all_classes": null, "length": 2132}
{"pred": "    The annotated clinical notes were obtained from the 2010 i2b2/VA BIBREF0 and 2013 ShARe/CLEF Task 1 BIBREF1 challenges. These challenges ask participants to design an algorithm to tag a set of predefined entities of interest in clinical notes. These entity tagging tasks are also known as clinical Named Entity Recognition (NER). For example, the CE task in 2010 i2b2/VA defines three types of entities: “problem”, “treatment”, and “test”. The CE task in 2013 ShARe/CLEF defines various types of disorder", "answers": ["clinical notes from the CE task in 2010 i2b2/VA", "clinical notes from the CE task in 2010 i2b2/VA "], "all_classes": null, "length": 3432}
{"pred": "    Masking words in the decoder helps to enhance the decoder using BERT's contextualized representations. By masking each word in the summary draft, the refine decoder can generate context vectors using BERT, and then predict a refined summary word using a Transformer decoder. This process provides a more complete input sequence which is consistent with BERT's pre-training processes, and helps the decoder generate more fluent and natural sequences. The parameters of the draft and refine decoders are shared, as the refine decoder learns to predict words given all the other ground-truth words of summary, which is similar to the language model's pre-train objective. The discrete", "answers": ["ecause this process is similar to the cloze task in BERT's pre-train process, therefore by using the ability of the contextual language model the decoder can generate more fluent and natural sequences."], "all_classes": null, "length": 3919}
{"pred": "    They use a dataset of tweets collected by positive and negative emoticons.\r\r\n\r\r\n    They use a dataset of tweets collected by positive and negative emoticons.\r\r\n\r\r\n    They use a dataset of tweets collected by positive and negative emoticons.\r\r\n\r\r\n    They use a dataset of tweets collected by positive and negative emoticons.\r\r\n\r\r\n    They use a dataset of tweets collected by positive and negative emoticons.\r\r\n\r\r\n    They use a dataset of tweets collected by positive and negative emoticons.\r\r\n\r\r\n    They use a dataset of tweets collected by positive and negative emoticons.\r\r\n\r\r\n    They use a dataset of tweets collected by positive and negative emoticons.\r\r\n\r\r\n", "answers": ["Unanswerable", " Paraphrase Database (PPDB) ,  book corpus", "Unanswerable"], "all_classes": null, "length": 1902}
{"pred": "    TF-IDF features are used. These features are based on the term frequency and inverse document frequency of words in a document. The features are used to extract and corroborate useful keywords from pathology cancer reports. The features are also used to predict the primary diagnosis from the text content of a given report. The features are extracted from training reports and used to train different classification models. The performance of classifiers is measured quantitatively on the test dataset using evaluation metrics. The features are also used to highlight the important keywords within a report. The top 50 keywords are extracted using TF-IDF weights and highlighted using different colors based on their associated", "answers": ["Unanswerable"], "all_classes": null, "length": 2108}
{"pred": "    The dataset is annotated based on a hierarchical model of depression-related symptoms. Each tweet is annotated as no evidence of depression or evidence of depression. If a tweet is annotated evidence of depression, then it is further annotated with one or more depressive symptoms, for example, depressed mood, disturbed sleep, or fatigue or loss of energy. For each class, every annotation is binarized as the positive class or negative class. The feature groups are encoded with associated feature values binarized to represent potentially informative features for classifying depression-related classes. The feature groups include lexical features, syntactic features, emotion features, demographic features, sentiment features,", "answers": ["no evidence of depression, depressed mood, disturbed sleep, fatigue or loss of energy", "The annotations are based on evidence of depression and further annotated by the depressive symptom if there is evidence of depression"], "all_classes": null, "length": 1947}
{"pred": "    They evaluated on eight biomedical NER tasks: BC5CDR, CheBI, CRAFT, DDI, GENIA, JNLPBA, NCBI-disease, and NCBI-drug. 1/1\n\nQuestion: What is the main contribution of this paper?\n\nAnswer: The main contribution of this paper is an inexpensive, CPU-only method for domain-adapting Pretrained Language Models (PTLMs). The method involves training Word2Vec vectors on target-domain data and aligning them with the wordpiece vector space of a general-domain PTLM. 1/1\n\nQuestion: How does the proposed", "answers": ["BC5CDR-disease, NCBI-disease, BC5CDR-chem, BC4CHEMD, BC2GM, JNLPBA, LINNAEUS, Species-800", "BC5CDR-disease, NCBI-disease, BC5CDR-chem, BC4CHEMD, BC2GM, JNLPBA, LINNAEUS, Species-800"], "all_classes": null, "length": 2800}
{"pred": "    The training data was translated using the machine translation platform Apertium.<translation data was translated using the machine translation platform Apertium.<translation data was translated using the machine translation platform Apertium.<translation data was translated using the machine translation platform Apertium.<translation data was translated using the machine translation platform Apertium.<translation data was translated using the machine translation platform Apertium.<translation data was translated using the machine translation platform Apertium.<translation data was translated using the machine translation platform Apertium.<translation data was translated using the machine translation platform Apertium.<translation data was translated using the machine translation platform", "answers": ["using the machine translation platform Apertium ", "machine translation platform Apertium BIBREF5"], "all_classes": null, "length": 2423}
{"pred": "    They used a multinomial Naive Bayes classifier.", "answers": ["AllWords model by counting the frequencies of all the remaining words and training a multinomial Naive Bayes classifier"], "all_classes": null, "length": 4177}
{"pred": "    The baseline system for the SLC task is a very simple logistic regression classifier with default parameters, where we represent the input instances with a single feature: the length of the sentence. The performance of this baseline on the SLC task is shown in Tables TABREF33 and TABREF34. The baseline for the FLC task generates spans and selects one of the 18 techniques randomly. The inefficacy of such a simple random baseline is illustrated in Tables TABREF36 and TABREF41. 】\n\nQuestion: What was the winning team for the FLC task?\n\nAnswer: overposting\n\n    The winning team for the FLC task", "answers": ["The baseline system for the SLC task is a very simple logistic regression classifier with default parameters. The baseline for the FLC task generates spans and selects one of the 18 techniques randomly.", "SLC task is a very simple logistic regression classifier, FLC task generates spans and selects one of the 18 techniques randomly"], "all_classes": null, "length": 3001}
{"pred": "    The baselines they compare with are the following: 1) A baseline model based on conditional random fields (CRF), where features like POS tags produced by the Stanford POS tagger, n-grams, label transitions, word suffixes and relative position to the end of the text are considered. 2) A system that performs pun detection followed by pun location, which is a pipeline approach. 3) A system that performs pun detection and location tasks separately, which is a joint learning approach. 4) A system that performs pun detection and location tasks separately, which is a joint learning approach. 5) A system", "answers": ["They compare with the following models: by Pedersen (2017), by Pramanick and Das (2017), by Mikhalkova and Karyakin (2017),  by Vadehra (2017), Indurthi and Oota (2017), by Vechtomova (2017), by (Cai et al., 2018), and CRF."], "all_classes": null, "length": 2991}
{"pred": "    The political bias of different sources is included in the model by assigning a political bias label to different US outlets following the procedure described in BIBREF2. In order to assess the robustness of the method, classification experiments are performed by training only on left-biased (or right-biased) outlets of both disinformation and mainstream domains and testing on the entire set of sources. Additionally, classification experiments are performed by excluding particular sources that outweigh the others in terms of samples to avoid over-fitting. The results show that the model is able to accurately classify credible vs non-credible diffusion networks (and consequently news articles) with", "answers": ["By assigning a political bias label to each news article and training only on left-biased or right-biased outlets of both disinformation and mainstream domains", "we also account for political biases inherent to different news sources, referring to the procedure proposed in BIBREF2 to label different outlets. Overall we show that we are able to classify credible vs non-credible diffusion networks (and consequently news articles) with high accuracy (AUROC up to 94%), even when accounting for the political bias of sources (and training only on left-biased or right-biased articles). We observe that the layer of mentions alone conveys useful information for the classification, denoting a different usage of this functionality when sharing news belonging to the two news domains. We also show that most discriminative features, which are relative to the breadth and depth of largest cascades in different layers, are the same across the two countries."], "all_classes": null, "length": 4882}
{"pred": "    The ancient Chinese dataset comes from ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era. They used plain and accurate words to express what happened at that time, and thus ensure the generality of the translated materials.", "answers": ["ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era", "Ancient Chinese history records in several dynasties and articles written by celebrities during 1000BC-200BC collected from the internet "], "all_classes": null, "length": 3722}
{"pred": "    English tweets.", "answers": ["English", "English ", "English"], "all_classes": null, "length": 2240}
{"pred": "    The Chinese datasets used were the Penn Chinese Treebank (CTB) and the Chinese Gigaword (CG) corpus. The CTB dataset contains 1.2 million words of Chinese text, while the CG corpus contains 1.5 billion words of Chinese text. Both datasets were used to train and evaluate the models. The CTB dataset was used to train the models, while the CG corpus was used to evaluate the models. The models were evaluated on the CG corpus to ensure that they could generalize to unseen data. The models were also evaluated on the CTB dataset to ensure that they could perform well on the dataset they", "answers": ["Answer with content missing: (Data section) Chinese with version 5.1 of the Chinese Penn Treebank (CTB)"], "all_classes": null, "length": 2545}
{"pred": "    The UTCNN model has three layers: a user layer, a topic layer, and a comment layer. The user layer consists of a user matrix embedding layer and a user vector embedding layer, while the topic layer consists of a topic matrix embedding layer and a topic vector embedding layer. The comment layer consists of a comment representation layer and a maximum pooling layer. The three layers are connected by a fully connected network and a softmax layer. The model has a total of 5 layers.", "answers": ["eight layers"], "all_classes": null, "length": 4487}
{"pred": "    The dataset used in this paper is Flickr, which is a photo-sharing website that provides short textual descriptions in the form of tags to help others find the images. The tags associated with georeferenced photos often describe the location where these photos were taken, and Flickr can thus be regarded as a source of environmental information. The use of Flickr for modelling urban environments has already received considerable attention. For instance, various approaches have been proposed for modelling urban regions, and for identifying points-of-interest and itineraries. However, the usefulness of Flickr for characterizing the natural environment, which is the focus of this paper, is less well-under", "answers": [" the same datasets as BIBREF7", "same datasets as BIBREF7"], "all_classes": null, "length": 4661}
{"pred": "    The clinical datasets used in the paper are NUBes-PHI and MEDDOCAN. NUBes-PHI is a corpus of real medical reports written in Spanish and annotated with sensitive information. MEDDOCAN is a synthetic corpus of clinical cases enriched with sensitive information by health documentalists. Both datasets consist of plain text containing clinical narrative written in Spanish, and their respective manual annotations of sensitive information in BRAT standoff format. The NUBes-PHI dataset contains 32,055 sentences annotated for 11 different sensitive information categories, while the MEDDOCAN dataset contains 21 sensitive information categories. The MEDDOCAN dataset is", "answers": ["MEDDOCAN, NUBes-PHI", "MEDDOCAN, NUBes "], "all_classes": null, "length": 4528}
{"pred": "    Unigrams and Pragmatic features, Stylistic patterns, and patterns related to situational disparity. 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, ", "answers": ["Unanswerable"], "all_classes": null, "length": 3543}
{"pred": "    The metrics used to establish that this makes chatbots more knowledgeable and better at learning and conversation are: Coverage, Average MCC, and Average Positive F1 Score. These metrics are used to evaluate the strategy formulation ability and predictive performance of the lifelong interactive learning and inference (LiLi) approach. The strategy formulation ability is measured by the fraction of total query data instances for which LiLi has successfully formulated strategies that lead to winning. The predictive performance is measured by the average Matthews correlation coefficient (MCC) and average positive F1 score. The results show that LiLi outperforms the baselines in terms of both predictive quality and strategy", "answers": ["Coverage, Avg. MCC and avg. +ve F1 score", "strategy formulation ability, we introduce a measure called Coverage( INLINEFORM0 ), To evaluate the predictive performance, we use Avg. MCC and avg. +ve F1 score"], "all_classes": null, "length": 5869}
{"pred": "    No, they do not employ their indexing-based method to create a sample of a QA Wikipedia dataset. Instead, they use a different method to map the answer contexts from all corpora to the same version of Wikipedia so they can be coherently used for answer retrieval. They first index each paragraph in Wikipedia using {1,2,3}-grams, then query each answer sentence from the corpora to Lucene, and retrieve the top-5 ranked paragraphs. They then measure the cosine similarity between each sentence in these paragraphs and the answer sentence for 1-grams, assign weights to each 2-gram score, and measure", "answers": ["Yes", "No"], "all_classes": null, "length": 1910}
{"pred": "    Galatasaray and Fenerbahçe are the targets.", "answers": ["Galatasaray, Fenerbahçe", "Galatasaray , Fenerbahçe "], "all_classes": null, "length": 2234}
{"pred": "    In this paper, we conduct experiments on the transformation from non-ironic sentences to ironic sentences and from ironic sentences to non-ironic sentences. We evaluate the performance of our model and baselines using automatic and human evaluation metrics. We also analyze the errors and limitations of our model and discuss potential future directions.", "answers": ["Irony Classifier, Sentiment Classifier for Irony, Sentiment Classifier for Non-irony, transformation from ironic sentences to non-ironic sentences"], "all_classes": null, "length": 4600}
{"pred": "    Gaussian-masked directional multi-head attention works by adjusting the weight between characters and their adjacent character to a larger value which stands for the effect of adjacent characters. It ensures that the relationship between two characters with long distances is weaker than adjacent characters. The Gaussian weight only relies on the distance between characters and is represented as a fix Gaussian weight for attention. The Gaussian weight matrix presents the localness relationship between each two characters. The Gaussian weight equals 1 when the distance between characters is 0 and decreases as the distance increases. The Gaussian-masked attention combines the Gaussian weight with the score matrix produced by QK^T. The triangular matrix", "answers": ["pays attentions to adjacent characters and casts a localness relationship between the characters as a fixed Gaussian weight assuming the weight relies on the distance between characters", "Gaussian-masked directional attention can be described as a function to map queries and key-value pairs to the representation of input, Gaussian-masked directional attention expects to pay attention to the adjacent characters of each positions and cast the localness relationship between characters as a fix Gaussian weight for attention, Gaussian weight only relys on the distance between characters"], "all_classes": null, "length": 3612}
{"pred": "    Facebook status update messages.", "answers": ["Facebook status update messages", "Facebook status update messages"], "all_classes": null, "length": 4005}
{"pred": "    The baseline features are the features extracted from the fully-connected layer of the baseline CNN. These features are used to classify a sentence as sarcastic vs non-sarcastic. The baseline CNN extracts the inherent semantics from the sarcastic corpus by employing deep domain understanding. The process of using baseline features with other features extracted from the pre-trained model is described in Section 4.4. analsex\n\nQuestion: What are the pre-trained models used for feature extraction?\n\nAnswer: davidjl\n\n    The pre-trained models used for feature extraction are sentiment, emotion, and personality models. These models are trained on benchmark datasets and used to extract features from", "answers": [" The features extracted from CNN."], "all_classes": null, "length": 4855}
{"pred": "    The number of clusters was varied in the experiments on the four tasks. The number of clusters was varied from 100 to 2000. The number of clusters was varied in increments of 100. The number of clusters was varied in increments of 100. The number of clusters was varied from 100 to 2000. The number of clusters was varied in increments of 100. The number of clusters was varied from 100 to 2000. The number of clusters was varied in increments of 100. The number of clusters was varied from 100 to 2000. The number of clusters was varied in increments of", "answers": ["number of clusters, seed value in clustering, selection of word vectors, window size and dimension of embedding", "different number of clusters, different embeddings"], "all_classes": null, "length": 2753}
{"pred": "    Second (EI-Reg), second (EI-Oc), fourth (V-Reg) and fifth (V-Oc) on the SemEval AIT-2018 leaderboard.ยนตร\n\n    Second (EI-Reg), second (EI-Oc), fourth (V-Reg) and fifth (V-Oc) on the SemEval AIT-2018 leaderboard.ยนตร\n\n    Second (EI-Reg), second (EI-Oc), fourth (V-Reg) and fifth (V-Oc) on the SemEval AIT-2018 leaderboard.ยนตร\n\n    Second (EI-Reg), second (EI-O", "answers": ["column Ens Test in Table TABREF19"], "all_classes": null, "length": 2424}
{"pred": "    The corpus consists of 53 documents, which contain an average number of 156.1 sentences per document, each with 19.55 tokens on average. The corpus comprises 8,275 sentences and 167,739 words in total. However, as mentioned above, only case presentation sections, headings and abstracts are annotated. The numbers of annotated entities are summarized in Table TABREF24. Findings are the most frequently annotated type of entity. This makes sense given that findings paint a clinical picture of the patient's condition. The number of tokens per entity ranges from one token for all types to 5 tokens for cases (", "answers": ["8,275 sentences and 167,739 words in total", "The corpus comprises 8,275 sentences and 167,739 words in total."], "all_classes": null, "length": 2669}
{"pred": "    Yes, it is possible to convert a cloze-style questions to a naturally-looking questions. The process involves identifying the missing words or phrases in the cloze-style questions and replacing them with the correct answers. This can be done using a combination of natural language processing techniques such as named entity recognition, part-of-speech tagging, and dependency parsing. The resulting questions can then be used to train a question-answering model. However, it is important to note that the quality of the converted questions may vary depending on the accuracy of the natural language processing techniques used. Additionally, the conversion process may introduce errors or biases that need to", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 2764}
{"pred": "    We consider text categorization and sentiment classification.", "answers": ["text classification for themes including sentiment, web-page, science, medical and healthcare"], "all_classes": null, "length": 3591}
{"pred": "    The previous methods are term frequency models, rule-based methods, and machine learning methods. The model is compared to these methods in terms of performance on different datasets. The model achieves state-of-the-art performance on multiple datasets, including TREC, GARD, and MLBioMedLAT. The model also demonstrates strong performance on the ARC dataset, which is a challenge dataset for science exams. The model is able to achieve strong performance across multiple datasets, which is a significant achievement. The model is also able to improve question answering performance when question classification information is incorporated into the model. Overall, the model demonstrates strong performance and has the potential to", "answers": ["bag-of-words model, CNN"], "all_classes": null, "length": 5838}
{"pred": "    The training sets of these versions of ELMo are significantly larger than the previous ones. The previous versions of ELMo were trained on a 20-million-word dataset, while the new versions of ELMo are trained on much larger datasets, ranging from 270 million words for Latvian to 280 million words for Finnish. The larger training sets allow the models to capture more semantic and contextual information, resulting in better performance on downstream tasks. The new versions of ELMo are therefore expected to perform better on tasks such as named entity recognition and question answering. However, the quality of the embeddings still depends on the quality", "answers": ["By 14 times.", "up to 1.95 times larger"], "all_classes": null, "length": 3294}
{"pred": "    6946 sentences and 16225 unique words.", "answers": ["3606", "6946"], "all_classes": null, "length": 2835}
{"pred": "    Eusboost and MWMOTE techniques.", "answers": ["MLP", "Eusboost, MWMOTE"], "all_classes": null, "length": 2474}
{"pred": "    Yes, their NER model learns NER from both text and images. The model takes as input a sequence of tokens, each of which comprises a word token, a character sequence, and an image, in their respective representation. The model then uses a Bi-LSTM and CRF hybrid model to predict the named entities in the text. The model also incorporates a modality attention module, which learns a unified representation space for multiple available modalities (words, characters, and images) and produces a single vector representation with aggregated knowledge among multiple modalities. The modality attention module is able to focus on the most effective modality (", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3784}
{"pred": "    No, they evaluate on the Wall Street Journal (WSJ) portion of the Penn Treebank, which is a corpus of English language data. They also train the skip-gram word embeddings on the one billion word language modeling benchmark dataset, which is a corpus of English language data. The skip-gram embeddings are trained on the one billion word language modeling benchmark dataset in addition to the WSJ corpus. Therefore, the evaluation is not limited to English datasets. The authors also mention that they use the WSJ corpus for training and testing, which is a corpus of English language data. They use 45 tag clusters, which are the", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 4320}
{"pred": "    0.7033\n\n    0.7033\n\n    0.7033\n\n    0.7033\n\n    0.7033\n\n    0.7033\n\n    0.7033\n\n    0.7033\n\n    0.7033\n\n    0.7033\n\n    0.7033\n\n    0.7033\n\n    0.7033\n\n    0.7033\n\n    0.7033\n\n    0.7033\n\n    0.7033\n\n    0.7033\n\n    ", "answers": ["0.5115", "0.6103"], "all_classes": null, "length": 6810}
{"pred": "    For both POS tagging and dependency parsing, they run experiments on the Wall Street Journal (WSJ) portion of the Penn Treebank. To create the observed data embeddings, they train skip-gram word embeddings that are found to capture syntactic properties well when trained with small context window. The dimensionality is set to 100, and the training context window size is set to 1. The skip-gram embeddings are trained on the one billion word language modeling benchmark dataset in addition to the WSJ corpus.\r\r\n\r\r\n    For the neural projector, they employ rectified networks as coupling function. They use a rectified network with an input", "answers": [" Wall Street Journal (WSJ) portion of the Penn Treebank", "Unanswerable"], "all_classes": null, "length": 4327}
{"pred": "    The authors cite a survey among engineers and identify a spectrum of three typical personas. They also analyze the NLP jobs submitted to a commercial centralized GPU cluster and find that about 87.5% NLP related jobs belong to a few common tasks, suggesting that more than 90% of the networks were composed of several common components. These observations suggest that many engineers find it a big overhead to choose from multiple frameworks, models and optimization techniques. The authors also provide technical contributions of NeuronBlocks, which include providing two layers of support to engineers, offering a suite of reusable and standard components, and supporting various NLP tasks. These", "answers": ["By conducting a survey among engineers", "Unanswerable"], "all_classes": null, "length": 1692}
{"pred": "    SimpleQuestions and WebQSP.ยนตร", "answers": ["SimpleQuestions, WebQSP", "WebQSP, SimpleQuestions"], "all_classes": null, "length": 4524}
