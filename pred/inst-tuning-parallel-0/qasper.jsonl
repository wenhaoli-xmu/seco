{"pred": "        The ground truth for fake news is established by manually inspecting the text field within the tweets to label them as containing fake news, or not containing them (according to the characterization presented before). This annotated dataset is publicly available and can be freely reused. (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no)", "answers": ["Ground truth is not established in the paper"], "all_classes": null, "length": 3141}
{"pred": "The GhostVLAD approach is a pooling strategy that uses ghost clusters to improve the performance of language identification. It works by adding ghost clusters to the NetVLAD approach, which helps to map noisy or irrelevant content into ghost clusters and reduces the contribution of unwanted features to normal clusters. The ghost clusters are excluded during the feature aggregation stage, which results in better language discrimination. The GhostVLAD approach has been shown to improve the performance of language identification for Indian languages. (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (un", "answers": ["extension of the NetVLAD, adds Ghost clusters along with the NetVLAD clusters", "An extension of NetVLAD which replaces hard assignment-based clustering with soft assignment-based clustering with the additon o fusing Ghost clusters to deal with noisy content."], "all_classes": null, "length": 2454}
{"pred": "        68.8% to 71.8% when applied to the IEMOCAP dataset.       \n\n        68.8% to 71.8% when applied to the IEMOCAP dataset.       \n\n        68.8% to 71.8% when applied to the IEMOCAP dataset.       \n\n        68.8% to 71.8% when applied to the IEMOCAP dataset.       \n\n        68.8% to 71.8% when applied to the IEMOCAP dataset.       \n\n        68.8% to ", "answers": ["the attention model, MDREA, also outperforms the best existing research results (WAP 0.690 to 0.688)"], "all_classes": null, "length": 3207}
{"pred": "        The additional features and context proposed are the use of context tweets and the use of latent topic clustering. The context tweets are used to provide additional information about the tweet being classified, while the latent topic clustering is used to extract latent topic information from the hidden states of the RNN and use it for additional information in classifying the text data. The use of these additional features and context is expected to improve the accuracy of the models in detecting abusive language.", "answers": ["using tweets that one has replied or quoted to as contextual information", "text sequences of context tweets"], "all_classes": null, "length": 2060}
{"pred": "    FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney. davidjl\n\nQuestion: What are the main decisions to be taken in developing their model?\n\nAnswer: davidjl\n\n    1. Which Facebook pages to select as training data, and 2. Which features to use to train the model. davidjl\n\nQuestion: What are the main observations from the results on development set?\n\nAnswer: davidjl\n\n    1. A simple tf-idf bag-of-word model works very well", "answers": ["FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney", "FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney."], "all_classes": null, "length": 3411}
{"pred": "Yes, both datasets contain only English data. The hashtags are extracted from tweets in the Stanford Sentiment Analysis Dataset, and the SemEval dataset consists of tweets in English. The hashtags are used to predict the sentiment of the tweets. The SemEval dataset is used to evaluate the performance of the sentiment analysis model. The hashtags are segmented into meaningful words to improve the performance of the sentiment analysis model. The segmentation is done using a pairwise neural ranking model that learns from candidate pairs and ranks each individual candidate directly rather than relatively. The model uses corpus-based and linguistic features to rank the segmentations. The model is trained on the STAN dataset and", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3735}
{"pred": "    The proposed evaluation is based on the concept map generated by the system. The evaluation measures the quality of the concept map by comparing it to the reference concept map. The evaluation metrics include the number of concepts and relations in the concept map, the accuracy of the concept labels, and the accuracy of the relation labels. The evaluation also considers the connectedness and size constraints of the concept map. The evaluation is performed by comparing the concept map generated by the system to the reference concept map, and the results are reported in terms of the evaluation metrics. The evaluation is designed to be objective and quantitative, allowing for a fair comparison between different systems.", "answers": ["Answer with content missing: (Evaluation Metrics section) Precision, Recall, F1-scores, Strict match, METEOR, ROUGE-2"], "all_classes": null, "length": 4263}
{"pred": "        CNN/DailyMail, New York Times Annotated Corpus (NYT), and XSum.       ยนตราะหัวหน้า网刊\n\n        CNN/DailyMail, New York Times Annotated Corpus (NYT), and XSum.       ยนตราะหัวหน้า网刊\n\n        CNN/DailyMail, New York Times Annotated Corpus (NYT), and XSum.       ยนตราะหัวหน้า网刊\n\n        CNN/DailyMail, New York Times Annotated Corpus (NYT), and XSum.       ยนตราะหัวหน้า网刊\n\n        CNN/DailyMail, New York Times Annotated Corpus (NYT), and XSum.", "answers": ["CNN/DailyMail news highlights, New York Times Annotated Corpus, XSum", "the CNN/DailyMail news highlights dataset BIBREF24, the New York Times Annotated Corpus (NYT; BIBREF25), XSum BIBREF22"], "all_classes": null, "length": 4369}
{"pred": "    This approach compares favorably to other WSD approaches employing word embeddings. It outperforms existing approaches on the SCWS dataset and achieves better correlation scores on the benchmark word similarity datasets. It also performs better than other approaches on the entailment datasets. Overall, this approach demonstrates the effectiveness of using KL divergence as an energy function for learning multi-sense word embedding distributions.΄΄΄΄΄΄΄΄΄΄΄΄΄΄΄΄΄΄΄΄΄΄΄΄΄΄΄΄΄΄΄΄΄΄΄΄΄΄΄΄΄΄΄΄΄΄΄΄΄΄΄΄΄", "answers": ["GM$\\_$KL achieves better correlation than existing approaches for various metrics on SCWS dataset."], "all_classes": null, "length": 2189}
{"pred": "        The ensemble method works by averaging the predictions from the constituent single models. The single models are selected using a greedy algorithm that starts with the best performing model according to validation performance and then tries adding the best performing model that has not been previously tried. The model is kept in the ensemble if it improves the validation performance and discarded otherwise. This way, each model is tried once and the final ensemble is formed by selecting the best performing models. The algorithm is offered 10 models and selects 5 of them for the final ensemble. The ensemble method is used to improve the performance of the model by combining the predictions of multiple models. The idea", "answers": ["simply averaging the predictions from the constituent single models"], "all_classes": null, "length": 4212}
{"pred": "    The sources of the datasets are the Friends TV sitcom and Facebook messenger chats. The Friends dataset is composed of dialogues from the scripts of the Friends TV sitcom, while the EmotionPush dataset is made up of Facebook messenger chats. Both datasets are annotated by five annotators on a crowd-sourcing platform (Amazon Mechanical Turk). The labeling work is only based on the textual content. The Friends dataset is speech-based, while the EmotionPush dataset is chat-based. The Friends dataset is annotated dialogues from the TV sitcom, while the EmotionPush dataset is made up of Facebook messenger chats. The Friends dataset is speech-based, while", "answers": ["Friends TV sitcom, Facebook messenger chats"], "all_classes": null, "length": 3185}
{"pred": "        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English\n\n        English", "answers": ["English", "Simple English"], "all_classes": null, "length": 2243}
{"pred": "    The IMDb dataset of movie reviews is used for sentiment analysis. (BIBREF11) (BIBREF11) (BIBREF11) (BIBREF11) (BIBREF11) (BIBREF11) (BIBREF11) (BIBREF11) (BIBREF11) (BIBREF11) (BIBREF11) (BIBREF11) (BIBREF11) (BIBREF11) (BIBREF11) (BIBREF11) (BIBREF11) (BIBREF11) (BIBREF11) (", "answers": ["IMDb dataset of movie reviews", "IMDb"], "all_classes": null, "length": 2327}
{"pred": "    89.3% on the development set and 88.5% on the test set. (Note: The accuracy is measured in terms of F1 score.) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable)", "answers": ["F1 scores of 85.99 on the DL-PS data,  75.15 on the EC-MT data and 71.53 on the EC-UQ data ", "F1 of 85.99 on the DL-PS dataset (dialog domain); 75.15 on EC-MT and 71.53 on EC-UQ (e-commerce domain)"], "all_classes": null, "length": 5310}
{"pred": "Yes, they experimented with this new dataset. They recorded data from 19 participants and discarded the data of one of them due to technical difficulties with the eye-tracking calibration. Hence, they share the data of 18 participants. All participants are healthy adults (mean age = 34 (SD=8.3), 10 females). Their native language is English, originating from Australia, Canada, UK, USA or South Africa. Two participants are left-handed and three participants wear glasses for reading. Details on subject demographics can be found in Table 4. All participants gave written consent for their participation and the re-use of the data prior to", "answers": ["No"], "all_classes": null, "length": 3445}
{"pred": "        The datasets used are the Wizard of Oz method and crowdsourcing. The Wizard of Oz method involves applying the Wizard of Oz method to a set of potential users of the system, and collecting questions that the users asked. These questions are manually classified into a set of intent classes, and used to train the first version of the system. The crowdsourcing approach involves increasing the training set by adding new classes and samples whenever the system is deemed necessary. The training set is used to train the intent classifier. The intent classifier is trained using features extracted from word vectors. The word vectors are created by considering a set of documents related to finance. The", "answers": ["Custom dataset with user questions; set of documents, twitter posts and news articles, all related to finance.", "a self-collected financial intents dataset in Portuguese"], "all_classes": null, "length": 13401}
{"pred": "        Energy sector achieved the best performance. {\r\r\n        {\r\r\n        {\r\r\n        {\r\r\n        {\r\r\n        {\r\r\n        {\r\r\n        {\r\r\n        {\r\r\n        {\r\r\n        {\r\r\n        {\r\r\n        {\r\r\n        {\r\r\n        {\r\r\n        {\r\r\n        {\r\r\n        {\r\r\n        {\r\r\n        {\r\r\n        {\r\r\n        {\r\r\n        {\r\r\n        {\r\r\n        {\r\r\n        {\r\r\n        {\r\r\n        {\r\r\n        {\r\r\n        {\r\r\n        {\r\r\n        {\r\r\n        {\r\r\n        {\r\r\n        {\r\r\n        {\r\r\n        {\r\r\n        {\r\r\n        {\r\r\n        {\r\r\n        {\r\r\n        {\r\r\n        {\r\r\n        {\r\r\n        {\r\r\n        {\r\r\n        {\r\r\n        {\r\r\n        {\r\r\n        {\r\r\n        {\r\r\n        {\r\r\n        {\r\r\n        {\r\r\n        {\r\r\n        {\r\r\n        {\r\r\n        {\r\r\n        {\r\r\n        {\r\r\n       ", "answers": ["Energy with accuracy of 0.538", "Energy"], "all_classes": null, "length": 10349}
{"pred": "    RNN-based NMT and Transformer-NMT. They also compared the performance of SMT and various NMT models on their built dataset. They found that the Transformer-NMT model performs better than the RNN-based NMT model and SMT model. They also observed that translating proper nouns and personal pronouns is difficult for all of these models. They plan to focus on solving this problem in the future. (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (", "answers": ["RNN-based NMT model, Transformer-NMT"], "all_classes": null, "length": 3708}
{"pred": "    1. A regularization term associated with neutral features. 2. The maximum entropy of class distribution regularization term. 3. The KL divergence between reference and predicted class distribution. davidjl", "answers": ["a regularization term associated with neutral features, the maximum entropy of class distribution regularization term, the KL divergence between reference and predicted class distribution", "a regularization term associated with neutral features,  the maximum entropy of class distribution, KL divergence between reference and predicted class distribution"], "all_classes": null, "length": 3604}
{"pred": "        1) SVM with unigram, bigram, and trigram features, which is a standard yet rather strong classifier for text features; 2) SVM with average word embedding, where a document is represented as a continuous representation by averaging the embeddings of the composite words; 3) SVM with average transformed word embeddings (the INLINEFORM0 in equation EQREF6), where a document is represented as a continuous representation by averaging the transformed embeddings of the composite words; 4) two mature deep learning models on text classification, CNN BIBREF3 and Recurrent Convolutional Neural Networks (RCNN) BIBREF0", "answers": ["SVM with unigram, bigram, and trigram features, SVM with average word embedding, SVM with average transformed word embeddings, CNN, ecurrent Convolutional Neural Networks, SVM and deep learning models with comment information", "SVM with unigram, bigram, trigram features, with average word embedding, with average transformed word embeddings, CNN and RCNN, SVM, CNN, RCNN with comment information"], "all_classes": null, "length": 4512}
{"pred": "        5.2 points. (0.2 points) (0.2 points) (0.2 points) (0.2 points) (0.2 points) (0.2 points) (0.2 points) (0.2 points) (0.2 points) (0.2 points) (0.2 points) (0.2 points) (0.2 points) (0.2 points) (0.2 points) (0.2 points) (0.2 points) (0.2 points) (0.2 points) (0.2 points) (", "answers": ["They decrease MAE in 0.34"], "all_classes": null, "length": 2735}
{"pred": "    The model improves interpretability by allowing different attention heads to learn different sparsity behaviors, which can be analyzed to identify head specializations. The model also learns to combine sparse and dense attention, leading to more diverse attention heads. The model can also identify sources of ambiguity where the head is less confident in its prediction. The model can also identify sources of ambiguity where the head is less confident in its prediction. The model can also identify sources of ambiguity where the head is less confident in its prediction. The model can also identify sources of ambiguity where the head is less confident in its prediction. The model can also identify sources of ambiguity where the", "answers": ["the attention heads in the proposed adaptively sparse Transformer can specialize more and with higher confidence", "We introduce sparse attention into the Transformer architecture"], "all_classes": null, "length": 4902}
{"pred": "The baseline was a context-agnostic machine translation system. The DocRepair model was trained to correct inconsistencies between sentence-level translations of this baseline system. The DocRepair model only requires monolingual document-level data, and it is decoupled from the baseline system. The DocRepair model performs automatic post-editing on a sequence of sentence-level translations, refining translations of sentences in context of each other. The DocRepair model results in substantial improvements in translation quality as measured by BLEU, targeted contrastive evaluation of several discourse phenomena, and human evaluation. Moreover, the DocRepair model is able to detect which discourse phenomena are hard to capture using", "answers": [" MT system on the data released by BIBREF11", "Transformer base, two-pass CADec model"], "all_classes": null, "length": 3716}
{"pred": "    The metrics used for evaluation are Labeled Attachment Scores (LAS) for zero-shot dependency parsing and accuracy for cross-lingual natural language inference (XNLI). The LAS score measures the percentage of correctly labeled dependencies in a treebank, while the accuracy score measures the percentage of correctly classified sentences in the XNLI dataset. The results are presented in tables and figures. davidjl\n\n    The metrics used for evaluation are Labeled Attachment Scores (LAS) for zero-shot dependency parsing and accuracy for cross-lingual natural language inference (XNLI). The LAS score measures the percentage of correctly labeled dependencies in a treebank, while", "answers": ["translation probabilities, Labeled Attachment Scores (LAS)", "accuracy, Labeled Attachment Scores (LAS)"], "all_classes": null, "length": 3402}
{"pred": "        The attention module is pretrained on the ASR and MT tasks. The attention module is task-specific, so it is pretrained on the tasks that it will be used for. This allows the attention module to learn the alignments between the input and output sequences for each task. The attention module is pretrained on the ASR and MT tasks, which allows it to learn the alignments between the input and output sequences for each task. This helps the attention module to better perform its task of aligning the input and output sequences during fine-tuning. The attention module is pretrained on the ASR and MT tasks, which allows it to learn the alignments between", "answers": ["the model is pre-trained on CTC-based ASR task and MT task in the pre-training stage."], "all_classes": null, "length": 4656}
{"pred": "    The stylistic features obtained are emoticons, laughter expressions such as \"lol\", and patterns related to situational disparity. These features are used to detect sarcasm in text.", "answers": ["Unanswerable"], "all_classes": null, "length": 3543}
{"pred": "    The encoder has an LSTM layer. (1) (2) (3) (4) (5) (6) (7) (8) (9) (10) (11) (12) (13) (14) (15) (16) (17) (18) (19) (20) (21) (22) (23) (24) (25) (26) (27) (28) (29) (30) (31) (32) (33) (34) (35) (36) (37) (38) (39) (40)", "answers": ["LSTM", "LSTM"], "all_classes": null, "length": 2289}
{"pred": "    Yes, WordNet is useful for taxonomic reasoning for this task. The WordNet dataset is used to construct the WordNetQA dataset, which probes models' ability to answer questions about definitions, hypernymy, hyponymy, synonyms, and other types of relational knowledge. The WordNet dataset is a directed, edge-labeled graph that represents a set of triples of various types, including ISA relations, hypernymy, and hyponymy. The WordNet dataset is used to construct the WordNetQA dataset, which probes models' ability to answer questions about definitions, hypernymy, hyponymy,", "answers": ["Unanswerable", "Yes"], "all_classes": null, "length": 6391}
{"pred": "        The baselines were the conventional automatic speech recognition (ASR) systems, which typically consist of several independently learned components: an acoustic model to predict context-dependent sub-phoneme states (senones) from audio, a graph structure to map senones to phonemes, and a pronunciation model to map phonemes to words. Hybrid systems combine hidden Markov models to model state dependencies with neural networks to predict states. Newer approaches such as end-to-end (E2E) systems reduce the overall complexity of the final system. The baselines were also the prior work that has explored using time-delay neural networks (TDNN), other", "answers": ["Unanswerable", "LF-MMI Attention\nSeq2Seq \nRNN-T \nChar E2E LF-MMI \nPhone E2E LF-MMI \nCTC + Gram-CTC"], "all_classes": null, "length": 1856}
{"pred": "        22,880 users", "answers": ["22,880 users", "20,000"], "all_classes": null, "length": 4160}
{"pred": "    BPE perplexity, BLEU-1/4, ROUGE-L, Distinct-1/2, Recipe-level coherence, Recipe step entailment, Human evaluation. davidjl\n\n    BPE perplexity, BLEU-1/4, ROUGE-L, Distinct-1/2, Recipe-level coherence, Recipe step entailment, Human evaluation. davidjl\n\n    BPE perplexity, BLEU-1/4, ROUGE-L, Distinct-1/2, Recipe-level coherence, Recipe step entailment, Human evaluation. davidjl\n\n    BPE perplexity, BLEU-1/4, RO", "answers": ["Byte-Pair Encoding perplexity  (BPE PPL),\nBLEU-1,\nBLEU-4,\nROUGE-L,\npercentage of distinct unigram (D-1),\npercentage of distinct bigrams(D-2),\nuser matching accuracy(UMA),\nMean Reciprocal Rank(MRR)\nPairwise preference over baseline(PP)", "BLEU-1/4 and ROUGE-L, likelihood of generated recipes using identical input specifications but conditioned on ten different user profiles, user matching accuracy (UMA), Mean Reciprocal Rank (MRR), neural scoring model from BIBREF33 to measure recipe-level coherence", " Distinct-1/2, UMA = User Matching Accuracy, MRR\n= Mean Reciprocal Rank, PP = Pairwise preference over baseline (evaluated for 310 recipe pairs per model)"], "all_classes": null, "length": 2673}
{"pred": "        No Answer, Yes, No, Unanswerable, Yes, No, Unanswerable, Yes, No, Unanswerable, Yes, No, Unanswerable, Yes, No, Unanswerable, Yes, No, Unanswerable, Yes, No, Unanswerable, Yes, No, Unanswerable, Yes, No, Unanswerable, Yes, No, Unanswerable, Yes, No, Unanswerable, Yes, No, Unanswerable, Yes, No, Unanswerable, Yes, No, Unanswerable, Yes, No, Unanswerable, Yes, No,", "answers": ["(1) the time the patient has been experiencing the symptom, (2) activities that trigger the symptom (to occur or worsen), (3) the extent of seriousness, (4) the frequency occurrence of the symptom, and (5) the location of symptom, No Answer", "the time the patient has been experiencing the symptom, activities that trigger the symptom, the extent of seriousness, the frequency occurrence of the symptom, the location of symptom, 9 symptoms"], "all_classes": null, "length": 3424}
{"pred": "        1000 sentences.  // The number of sentences needed to train the task-specific encoder is 1000.  // The number of sentences needed to train the task-specific encoder is 1000.  // The number of sentences needed to train the task-specific encoder is 1000.  // The number of sentences needed to train the task-specific encoder is 1000.  // The number of sentences needed to train the task-specific encoder is 1000.  // The number of sentences needed to train the task-specific encoder is 1000.  // The number of sentences needed to train the task-specific encoder is", "answers": ["57,505 sentences", "57,505 sentences"], "all_classes": null, "length": 4371}
{"pred": "    Neural machine translation tasks. The models are evaluated on four datasets: IWSLT 2017 German $\\rightarrow$ English, KFTT Japanese $\\rightarrow$ English, WMT 2016 Romanian $\\rightarrow$ English, and WMT 2014 English $\\rightarrow$ German.rbrakk\n\nQuestion: What are the main contributions of the paper?\n\nAnswer: 노출등록\n\n    The main contributions of the paper are the introduction of adaptively sparse attention into the Transformer architecture, the proposal of an adaptive version of sparse attention, and an extensive analysis of the added interpretability of these models. The paper also makes a qualitative analysis of the interpret", "answers": ["four machine translation tasks: German -> English, Japanese -> English, Romanian -> English, English -> German", " four machine translation tasks, IWSLT 2017 German $\\rightarrow $ English BIBREF27, KFTT Japanese $\\rightarrow $ English BIBREF28, WMT 2016 Romanian $\\rightarrow $ English BIBREF29, WMT 2014 English $\\rightarrow $ German BIBREF30"], "all_classes": null, "length": 4898}
{"pred": "        The improvement in performance for Estonian in the NER task is 0.12. The baseline fastText embeddings achieve a Macro F1 score of 0.68, while the ELMo embeddings achieve a score of 0.80. This represents a 12% improvement over the baseline. (0.80 - 0.68)", "answers": ["5 percent points.", "0.05 F1"], "all_classes": null, "length": 3290}
{"pred": "    They have backgrounds in computational linguistics, political science, and the social sciences. They have experience working with large-scale textual data and have conducted research on topics such as hate speech, rumors, and conversion. They have also worked on developing computational methods for analyzing text as social and cultural data. davidjl\n\nQuestion: What are the main steps in the research process?\n\nAnswer: davidjl\n\n    1. Identifying the data source(s) 2. Compiling the data 3. Labeling and metadata 4. Conceptualization 5. Operationalization 6. Validation 7. Analysis davidjl\n\nQuestion: What are some", "answers": ["Unanswerable"], "all_classes": null, "length": 8506}
{"pred": "    Yes, the paper introduces an unsupervised approach to spam detection. The proposed method uses LDA to extract topic-based features from the historical tweets of each user, which are then used to discriminate human-like spammers from legitimate users. The method is unsupervised because it does not require labeled data to train a classifier. Instead, it uses the topic distribution of each user to extract features that can distinguish spammers from legitimate users. The paper does not mention any supervised methods for spam detection, so it is likely that the proposed method is unsupervised. However, the paper does not explicitly state whether the method is supervised or uns", "answers": ["No", "No"], "all_classes": null, "length": 2239}
{"pred": "    The Nguni languages are similar to each other and harder to distinguish. The same is true of the Sotho languages. (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (un", "answers": ["Nguni languages (zul, xho, nbl, ssw), Sotho languages (nso, sot, tsn)", "The Nguni languages are similar to each other, The same is true of the Sotho languages"], "all_classes": null, "length": 1877}
{"pred": "    They compared the performance of 6-layers and 9-layers sMBR models. The 9-layers model was initialized with Xavier initialization, and the first 6 layers were initialized by a 6-layers model. The deeper LSTM was also trained in the same way. The teacher model of the 9-layers model was the 8-layers model trained by sMBR, while the other teacher model was CE. The layer-wise training showed that the deeper model performed better than the models with Xavier initialization, as the model was deep. The comparison between the 6-layers and 9-layers s", "answers": ["Unidirectional LSTM networks with 2, 6, 7, 8, and  9 layers."], "all_classes": null, "length": 4286}
{"pred": "    29,794 articles. udál", "answers": ["a sample of  29,794 wikipedia articles and 2,794 arXiv papers "], "all_classes": null, "length": 4187}
{"pred": "        A group of 50 native people who were well-versed in both English and Tamil languages acted as annotators for the evaluation. A collection of samples of about 100 sentences were taken from the test set results for comparison. This set included a randomized selection of the translation results to ensure the objectivity of evaluation. Fluency and adequacy results for the RNNMorph results are tabulated. Adequacy rating was calculated on a 5-point scale of how much of the meaning is conveyed by the translation (All, Most, Much, Little, None). The fluency rating was calculated based on grammatical correctness on a", "answers": ["50 human annotators ranked a random sample of 100 translations by Adequacy, Fluency and overall ranking on a 5-point scale.", "adequacy, precision and ranking values"], "all_classes": null, "length": 5344}
{"pred": "        Yes, they test their framework performance on commonly used language pairs, such as English-to-German. They also test their framework performance on other language pairs, such as French-to-German. Their results show that their framework performs well on both language pairs. They also test their framework performance on a language pair that does not exist any parallel data at all, and their results show that their framework performs well on this language pair as well. Their framework is able to perform multilingual translation without any special treatment in training. Their framework is able to perform multilingual translation without any special treatment in training. Their framework is able to perform multilingual", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4472}
{"pred": "The models are evaluated based on the efficiency and accuracy of the communication schemes they learn. The efficiency is measured by the retention rate of tokens, while the accuracy is measured by the fraction of sentences generated by greedily decoding the model that exactly matches the target sentence. The models are also evaluated for their robustness and ability to adapt to different user preferences. The user study shows that the models are efficient and accurate, and that users can easily adapt to the keyword-based autocomplete system. The models are also shown to be robust to changes in the user's preferences and to be able to learn from a small amount of data. The models are evaluated in", "answers": ["by training an autocomplete system on 500K randomly sampled sentences from Yelp reviews", "efficiency of a communication scheme $(q_{\\alpha },p_{\\beta })$ by the retention rate of tokens, which is measured as the fraction of tokens that are kept in the keywords, accuracy of a scheme is measured as the fraction of sentences generated by greedily decoding the model that exactly matches the target sentence"], "all_classes": null, "length": 1873}
{"pred": "    Precision, recall, and F-measure are looked at for classification tasks. These metrics are used to evaluate the performance of a classifier in identifying the correct class for a given instance. Precision measures the proportion of instances that are correctly classified as positive, while recall measures the proportion of positive instances that are correctly classified. F-measure is a weighted average of precision and recall, which provides a single score that combines both metrics. These metrics are important for evaluating the performance of a classifier and determining whether it is effective in identifying the correct class for a given instance. davidjl\n\n        Precision, recall, and F-measure are looked at for classification", "answers": ["Precision, Recall, F-measure, accuracy", "Precision, Recall and F-measure"], "all_classes": null, "length": 3044}
{"pred": "    The source domain is the existing domain with sufficient labeled data, and the target domain is the new domain with very few or no labeled data. The goal is to transfer knowledge from the source domain to the target domain to alleviate the required labeling effort. (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (", "answers": ["Book, electronics, beauty, music, IMDB, Yelp, cell phone, baby, DVDs, kitchen", "we use set 1 of the source domain as the only source with sentiment label information during training, and we evaluate the trained model on set 1 of the target domain, Book (BK), Electronics (E), Beauty (BT), and Music (M)"], "all_classes": null, "length": 5061}
{"pred": "    They compare with LSTM and other recent state-of-the-art methods. davidjl\n\n    They compare with LSTM and other recent state-of-the-art methods. davidjl\n\n    They compare with LSTM and other recent state-of-the-art methods. davidjl\n\n    They compare with LSTM and other recent state-of-the-art methods. davidjl\n\n    They compare with LSTM and other recent state-of-the-art methods. davidjl\n\n    They compare with LSTM and other recent state-of-the-art methods. davidjl\n\n    They compare with LSTM and other recent state-of-the-art methods. davidjl\n\n    They compare with LSTM and other recent state-of-the-art methods. davidjl\n\n", "answers": ["Variational LSTM, CharCNN, Pointer Sentinel-LSTM, RHN, NAS Cell, SRU, QRNN, RAN, 4-layer skip-connection LSTM, AWD-LSTM, Quantized LSTM"], "all_classes": null, "length": 3319}
{"pred": "    Embedding Layer, Neural Network Layers, Loss Function, Metrics. davidjl\n\n    Embedding Layer, Neural Network Layers, Loss Function, Metrics. davidjl\n\n    Embedding Layer, Neural Network Layers, Loss Function, Metrics. davidjl\n\n    Embedding Layer, Neural Network Layers, Loss Function, Metrics. davidjl\n\n    Embedding Layer, Neural Network Layers, Loss Function, Metrics. davidjl\n\n    Embedding Layer, Neural Network Layers, Loss Function, Metrics. davidjl\n\n    Embedding Layer, Neural Network Layers, Loss Function, Metrics. davidjl\n\n    Embedding Layer, Neural Network Layers, Loss Function, Metrics. davidjl\n\n", "answers": ["Embedding Layer, Neural Network Layers, Loss Function, Metrics", "Embedding Layer, Neural Network Layers, Loss Function, Metrics"], "all_classes": null, "length": 1678}
{"pred": "    The datasets used were the Carnegie Mellon Pronouncing Dictionary and the multilingual pronunciation corpus collected by deri2016grapheme. The corpus consists of spelling–pronunciation pairs extracted from Wiktionary. The corpus is already partitioned into training and test sets. The cleaned version of transcriptions is also provided. The data cleaning attempts to make the transcriptions consistent with the phonemic inventories used in Phoible. The cleaning algorithm can have negative effects on the data quality. The cleaning algorithm removes most suprasegmentals, even though these are often an important part of a language's phonology. Developing a more sophisticated procedure for", "answers": ["the Carnegie Mellon Pronouncing Dictionary BIBREF12, the multilingual pronunciation corpus collected by deri2016grapheme , ranscriptions extracted from Wiktionary", "multilingual pronunciation corpus collected by deri2016grapheme"], "all_classes": null, "length": 3244}
{"pred": "        The baselines were the results reported by Khandelwal and Sawant (BIBREF12) for BERT, XLNet and RoBERTa. The results for BERT are taken from BIBREF12. The results for XLNet and RoBERTa are averaged across 5 runs for statistical significance. (BIBREF12) (BIBREF12) (BIBREF12) (BIBREF12) (BIBREF12) (BIBREF12) (BIBREF12) (BIBREF12) (BIBREF12) (BIBREF12) (BIB", "answers": ["varied from Maximum Entropy Classifiers (BIBREF4) to Support Vector Machines (BIBREF5,BIBREF6,BIBREF7,BIBREF8), Recursive Neural Networks (BIBREF9,BIBREF10), Convolutional Neural Networks (BIBREF11) and most recently transfer learning-based architectures like Bidirectional Encoder Representation from Transformers (BERT) (BIBREF12)"], "all_classes": null, "length": 2215}
{"pred": "    English, Spanish, Finnish, and other languages.", "answers": ["English\nFrench\nSpanish\nGerman\nGreek\nBulgarian\nRussian\nTurkish\nArabic\nVietnamese\nThai\nChinese\nHindi\nSwahili\nUrdu\nFinnish", "English, Spanish, Finnish"], "all_classes": null, "length": 4086}
{"pred": "    They test their method on Named Entity Recognition, POS tagging, text classification, and language modeling. They also test it on a task of document recommendation. davidjl\n\nQuestion: What are the main benefits of their approach compared to other state-of-the-art approaches?\n\nAnswer:雅黑\n\n    The main benefits of their approach compared to other state-of-the-art approaches are that it can be easily generalized to unseen words at test time, it does not require preprocessing to find word boundaries, and it can be extended to other languages. davidjl\n\nQuestion: What are some of the strengths of their method that they highlight?\n\nAnswer:雅黑\n\n    Some of", "answers": ["None"], "all_classes": null, "length": 2473}
{"pred": "Yes, they use pretrained Glove embeddings. They initialize the embeddings of the top 20K words in the vocabulary with 300-dimensional Glove embeddings. davidjl\n\nQuestion: What is the main contribution of this work?\n\nAnswer: overposting\n\n     overposting\n\nThe main contribution of this work is the introduction of neural components for fused bifocal attention and gated orthogonalization to address specific characteristics of the problem of generating natural language descriptions from structured data. The proposed model outperforms existing state-of-the-art models on a large-scale WikiBio dataset by 21%. Additionally, the authors introduce datasets for French and German and demonstrate that their model gives state", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4457}
{"pred": "        Yes, PolyReponse was evaluated against a baseline that used a structured ontology and a task-specific ontology. The results showed that PolyReponse outperformed the baseline in terms of accuracy and relevance. Moreover, PolyReponse was able to handle more complex and natural language inputs than the baseline. Therefore, the answer to the question is \"yes\".΄΄΄΄΄΄΄΄΄΄΄΄΄΄΄΄΄΄΄΄΄΄΄΄΄΄΄΄΄΄΄΄΄΄΄΄΄΄΄΄΄΄΄΄΄΄΄΄΄΄΄΄΄΄΄΄΄", "answers": ["No", "No"], "all_classes": null, "length": 2738}
{"pred": "        They obtain psychological dimensions of people by analyzing the language used in their blogs. The blog posts are cleaned of HTML tags and tokenized, and then the language use is analyzed to generate maps that reflect the demographics of the population, variations in language use, and differences in psycholinguistic and semantic categories. These mappings can be valuable to both psychologists and linguists. (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no", "answers": ["using the Meaning Extraction Method", "Unanswerable"], "all_classes": null, "length": 1440}
{"pred": "        The ML methods aim to identify claims, premises, backing, rebuttals, and refutations. These components are the main building blocks of an argument, and their identification is crucial for understanding the structure and logic of an argument. The methods use a variety of linguistic features to identify these components, such as n-grams, structural features, syntactic features, topic distribution, sentiment distribution, semantic features, coreference features, discourse features, and features based on word embeddings. The goal is to develop a system that can automatically identify these components in user-generated Web content and provide a deeper understanding of the argumentative discourse. (unanswer", "answers": ["claim, premise, backing, rebuttal, and refutation", "claim, premise, backing, rebuttal, refutation"], "all_classes": null, "length": 14472}
{"pred": "        4-grams (n=4) are aligned using PARENT. The n-grams are aligned using an entailment model to decide if they are entailed by the table. The entailment model is trained on the training set of WikiBio. The n-grams are aligned using an entailment model to decide if they are entailed by the table. The entailment model is trained on the training set of WikiBio. The n-grams are aligned using an entailment model to decide if they are entailed by the table. The entailment model is trained on the training set of WikiBio. The n-grams", "answers": ["Unanswerable", "Answer with content missing: (Parent subsections) combine precisions for n-gram orders 1-4"], "all_classes": null, "length": 3827}
{"pred": "        1,873 conversation threads, roughly 14k tweets. (Twitter) (14k tweets) (1,873 conversation threads) (Twitter) (14k tweets) (1,873 conversation threads) (Twitter) (14k tweets) (1,873 conversation threads) (Twitter) (14k tweets) (1,873 conversation threads) (Twitter) (14k tweets) (1,873 conversation threads) (Twitter) (14k tweets) (1,873 conversation threads) (Twitter) (14k tweets) (1,873 conversation threads) (Twitter) (14k tweets) (", "answers": ["1,873 Twitter conversation threads, roughly 14k tweets", "1,873 Twitter conversation threads, roughly 14k tweets"], "all_classes": null, "length": 3721}
{"pred": "        English, Finnish, Spanish, Mandarin Chinese, French, Russian, Polish, Hebrew, Estonian, Welsh, Kiswahili, and Yue Chinese.        //        12 languages: English, Finnish, Spanish, Mandarin Chinese, French, Russian, Polish, Hebrew, Estonian, Welsh, Kiswahili, and Yue Chinese.        //        12 languages: English, Finnish, Spanish, Mandarin Chinese, French, Russian, Polish, Hebrew, Estonian, Welsh, Kiswahili, and Yue Chinese.        //        12 languages: English, Finnish, Spanish, Mandarin Chinese, French, Russian,", "answers": ["Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese", "Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese"], "all_classes": null, "length": 14660}
{"pred": "    Wikipedia data and Reddit CMV data.", "answers": [" `Conversations Gone Awry' dataset, subreddit ChangeMyView", "An expanded version of the existing 'Conversations Gone Awry' dataset and the ChangeMyView dataset, a subreddit whose only annotation is whether the conversation required action by the Reddit moderators. "], "all_classes": null, "length": 4718}
{"pred": "No, the pipeline components were not based on deep learning models. The pipeline components were based on existing natural language processing (NLP) tools and techniques, such as part-of-speech tagging, named entity recognition, dependency parsing, and semantic role labeling. These tools and techniques are not based on deep learning models, but rather on statistical and rule-based approaches. The pipeline components were integrated and customized to work together to perform the full processing pipeline for Portuguese texts. The pipeline components were not based on deep learning models, but rather on existing NLP tools and techniques. The pipeline components were integrated and customized to work together to perform the full processing", "answers": ["No", "No"], "all_classes": null, "length": 2276}
{"pred": "    The quality of the data is empirically evaluated by applying various sanity checks to the translations. These checks include computing sentence-level BLEU scores with the NLTK implementation between the human translations and the automatic translations produced by a state-of-the-art system, manually inspecting examples where the source transcript is identical to the translation, measuring the perplexity of the translations using a language model trained on a large amount of clean monolingual data, computing the ratio of English characters in the translations, and calculating similarity scores between transcripts and translations based on LASER cross-lingual sentence embeddings. Samples with low scores are manually inspected and sent back for translation", "answers": ["Validated transcripts were sent to professional translators., various sanity checks to the translations,  sanity check the overlaps of train, development and test sets", "computed sentence-level BLEU, We manually inspected examples where the source transcript was identical to the translation, measured the perplexity of the translations, computed the ratio of English characters in the translations, calculate similarity scores between transcripts and translations"], "all_classes": null, "length": 2435}
{"pred": "    The authors propose a novel multimodal dual recurrent encoder model that simultaneously utilizes text data, as well as audio signals, to permit the better understanding of speech data. The model encodes the information from audio and text sequences using dual RNNs and then combines the information from these sources using a feed-forward neural model to predict the emotion class. The authors conduct extensive experiments to show that their proposed model outperforms other state-of-the-art methods in classifying the four emotion categories, and accuracies ranging from 68.8% to 71.8% are obtained when the model is applied to the IEMOCAP dataset.", "answers": ["combines the information from these sources using a feed-forward neural model", "encodes the information from audio and text sequences using dual RNNs and then combines the information from these sources using a feed-forward neural model"], "all_classes": null, "length": 3201}
{"pred": "        2.11 BLEU, 1.7 FKGL and 1.07 SARI. (The model improved by 2.11 BLEU, 1.7 FKGL and 1.07 SARI.)       \n\n        2.11 BLEU, 1.7 FKGL and 1.07 SARI. (The model improved by 2.11 BLEU, 1.7 FKGL and 1.07 SARI.)       \n\n        2.11 BLEU, 1.7 FKGL and 1.07 SARI. (The model improved by ", "answers": ["For the WikiLarge dataset, the improvement over baseline NMT is 2.11 BLEU, 1.7 FKGL and 1.07 SARI.\nFor the WikiSmall dataset, the improvement over baseline NMT is  8.37 BLEU.", "6.37 BLEU"], "all_classes": null, "length": 2271}
{"pred": "        52% of the cases. (Table TABREF30) davidjl\n\n        73% of the cases. (Table TABREF30) davidjl\n\n        14% of the cases. (Figure FIGREF38) davidjl\n\n        82.5. (Figure FIGREF38) davidjl\n\n        20% of the cases. (Figure FIGREF38) davidjl\n\n        40% of the cases. (Figure FIGREF38) davidjl\n\n        14% of the cases. (Figure FIGREF38) davidjl\n\n        82.5. (Figure FIGREF38) davidjl\n\n        20%", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3711}
{"pred": "    A tweet goes viral if it is retweeted more than 1000 times. (1) A tweet goes viral if it is retweeted more than 1000 times. (2) A tweet goes viral if it is retweeted more than 1000 times. (3) A tweet goes viral if it is retweeted more than 1000 times. (4) A tweet goes viral if it is retweeted more than 1000 times. (5) A tweet goes viral if it is retweeted more than 1000 times. (6) A tweet goes viral if it is retweeted more than 100", "answers": ["Viral tweets are the ones that are retweeted more than 1000 times", "those that contain a high number of retweets"], "all_classes": null, "length": 3144}
{"pred": "        BERT performs best by itself. (SLC) (FLC) (SLC) (FLC) (SLC) (FLC) (SLC) (FLC) (SLC) (FLC) (SLC) (FLC) (SLC) (FLC) (SLC) (FLC) (SLC) (FLC) (SLC) (FLC) (SLC) (FLC) (SLC) (FLC) (SLC) (FLC) (SLC) (FLC) (SLC) (FLC)", "answers": ["BERT"], "all_classes": null, "length": 1507}
{"pred": "The data was collected using crowdsourcing and an Android application. Each respondent installed the application on their personal device and recorded several phrases in different sessions. The Android application did various checks on each utterance and if it passed all of them, the respondent was directed to the next phrase. For more information about data collection scenario, please refer to BIBREF4.♪\n\n        نويسنده", "answers": ["Android application"], "all_classes": null, "length": 3795}
{"pred": "    Deep learning models and logistic regression classifiers are used for RQE. The deep learning model is based on a neural network with a sentence embedding model and a word embedding model. The logistic regression classifier uses a list of features to classify question pairs as entailed or not entailed. The features include similarity measures between questions and morphosyntactic features. The question type is also used as a feature. The deep learning model achieved better results on three datasets, while the logistic regression classifier achieved the best accuracy on the clinical-RQE dataset. When tested on medical RQE data, the deep learning models did not perform well, while the logistic regression", "answers": ["Logistic Regression, neural networks"], "all_classes": null, "length": 7257}
{"pred": "    The benchmark dataset is the Social Honeypot dataset, which is a collection of legitimate users and spammers on Twitter. The quality of the dataset is high, as it was created by deploying 60 seed social accounts on Twitter to attract spammers and collecting their interactions with legitimate users. The dataset has been extensively explored in the paper and has been used to validate the effectiveness of the proposed features. (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswer", "answers": ["Social Honeypot dataset (public) and Weibo dataset (self-collected); yes", "Social Honeypot, which is not of high quality"], "all_classes": null, "length": 2242}
{"pred": "    The decoder has an LSTM one component at a time, e.g. the tag PRO;NOM;SG;1 is predicted as a sequence of four components, PRO, NOM, SG, 1. davidjl\n\nQuestion: What is the main objective of the auxiliary task?\n\nAnswer: overposting", "answers": ["LSTM", "LSTM"], "all_classes": null, "length": 2289}
{"pred": "        No, they report results on English and Chinese data. They also report results on different datasets, including social media (FSD and Twitter) and news media (Google). The results show that AEM performs better than the baseline approaches on all datasets.", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3838}
{"pred": "        BERT with ensemble voting scheme has the best performance with F1 score of 0.673 on dev (external) set. The performance of BERT alone is 0.665. The performance of CNN alone is 0.662. The performance of Logistic Regression alone is 0.661. The performance of ensemble voting scheme is 0.673. The performance of BERT with postprocess step is 0.672. The performance of BERT with postprocess step and ensemble voting scheme is 0.673. The performance of BERT with postprocess step and ensemble voting scheme is 0.673. The", "answers": ["For SLC task, the \"ltuorp\" team  has the best performing  model (0.6323/0.6028/0.6649 for F1/P/R  respectively) and for FLC task the \"newspeak\" team  has the best performing  model (0.2488/0.2863/0.2201 for F1/P/R respectively)."], "all_classes": null, "length": 1541}
{"pred": "        The baseline was the M2M Transformer NMT model (b3) which achieved best results for most of the translation directions and competitive results for the rest. (b3) is the baseline. (b3) is the baseline. (b3) is the baseline. (b3) is the baseline. (b3) is the baseline. (b3) is the baseline. (b3) is the baseline. (b3) is the baseline. (b3) is the baseline. (b3) is the baseline. (b3) is the baseline. (b3) is the baseline", "answers": ["pivot-based translation relying on a helping language BIBREF10, nduction of phrase tables from monolingual data BIBREF14 , attentional RNN-based model (RNMT) BIBREF2, Transformer model BIBREF18, bi-directional model BIBREF11, multi-to-multi (M2M) model BIBREF8, back-translation BIBREF17", "M2M Transformer"], "all_classes": null, "length": 4542}
{"pred": "    0.7033\n\nQuestion: What was their lowest precision score?\n\nAnswer: 노출등록\n\n     노출등록assistant？」\n\n    0.1119\n\nQuestion: What was their highest F-measure score?\n\nAnswer: 노출등록\n\n     노출등록assistant？」\n\n    0.2862\n\nQuestion: What was their lowest F-measure score?\n\nAnswer: 노출등록\n\n     노출등록assistant？」\n\n    0.0786\n\nQuestion: What was their highest MRR score?\n\nAnswer: 노출등록\n\n     노출등록assistant？」\n\n    0.5\n\nQuestion: What was their lowest MRR score?\n\nAnswer: 노출등록\n\n     노출등록assistant？」\n\n    0", "answers": ["0.7033", "0.7033"], "all_classes": null, "length": 6810}
{"pred": "    Word embeddings are explored in the paper. The basic idea is that the neural network learns a representation of a word by word co-occurrence matrix. The authors train both the Skip-gram and CBOW models over the PubMed Central Open Access (PMC) corpus of approximately 1.25 million articles. They evaluate the models on a subset of the UMNSRS data, removing word pairs that did not occur in their training corpus more than ten times. The authors also evaluate the models on a subset of the UMNSRS ignoring those words that did not appear in their training corpus. Finally, the authors train a CBOW model over", "answers": ["Skip–gram, CBOW", "integrated vector-res, vector-faith, Skip–gram, CBOW"], "all_classes": null, "length": 4259}
{"pred": "    The words are matched before reordering them by translating each word in the source language into English using a bilingual dictionary. The resulting English words are then used to reorder the sentences.雅黑\n\nQuestion: What are the limitations of the current work?\n\nAnswer:limitations of the current work are that the quality of publicly available bilingual embeddings for English-Indian languages is very low for obtaining good-quality, bilingual representations, and that these embeddings were not useful for transfer learning.雅黑\n\nQuestion: What are the results of the experiments?\n\nAnswer: The results of the experiments show that both the pre-ordering configurations significantly improve the BLEU scores over the baseline", "answers": ["Unanswerable", "CFILT-preorder system"], "all_classes": null, "length": 2231}
{"pred": "        No, the paper does not explore extraction from electronic health records. The paper focuses on extraction from scientific articles. The paper mentions that electronic health records are a potential source of information, but does not explore extraction from this source. The paper also mentions that extraction from electronic health records is a challenging task due to the complexity of the data and the lack of standardization. The paper suggests that future work could focus on developing methods for extracting information from electronic health records. However, the paper does not provide any specific details or results on this topic. The paper is primarily focused on extraction from scientific articles. (unanswerable) (yes)", "answers": ["Yes"], "all_classes": null, "length": 3035}
{"pred": "    The experts used for annotation were seven legal experts with training in privacy law. They were tasked with identifying relevant evidence within the privacy policy, as well as providing meta-annotation on the question's relevance, subjectivity, OPP-115 category, and how likely any privacy policy is to contain the answer to the question asked. davidjl\n\nQuestion: What is the distribution of questions in the corpus across OPP-115 categories?\n\nAnswer: davidjl\n\n     davidjl\n\n    First party and third party related questions are the largest categories, forming nearly 66.4% of all questions asked to the privacy assistant. davidjl\n\nQuestion: What are the sources", "answers": ["Individuals with legal training", "Yes"], "all_classes": null, "length": 3846}
{"pred": "    The models used for painting embedding are a CNN-RNN generative model and a sequence-to-sequence model trained by policy gradient. The models used for language style transfer are a sequence-to-sequence model with global attention and a sequence-to-sequence model with pointer networks. (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes", "answers": ["generating a poem from images we use an existing actor-critic architecture, various types of sequence to sequence models"], "all_classes": null, "length": 1653}
{"pred": "    The RNN layer works better on top of BERT for long documents. The transformer layer works better for short documents. The RNN layer is able to capture long-term dependencies in the document, while the transformer layer is better at capturing short-term dependencies. The RNN layer also has a lower computational complexity than the transformer layer, making it more suitable for long documents. However, the transformer layer is more efficient for short documents. Therefore, the choice of layer depends on the length of the document and the task at hand. For long documents, the RNN layer is a better choice, while for short documents, the transformer layer is", "answers": ["Transformer over BERT (ToBERT)", "The transformer layer"], "all_classes": null, "length": 2655}
{"pred": "    Yes, the authors hypothesize that humans' robustness to noise is due to their general knowledge. They believe that humans can utilize general knowledge in addition to the knowledge contained in each given passage-question pair, which allows them to achieve good performance on evaluation examples without training examples. The authors also believe that humans can utilize general knowledge to overcome the limitations of neural networks in MRC models. They argue that neural networks can only utilize the knowledge contained in each given passage-question pair, but in addition to this, humans can also utilize general knowledge. A typical category of general knowledge is inter-word semantic connections, which is essential to the reading comprehension", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4127}
{"pred": "        The authors addressed three topics of cyberbullying: personal attack, racism, and sexism. They used three datasets to study the nature of cyberbullying across these topics and social media platforms. The Formspring dataset contains examples of cyberbullying that are not specifically about any single topic, while the Twitter and Wikipedia datasets contain examples of racism and sexism, respectively. The Wikipedia dataset also contains examples of personal attack. The authors used these datasets to study the nature of cyberbullying across different topics and social media platforms. They also used transfer learning to see if the knowledge gained by the DNN models on one dataset can be used to improve", "answers": ["personal attack, racism, and sexism", "racism, sexism, personal attack, not specifically about any single topic"], "all_classes": null, "length": 3244}
{"pred": "        The new context representation is obtained by splitting the context into three disjoint regions based on the two relation arguments: the left context, the middle context and the right context. Since in most cases the middle context contains the most relevant information for the relation, they want to focus on it but not ignore the other regions completely. Hence, they propose to use two contexts: (1) a combination of the left context, the left entity and the middle context; and (2) a combination of the middle context, the right entity and the right context. Due to the repetition of the middle context, they force the network to pay special attention to", "answers": ["They use two independent convolutional and max-pooling layers on (1) a combination of the left context, the left entity and the middle context; and (2) a combination of the middle context, the right entity and the right context. They concatenated the two results after pooling to get the new context representation."], "all_classes": null, "length": 2435}
{"pred": "        4 (PER, LOC, ORG, MISC)", "answers": ["OurNepali contains 3 different types of entities, ILPRL contains 4 different types of entities", "three"], "all_classes": null, "length": 2851}
{"pred": "    The resulting annotated data is of higher quality. The expert annotations are more consistent and accurate than the crowd annotations. The expert annotations also provide more useful information for the information extraction task. The resulting annotated data is more useful for training the information extraction model. The resulting annotated data is more likely to lead to better performance of the information extraction model. The resulting annotated data is more likely to lead to better results in the information extraction task. The resulting annotated data is more likely to lead to better outcomes in the information extraction task. The resulting annotated data is more likely to lead to better insights in the information extraction task. The resulting annotated data is", "answers": ["improvement when the difficult subset with expert annotations is mixed with the remaining crowd annotation is 3.5 F1 score, much larger than when a random set of expert annotations are added"], "all_classes": null, "length": 4399}
{"pred": "        65% of speakers are men, speaking more than 75% of the time. (unanswerable) (yes) (no) (unanswerable) (no) (unanswerable) (no) (unanswerable) (no) (unanswerable) (no) (unanswerable) (no) (unanswerable) (no) (unanswerable) (no) (unanswerable) (no) (unanswerable) (no) (unanswerable) (no) (unanswerable) (no) (unanswerable) (no) (un", "answers": ["Women represent 33.16% of the speakers"], "all_classes": null, "length": 4055}
{"pred": "    The approach achieves state of the art results on the Multi30K dataset. (Standard setup) (Standard setup) (Standard setup) (Standard setup) (Standard setup) (Standard setup) (Standard setup) (Standard setup) (Standard setup) (Standard setup) (Standard setup) (Standard setup) (Standard setup) (Standard setup) (Standard setup) (Standard setup) (Standard setup) (Standard setup) (Standard setup) (Standard setup) (Standard setup) (Standard setup) (Standard setup) (Standard setup) (Standard setup) (Standard setup) (Standard setup) (Standard setup)", "answers": ["the English-German dataset"], "all_classes": null, "length": 1833}
{"pred": "    The strong baselines model is compared to the models of BIBREF20, BIBREF18, BIBREF11, BIBREF17, BIBREF9, BIBREF2, BIBREF4, BIBREF13, BIBREF3, BIBREF1, BIBREF0, BIBREF21, BIBREF22, BIBREF23, BIBREF24, BIBREF25, BIBREF26, BIBREF27, BIBREF29, BIBREF30, BIBREF38, BIBREF33, BIBREF34, B", "answers": ["Baseline models are:\n- Chen et al., 2015a\n- Chen et al., 2015b\n- Liu et al., 2016\n- Cai and Zhao, 2016\n- Cai et al., 2017\n- Zhou et al., 2017\n- Ma et al., 2018\n- Wang et al., 2019"], "all_classes": null, "length": 3629}
{"pred": "    Multilayer Perceptron (MLP) and Logistic Regression (LR) are used as the target models. (1) (2) (3) (4) (5) (6) (7) (8) (9) (10) (11) (12) (13) (14) (15) (16) (17) (18) (19) (20) (21) (22) (23) (24) (25) (26) (27) (28) (29) (30) (31) (32) (33) (34) (35", "answers": ["probabilistic model", "Logistic Regression, Multilayer Perceptron"], "all_classes": null, "length": 4475}
{"pred": "    NLTK, Stanford CoreNLP, and TwitterNLP. (NLTK, Stanford CoreNLP, TwitterNLP) (NLTK, Stanford CoreNLP, TwitterNLP) (NLTK, Stanford CoreNLP, TwitterNLP) (NLTK, Stanford CoreNLP, TwitterNLP) (NLTK, Stanford CoreNLP, TwitterNLP) (NLTK, Stanford CoreNLP, TwitterNLP) (NLTK, Stanford CoreNLP, TwitterNLP) (NLTK, Stanford CoreNLP, TwitterNLP) (NLTK, Stanford CoreNLP, Twitter", "answers": ["BIBREF17, BIBREF18, TensiStrength BIBREF13, TwitterNLP BIBREF6, BIBREF19, CogComp-NLP BIBREF20, Stanford NLP NER BIBREF21", "BIBREF23, BIBREF17, BIBREF18, BIBREF19, BIBREF24, BIBREF25, BIBREF26"], "all_classes": null, "length": 1452}
{"pred": "        SQuAD dataset\n\n        SQuAD dataset\n\n        SQuAD dataset\n\n        SQuAD dataset\n\n        SQuAD dataset\n\n        SQuAD dataset\n\n        SQuAD dataset\n\n        SQuAD dataset\n\n        SQuAD dataset\n\n        SQuAD dataset\n\n        SQuAD dataset\n\n        SQuAD dataset\n\n        SQuAD dataset\n\n        SQuAD dataset\n\n        SQuAD dataset\n\n        SQuAD dataset\n\n        SQuAD dataset\n\n        SQuAD dataset\n\n        SQuAD dataset\n\n        SQuAD dataset\n\n        SQuAD dataset\n\n        S", "answers": ["SQuAD", "SQuAD"], "all_classes": null, "length": 3757}
{"pred": "Existing approaches for modelling urban environments and identifying points-of-interest and itineraries have been proposed, but the usefulness of Flickr for characterizing the natural environment is less well-understood. Social media has been used to conduct manual evaluations of image content with little automated exploitation of the associated tags. One recent exception is a bag-of-words representation derived from Flickr tags that gives promising results for predicting a range of different environmental phenomena. However, ecologists have mostly used social media to conduct manual evaluations of image content with little automated exploitation of the associated tags. One recent exception is a bag-of-words representation derived from Flickr tags that gives promising results for", "answers": ["BOW-Tags, BOW-KL(Tags), BOW-All, GloVe"], "all_classes": null, "length": 4658}
{"pred": "    Yes, they use attention to compute the similarity score between passages and questions. The attention function is used to compute the similarity score between passages and questions as: A question-aware passage representation is computed as: After that, they use the method of BIBREF13 to apply self attention to the passage. Finally, the final memory is formed by concatenating the question-aware passage representation and the passage representation. The attention mechanism is used to focus on the relevant parts of the passage and question, which helps the model to better understand the context and generate more accurate answers. (Note: The attention mechanism is not explicitly mentioned in the article, but", "answers": ["Yes", "Yes"], "all_classes": null, "length": 1687}
{"pred": "        CSAT dataset for CSAT prediction, consisting of spoken transcripts (automatic via ASR). 20 newsgroups for topic identification task, consisting of written text; Fisher Phase 1 corpus for topic identification task, consisting of spoken transcripts (manual).       ", "answers": ["CSAT dataset, 20 newsgroups, Fisher Phase 1 corpus", "CSAT dataset , 20 newsgroups, Fisher Phase 1 corpus"], "all_classes": null, "length": 2652}
{"pred": "    IMDb movie review dataset\n\n    IMDb movie review dataset\n\n    IMDb movie review dataset\n\n    IMDb movie review dataset\n\n    IMDb movie review dataset\n\n    IMDb movie review dataset\n\n    IMDb movie review dataset\n\n    IMDb movie review dataset\n\n    IMDb movie review dataset\n\n    IMDb movie review dataset\n\n    IMDb movie review dataset\n\n    IMDb movie review dataset\n\n    IMDb movie review dataset\n\n    IMDb movie review dataset\n\n    IMDb movie review dataset\n\n    IMDb movie review dataset\n\n    IMDb movie review dataset\n\n    IMDb movie review dataset\n\n    IMDb movie review dataset\n\n    IMDb movie review dataset\n\n    IMDb movie review dataset\n\n    IMDb", "answers": ["the IMDb movie review dataset BIBREF17", "IMDb movie review"], "all_classes": null, "length": 3432}
{"pred": "Yes, previous work has evaluated these tasks in the context of LSTM-based models. The tasks include subject-verb agreement, reflexive anaphora, and negative polarity items. The stimuli used in these previous works were manually constructed and controlled, allowing for greater coverage and control than naturally occurring sentences. The evaluation protocol was also adapted to the bidirectional nature of the BERT model, which required discarding some of the stimuli. The results showed that the BERT models performed very well on all the syntactic tasks, with high scores in the vast majority of cases. The BERT-Base model outperformed the BERT-Large model", "answers": ["Yes", "Yes"], "all_classes": null, "length": 1464}
{"pred": "        No, the datasets for sentiment analysis are not balanced. The datasets are skewed towards positive and neutral sentiments, with negative sentiments being underrepresented. This can lead to biased results and inaccurate predictions. To address this issue, researchers have proposed various techniques such as oversampling, undersampling, and synthetic data generation to balance the datasets. These techniques can help improve the accuracy and fairness of sentiment analysis models. (unanswerable) (yes) (no) (unanswerable) (no) (no) (unanswerable) (no) (no) (unanswerable) (no) (no) (unanswerable", "answers": ["No"], "all_classes": null, "length": 1441}
{"pred": "        The invertibility condition is that the neural projector is invertible, meaning that the inverse of the projection exists. This condition allows for tractable exact inference and marginal likelihood computation. (EQREF19) (EQREF22) (FIGREF24) (FIGREF24) (FIGREF24) (FIGREF24) (FIGREF24) (FIGREF24) (FIGREF24) (FIGREF24) (FIGREF24) (FIGREF24) (FIGREF24) (FIGREF24) (FIGREF24) (FIGREF24) (FIGREF24) (FIGREF24) (FIG", "answers": ["The neural projector must be invertible.", "we constrain our neural projector with two requirements: (1) INLINEFORM0 and (2) INLINEFORM1 exists"], "all_classes": null, "length": 4323}
{"pred": "    The proposed qualitative annotation schema looks like a taxonomy of dimensions of interest, including factual correctness, required reasoning, knowledge, and linguistic complexity. The dimensions are further enriched with metrics based on lexical cues to approximate a lower bound for the complexity of the reading comprehension task. The resulting taxonomy is shown in Figure 10. The full catalogue of features, their description, detailed annotation guidelines, and illustrating examples can be found in Appendix. (Figure 10) (Appendix) (Dimensions of Interest: Factual Correctness, Required Reasoning, Knowledge, Linguistic Complexity) (Metrics based on lexical cues) (Figure 10) (Append", "answers": ["The resulting taxonomy of the framework is shown in Figure FIGREF10", "FIGREF10"], "all_classes": null, "length": 4958}
{"pred": "        89,042 sentence pairs for WikiSmall and 296,402 sentence pairs for WikiLarge. davidjl\n\n        89,042 sentence pairs for WikiSmall and 296,402 sentence pairs for WikiLarge. davidjl\n\n        89,042 sentence pairs for WikiSmall and 296,402 sentence pairs for WikiLarge. davidjl\n\n        89,042 sentence pairs for WikiSmall and 296,402 sentence pairs for WikiLarge. davidjl\n\n        89,042 sentence pairs for WikiSmall and 296,402 sentence pairs for WikiLarge. davidjl\n\n        89,042 sentence pairs for WikiSmall and 296", "answers": ["training set has 89,042 sentence pairs, and the test set has 100 pairs, training set contains 296,402, 2,000 for development and 359 for testing", "WikiSmall  89 142 sentence pair and  WikiLarge 298 761 sentence pairs. "], "all_classes": null, "length": 2266}
{"pred": "    Vanilla ST baseline, Encoder pre-training, Decoder pre-training, Encoder-decoder pre-training, One-to-many setting, Many-to-one setting, Many-to-many setting, Many-to-many+pre-training, Triangle+pre-train. davidjl\n\n        davidjl\n\n        Vanilla ST baseline, Encoder pre-training, Decoder pre-training, Encoder-decoder pre-training, One-to-many setting, Many-to-one setting, Many-to-many setting, Many-to-many+pre-training, Triangle+pre-train. davidjl\n\n        davidjl\n\n        Vanilla ST baseline, Encoder pre-training, Decoder pre-training, Encoder-decoder pre-training, One-to-many setting,", "answers": ["Vanilla ST baseline, encoder pre-training, in which the ST encoder is initialized from an ASR model, decoder pre-training, in which the ST decoder is initialized from an MT model, encoder-decoder pre-training, where both the encoder and decoder are pre-trained, many-to-many multi-task model where the encoders and decoders are derived from pre-trained ASR and MT models, Triangle+pre-train: BIBREF18 DBLP:conf/naacl/AnastasopoulosC18 proposed a triangle multi-task strategy for speech translation", "Vanilla ST baseline, Pre-training baselines, Multi-task baselines, Many-to-many+pre-training, Triangle+pre-train", "Vanilla ST baseline: The vanilla ST BIBREF9 has only a speech encoder and a decoder. It is trained from scratch on the ST-TED corpus.\n\nPre-training baselines: We conduct three pre-training baseline experiments: 1) encoder pre-training, in which the ST encoder is initialized from an ASR model; 2) decoder pre-training, in which the ST decoder is initialized from an MT model; and 3) encoder-decoder pre-training, where both the encoder and decoder are pre-trained. The ASR model has the same architecture with vanilla ST model, trained on the mixture of ST-TED and TED-LIUM2 corpus. The MT model has a text encoder and decoder with the same architecture of which in TCEN. It is first trained on WMT data (out-of-domain) and then fine-tuned on in-domain data.\n\nMulti-task baselines: We also conduct three multi-task baseline experiments including one-to-many setting, many-to-one setting, and many-to-many setting. In the first two settings, we train the model with $\\alpha _{st}=0.75$ while $\\alpha _{asr}=0.25$ or $\\alpha _{mt}=0.25$. For many-to-many setting, we use $\\alpha _{st}=0.6, \\alpha _{asr}=0.2$ and $\\alpha _{mt}=0.2$.. For MT task, we use only in-domain data.\n\nMany-to-many+pre-training: We train a many-to-many multi-task model where the encoders and decoders are derived from pre-trained ASR and MT models. "], "all_classes": null, "length": 4704}
{"pred": "        English and Arabic.", "answers": ["Unanswerable", "English"], "all_classes": null, "length": 4513}
{"pred": "    The models used in the experiment are SVMs, BiLSTMs, and CNNs. The SVMs are used to train on word unigrams, while the BiLSTMs and CNNs are used to train on the input embeddings and average pooling layer. The CNNs are used to train on the same multi-channel inputs as the BiLSTMs. The models are trained on the training data and evaluated by predicting the labels for the held-out test set. The distribution of the training and test data is described in Table 15. The models are compared against baselines of predicting all labels as the majority or minority classes.", "answers": ["linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM trained on word unigrams,  bidirectional Long Short-Term-Memory (BiLSTM),  Convolutional Neural Network (CNN) "], "all_classes": null, "length": 2250}
{"pred": "        The answered questions measure for the usefulness of the answer. The answer is considered useful if it is relevant, accurate, and complete. The answer is also considered useful if it is well-written and easy to understand. The answer is not considered useful if it is irrelevant, inaccurate, or incomplete. The answer is also not considered useful if it is poorly written or difficult to understand. The answer is not considered useful if it is not relevant to the question. The answer is also not considered useful if it is not accurate or complete. The answer is not considered useful if it is not well-written or easy to understand. The answer is not considered", "answers": ["No"], "all_classes": null, "length": 1561}
{"pred": "    GloVe and Edinburgh embeddings were used.雅黑\n\nQuestion: what are the limitations of the system?\n\nAnswer:limitations of the system are that it has difficulties in capturing the overall sentiment and it leads to amplifying or vanishing intensity signals. It also has difficulties in understanding sarcastic tweets and predicting sentences having deeper emotion and sentiment.雅黑\n\nQuestion: what are some potential future directions for research?\n\nAnswer:potential future directions for research include using sentence embeddings to improve performance, benchmarking the performance of the system in a real-world setting, and exploring other affective computing tasks on social media text.雅黑\n\nQuestion: what are some potential", "answers": ["Pretrained word embeddings  were not used", "GloVe, Edinburgh embeddings BIBREF14, Emoji embeddings BIBREF16"], "all_classes": null, "length": 1771}
{"pred": "        The results on the new dataset show that the personalized generative models can generate plausible, personalized, and coherent recipes preferred by human evaluators for consumption. The models also achieve higher levels of recipe-level coherence and recipe step entailment than the baseline model. Moreover, the personalized models are able to personalize the generated recipes to the given user profiles, as measured by user matching accuracy and mean reciprocal rank. The Prior Name model achieves the best user matching accuracy and mean reciprocal rank, indicating that prior recipe names are strong signals for personalization. Overall, the results demonstrate the effectiveness of personalized attention in generating high-quality and specific recipes that align with historical", "answers": ["average recipe-level coherence scores of 1.78-1.82, human evaluators preferred personalized model outputs to baseline 63% of the time"], "all_classes": null, "length": 2666}
{"pred": "        The combination of rewards for reinforcement learning is the harmonic mean of irony reward and sentiment reward. The irony reward is the difference between the irony score of the input sentence and that of the output sentence, while the sentiment reward is one minus the absolute value of the difference between the standardized sentiment score of the input sentence and that of the generated sentence. The harmonic mean encourages the model to focus on both the irony accuracy and the sentiment preservation. (EQREF9) (EQREF10) (EQREF11) (EQREF12) (EQREF13) (EQREF14) (EQREF15) (EQREF16) (", "answers": ["irony accuracy, sentiment preservation", " irony accuracy and sentiment preservation"], "all_classes": null, "length": 4592}
{"pred": "        The authors demonstrate that their model has limitations in generating English poems that work well with Shakespeare style transfer. This happens when the style transfer dataset does not have similar words in the training set of sentences. A solution would be to expand the style transfer dataset, for a better representation of the poem data.", "answers": ["Since we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer", "we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer as shown in Figure FIGREF12 for \"Starry Night\" with a low average content score"], "all_classes": null, "length": 1651}
{"pred": "        The existing benchmarks they compared to were the Affective Text dataset, the Fairy Tales dataset, and the ISEAR dataset. These datasets are commonly used for the evaluation of emotion detection systems. The Affective Text dataset contains news headlines annotated with emotions and valence, while the Fairy Tales dataset contains sentences from fairy tales annotated with emotions. The ISEAR dataset contains reports from psychology questionnaires answered by people with different cultural backgrounds, labeled with emotions. The authors mapped the emotions used in these datasets to a subset of emotions they used in their experiments: anger, joy, sadness, and surprise. They also provided an overview of the emotions", "answers": ["Affective Text, Fairy Tales, ISEAR", " Affective Text dataset, Fairy Tales dataset, ISEAR dataset"], "all_classes": null, "length": 3390}
{"pred": "    The distribution results showed that tweets containing fake news were created more recently, had less favourites, used more hashtags, and had a higher proportion of unverified accounts. They also had a higher ratio of friends/followers, used fewer mentions, and included more URLs. The content of viral fake news was highly polarized. (10 points) (10 points) (10 points) (10 points) (10 points) (10 points) (10 points) (10 points) (10 points) (10 points) (10 points) (10 points) (10 points) (10 points) (10 points) (10 points", "answers": ["Distributions of Followers, Friends and URLs are significantly different between the set of tweets containing fake news and those non containing them, but for Favourites, Mentions, Media, Retweets and Hashtags they are not significantly different"], "all_classes": null, "length": 3164}
{"pred": "        The dataset of hashtags is sourced from the Stanford Sentiment Analysis Dataset. The dataset consists of 1,108 unique English hashtags from 1,268 randomly selected tweets. The hashtags are manually segmented by crowdsourced workers. The dataset also includes additional corrections made by the authors. The authors also created a new expert curated dataset, which includes all 12,594 unique English hashtags and their associated tweets from the same Stanford dataset. The authors used the same Stanford dataset to create both datasets. The authors also used the Stanford Sentiment Analysis Dataset to train their models. The authors used the Stanford Sentiment Analysis Dataset to train their models because it", "answers": ["1,268 randomly selected tweets in the Stanford Sentiment Analysis Dataset BIBREF36, all 12,594 unique English hashtags and their associated tweets from the same Stanford dataset", "Stanford Sentiment Analysis Dataset BIBREF36"], "all_classes": null, "length": 3756}
{"pred": "The corpus contains a variety of accents, including Persian, English, and other languages. The accents are present in the speech data collected from the respondents. The database is designed to be representative of the speech patterns and accents of the general population, so it includes a wide range of accents and dialects. The database is also designed to be representative of the speech patterns and accents of the general population, so it includes a wide range of accents and dialects. The database is also designed to be representative of the speech patterns and accents of the general population, so it includes a wide range of accents and dialects. The database is also designed to", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3794}
{"pred": "    A compact, scalable, and meaningful representation of a set of word vectors. davidjl\n\n    A compact, scalable, and meaningful representation of a set of word vectors. davidjl\n\n    A compact, scalable, and meaningful representation of a set of word vectors. davidjl\n\n    A compact, scalable, and meaningful representation of a set of word vectors. davidjl\n\n    A compact, scalable, and meaningful representation of a set of word vectors. davidjl\n\n    A compact, scalable, and meaningful representation of a set of word vectors. davidjl\n\n    A compact, scalable, and meaningful representation of a set of word vectors. davidjl\n\n    A", "answers": ["Word vectors, usually in the context of others within the same class"], "all_classes": null, "length": 5151}
{"pred": "    The baseline model used is the one proposed by Dunietz and Gillick (BIBREF11). It uses a variety of features to measure the salience of an entity in text. The features include positional features, occurrence frequency, and the internal POS structure of the entity and the sentence it occurs in. (BIBREF11) (BIBREF11) (BIBREF11) (BIBREF11) (BIBREF11) (BIBREF11) (BIBREF11) (BIBREF11) (BIBREF11) (BIBREF11) (BIBREF11", "answers": ["For Article-Entity placement, they consider two baselines: the first one using only salience-based features, and the second baseline checks if the entity appears in the title of the article. \n\nFor Article-Section Placement, they consider two baselines: the first picks the section with the highest lexical similarity to the article, and the second one picks the most frequent section.", "B1. The first baseline uses only the salience-based features by Dunietz and Gillick BIBREF11 ., B2. The second baseline assigns the value relevant to a pair INLINEFORM0 , if and only if INLINEFORM1 appears in the title of INLINEFORM2 .\n\n, S1: Pick the section from template INLINEFORM0 with the highest lexical similarity to INLINEFORM1 : S1 INLINEFORM2, S2: Place the news into the most frequent section in INLINEFORM0"], "all_classes": null, "length": 7891}
{"pred": "    Yes, SemCor3.0 is reflective of English language data in general. It is a large corpus manually annotated with WordNet sense for WSD, and it is the largest corpus used in previous work on WSD. The training corpus is used to train the models, and the evaluation datasets are used to evaluate the performance of the models. The datasets are all-words WSD datasets, and they are standard benchmark datasets from the Senseval and SemEval competitions. The word sense glosses are extracted from WordNet 3.0, which is a standard lexical resource for WSD. The pre-trained BERT model is used", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 2000}
{"pred": "    100 hours of speech data. (Augmented LibriSpeech dataset is 100 hours of speech data.) (Augmented LibriSpeech dataset is 100 hours of speech data.) (Augmented LibriSpeech dataset is 100 hours of speech data.) (Augmented LibriSpeech dataset is 100 hours of speech data.) (Augmented LibriSpeech dataset is 100 hours of speech data.) (Augmented LibriSpeech dataset is 100 hours of speech data.) (Augmented LibriSpeech dataset is 100 hours of speech data.) (Augmented LibriSpeech dataset is 100 hours of speech data.)", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 2410}
{"pred": "        The dataset for fine-grained classification is split in training, development, development_test and test parts. In the rest, we refer to these splits as train, development and test, where train is composed by the training and the development instances. Table TABREF7 presents an overview of the data. As discussed in BIBREF16 and illustrated in the Table, the fine-grained dataset is highly unbalanced and skewed towards the positive sentiment: onlyINLINEFORM0 of the training examples are labeled with one of the negative classes. (Table TABREF7) (BIBREF16) (BIBREF16) (BIBREF", "answers": [" high-quality datasets  from SemEval-2016 “Sentiment Analysis in Twitter” task", " SemEval-2016 “Sentiment Analysis in Twitter”"], "all_classes": null, "length": 2738}
{"pred": "        BERT is used in the fine-tuning process. The pre-trained BERT model is used in the fine-tuning process. The number of Transformer blocks is 12, the number of the hidden layer is 768, the number of self-attention heads is 12, and the total number of parameters of the pre-trained model is 110M. The pre-trained BERT model is used in the fine-tuning process. The number of Transformer blocks is 12, the number of the hidden layer is 768, the number of self-attention heads is 12, and the total number of parameters of the pre-trained", "answers": ["small BERT", "small BERT"], "all_classes": null, "length": 1999}
{"pred": "    Yes, the datasets are subject to quality control. The authors use crowdsourcing to validate samples of the data and find high agreement across probes. The human scores correlate with the model trends across the probe categories. The authors also note that the expert knowledge they use is hand-curated by human experts, but is fallible and error-prone. The authors emphasize the importance of validating the quality of synthetic datasets at varying levels of complexity. The authors also note that the positive results should be taken with a grain of salt, as there is much room for improvement and the models struggle with certain categories of hard distractors and multi-hop inferences.", "answers": ["No", "No"], "all_classes": null, "length": 6391}
{"pred": "        Yes, the images are from a specific domain. The images are from the ShapeWorld framework, which consists of abstract colored shapes. The images are used to evaluate image captioning models. The images are used to evaluate the grammaticality, truthfulness, and diversity of the captions generated by the models. The images are used to evaluate the performance of the models on the ShapeWorldICE datasets. The images are used to evaluate the performance of the models on the ShapeWorldICE datasets. The images are used to evaluate the performance of the models on the ShapeWorldICE datasets. The images are used to evaluate the performance of the models on", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3472}
{"pred": "        The model achieved competitive results on standard benchmarks, without relying on any handcrafted resources. The best model (B-M) achieved an average f-score of 0.368 on the development set. On the three standard datasets, the model achieved an average f-score of 0.55, 0.58, and 0.62, respectively. The model performed better than some existing models, but not as well as others. The model's performance was affected by the choice of Facebook pages and features used. The model's performance could be improved by training emotion-aware embeddings and exploring different ways of selecting training instances. The model's", "answers": ["Answer with content missing: (Table 3) Best author's model B-M average micro f-score is 0.409, 0.459, 0.411 on Affective, Fairy Tales and ISEAR datasets respectively. "], "all_classes": null, "length": 3410}
{"pred": "        {INLINEFORM0}, {INLINEFORM1} and {INLINEFORM2} for the simple tagging scheme, and {INLINEFORM3}, {INLINEFORM4} and {INLINEFORM5} for the new tagging scheme. {INLINEFORM6} and {INLINEFORM7} for the new tagging scheme. {INLINEFORM8} for the new tagging scheme. {INLINEFORM9} for the new tagging scheme. {INLINEFORM10} for the new tagging scheme. {INLINEFORM11} for the new tagging scheme. {INLINEFORM12} for the new tagging scheme. {INLINEFORM13} for the new tagging scheme", "answers": ["A new tagging scheme that tags the words before and after the pun as well as the pun words.", "a new tagging scheme consisting of three tags, namely { INLINEFORM0 }"], "all_classes": null, "length": 2974}
{"pred": "Yes, Arabic is one of the 11 languages in CoVost. The other languages are French, German, Dutch, Russian, Spanish, Italian, Turkish, Persian, Swedish, Mongolian, and Chinese. The corpus is diversified with over 11,000 speakers and over 60 accents. The corpus includes a total of 708 hours of speech in 11 languages, with French and German having the largest durations among existing public corpora. The corpus is also diversified with multiple speakers per transcript/translation, and includes additional out-of-domain test sets. The corpus is free to use with a CC0 license, and the additional Tato", "answers": ["No", "No"], "all_classes": null, "length": 2413}
{"pred": "        A model is considered robust if it is able to handle and perform well in situations where the prior knowledge is biased or incomplete. The model should be able to adapt to new data and not be overly reliant on the prior knowledge. The model should also be able to generalize to new data and not overfit to the training data. The model should be able to handle and perform well in situations where the prior knowledge is biased or incomplete. The model should be able to adapt to new data and not be overly reliant on the prior knowledge. The model should also be able to generalize to new data and not overfit to the training data. The", "answers": ["ability to accurately classify texts even when the amount of prior knowledge for different classes is unbalanced, and when the class distribution of the dataset is unbalanced", "Low sensitivity to bias in prior knowledge"], "all_classes": null, "length": 3609}
{"pred": "    InferSent and Universal Sentence Encoder.", "answers": ["GloVe, BERT, Universal Sentence Encoder, TF-IDF, InferSent", "Avg. GloVe embeddings, Avg. fast-text embeddings, Avg. BERT embeddings, BERT CLS-vector, InferSent - GloVe and Universal Sentence Encoder."], "all_classes": null, "length": 3862}
{"pred": "        The proposed method outperforms BERT-MRC by +0.29 and +0.96 respectively for English datasets including CoNLL2003 and OntoNotes5.0. For Chinese datasets, the proposed method achieves F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively. The proposed method sets new SOTA performances on all of the four NER datasets. (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable)", "answers": ["English datasets including CoNLL2003 and OntoNotes5.0, our proposed method outperforms BERT-MRCBIBREF38 by +0.29 and +0.96 respectively, Chinese datasets, achieving F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively", "For English datasets including CoNLL2003 and OntoNotes5.0, our proposed method outperforms BERT-MRCBIBREF38 by +0.29 and +0.96 respectively., huge performance boosts on Chinese datasets, achieving F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively"], "all_classes": null, "length": 3586}
{"pred": "        They test their conflict method on two tasks: Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask. They use the same model architecture for both tasks, but the input data and the output layer are different. For Quora Duplicate Question Pair Detection, the input data consists of pairs of questions and the output layer predicts whether the pair is duplicate or not. For Ranking questions in Bing's People Also Ask, the input data consists of user queries and candidate questions, and the output layer predicts whether the candidate question is a high-click question or not. The conflict method is used in the interaction part of the model, which", "answers": ["Task 1: Quora Duplicate Question Pair Detection, Task 2: Ranking questions", "Quora Duplicate Question Pair Detection, Ranking questions in Bing's People Also Ask"], "all_classes": null, "length": 2577}
{"pred": "    Tree-based CNN, Gumbel Tree-LSTM, Latent Syntax Tree-LSTM, NSE, Reinforced Self-Attention Network, Residual stacked encoders, BiLSTM with generalized pooling.", "answers": ["Various tree structured neural networks including variants of Tree-LSTM, Tree-based CNN, RNTN, and non-tree models including variants of LSTMs, CNNs, residual, and self-attention based networks", "Sentence classification baselines: RNTN (Socher et al. 2013), AdaMC-RNTN (Dong et al. 2014), TE-RNTN (Qian et al. 2015), TBCNN (Mou et al. 2015), Tree-LSTM (Tai, Socher, and Manning 2015), AdaHT-LSTM-CM (Liu, Qiu, and Huang 2017), DC-TreeLSTM (Liu, Qiu, and Huang 2017), TE-LSTM (Huang, Qian, and Zhu 2017), BiConTree (Teng and Zhang 2017), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), TreeNet (Cheng et al. 2018), CNN (Kim 2014), AdaSent (Zhao, Lu, and Poupart 2015), LSTM-CNN (Zhou et al. 2016), byte-mLSTM (Radford, Jozefowicz, and Sutskever 2017), BCN + Char + CoVe (McCann et al. 2017), BCN + Char + ELMo (Peters et al. 2018). \nStanford Natural Language Inference baselines: Latent Syntax Tree-LSTM (Yogatama et al. 2017), Tree-based CNN (Mou et al. 2016), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), NSE (Munkhdalai and Yu 2017), Reinforced Self- Attention Network (Shen et al. 2018), Residual stacked encoders: (Nie and Bansal 2017), BiLSTM with generalized pooling (Chen, Ling, and Zhu 2018)."], "all_classes": null, "length": 4781}
{"pred": "    The core component for KBQA is relation detection. The relation detection model is used to identify the relations between entities in the KB and the question. The identified relations are then used to generate the KB query that is used to retrieve the answers from the KB. The relation detection model is a key component in KBQA systems and plays a crucial role in determining the accuracy and performance of the system. The relation detection model is typically implemented using deep learning techniques such as neural networks and recurrent neural networks. The model is trained on a large corpus of KB tuples and questions to learn the patterns and relationships between entities and questions. The model is then used", "answers": ["answer questions by obtaining information from KB tuples ", "hierarchical matching between questions and relations with residual learning"], "all_classes": null, "length": 4527}
{"pred": "    The baseline models are a name-based Nearest-Neighbor model (NN) and an Encoder-Decoder model with ingredient attention (Enc-Dec). The Encoder-Decoder model is used as a strong non-personalized baseline. The Nearest-Neighbor model is used as a baseline for personalization.", "answers": ["name-based Nearest-Neighbor model (NN), Encoder-Decoder baseline with ingredient attention (Enc-Dec)"], "all_classes": null, "length": 2655}
{"pred": "        The methods considered to find examples of biases and unwarranted inferences are manual detection, part-of-speech tagging, and leveraging the structure of Flickr30K Entities. The manual detection method involves looking at a collection of images and identifying patterns. Part-of-speech tagging involves tagging all descriptions with part-of-speech information to see which adjectives are most commonly used for particular nouns. Leveraging the structure of Flickr30K Entities involves using coreference annotations to create a coreference graph and applying Louvain clustering to the coreference graph to identify clusters of expressions that refer to similar entities. These methods help to get a sense of", "answers": ["spot patterns by just looking at a collection of images, tag all descriptions with part-of-speech information, I applied Louvain clustering", "Looking for adjectives marking the noun \"baby\" and also looking for most-common adjectives related to certain nouns using POS-tagging"], "all_classes": null, "length": 2204}
{"pred": "        English and French. davidjl", "answers": ["English, French, German ", "French, English, Spanish, Italian, Portuguese, Hebrew, Arabic"], "all_classes": null, "length": 2285}
{"pred": "    They experimented with stacked LSTMs, Cell-aware Stacked LSTMs (CAS-LSTMs), and variants of CAS-LSTMs.", "answers": ["Stacked LSTMs, Cell-aware Stacked LSTMs, Sentence Encoders, Top-layer Classifiers"], "all_classes": null, "length": 3210}
{"pred": "        Yes, they report results only on English data. davidjl\n\n        No, they report results on both English and non-English data. davidjl\n\n        Unanswerable. davidjl\n\n        Yes, they report results only on English data. davidjl\n\n        No, they report results on both English and non-English data. davidjl\n\n        Unanswerable. davidjl\n\n        Yes, they report results only on English data. davidjl\n\n        No, they report results on both English and non-English data. davidjl\n\n        Unanswerable. davidjl\n\n        Yes, they report results only on English data. davidjl\n\n        No, they", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 6169}
{"pred": "    The authors experimented with a few summarization algorithms provided by the Sumy package. These algorithms include TextRank, LexRank, and SumBasic. The authors also implemented their own ILP-based summarization algorithm.", "answers": ["LSA, TextRank, LexRank and ILP-based summary.", "LSA, TextRank, LexRank"], "all_classes": null, "length": 3045}
{"pred": "        The previous state of the art for this task was a probabilistic graphical model that used latent post categories to model the thread sequence and infer states that triggered intervention. However, this model required a hyperparameter for the number of latent states, which may not generalize well. (BIBREF0) (BIBREF7) (BIBREF1) (BIBREF8) (BIBREF2) (BIBREF3) (BIBREF4) (BIBREF5) (BIBREF6) (BIBREF7) (BIBREF9) (BIBREF10) (B", "answers": ["hLSTM", "hLSTM"], "all_classes": null, "length": 3725}
{"pred": "        The master node skip connection. The reason behind this choice is that we expect the special document node to learn a high-level summary about the document, such as its size, vocabulary, etc. (more details are given in subsection SECREF30). Therefore, by making the master node bypass the attention layer, we directly inject global information about the document into its final representation. (more details are given in subsection SECREF16). (more details are given in subsection SECREF30). (more details are given in subsection SECREF16). (more details are given in subsection SECREF30). (more details are given in subsection SECREF", "answers": ["Based on table results provided changing directed to undirected edges had least impact - max abs difference of 0.33 points on all three datasets."], "all_classes": null, "length": 4180}
{"pred": "    The corpus used for the task is DTA18 and DTA19, which are subparts of the DTA corpus. These corpora consist of texts from the 16th to the 20th century, and the corpus version used in the task has the following format: \"year [tab] lemma1 lemma2 lemma3...\". The two corpora correspond to the diachronic corpus pair from BIBREF0: DTA18 and DTA19. They consist of subparts of DTA corpus BIBREF11 which is a freely available lemmatized, POS-tagged and spelling-normalized diach", "answers": ["DTA18, DTA19", "Diachronic Usage Relatedness (DURel) gold standard data set"], "all_classes": null, "length": 1908}
{"pred": "        Kannada, Hindi, Telugu, Malayalam, Bengali, and English\n\n        Kannada, Hindi, Telugu, Malayalam, Bengali, and English\n\n        Kannada, Hindi, Telugu, Malayalam, Bengali, and English\n\n        Kannada, Hindi, Telugu, Malayalam, Bengali, and English\n\n        Kannada, Hindi, Telugu, Malayalam, Bengali, and English\n\n        Kannada, Hindi, Telugu, Malayalam, Bengali, and English\n\n        Kannada, Hindi, Telugu, Malayalam, Bengali, and English\n\n        Kann", "answers": ["Hindi, English, Kannada, Telugu, Assamese, Bengali and Malayalam", "Kannada, Hindi, Telugu, Malayalam, Bengali,  English and Assamese (in table, missing in text)"], "all_classes": null, "length": 2453}
{"pred": "The model performance on target language reading comprehension is reasonable, with EM/F1 scores of 53.8/44.1 on Chinese and 44.1/33.3 on Korean. However, the performance is degraded when the training data is translated into the target language, indicating that translation degrades the quality of data. The results show that the quality and the size of dataset are much more important than whether the training and testing are in the same language or not. (Unanswerable) (Yes) (No) (Unanswerable) (Yes) (No) (Unanswerable) (Yes) (No)", "answers": ["Table TABREF6, Table TABREF8", "when testing on English, the F1 score of the model training on Chinese (Zh) is 53.8,  F1 score is only 44.1 for the model training on Zh-En"], "all_classes": null, "length": 2492}
{"pred": "        The proposed model outperforms the baselines by a significant margin. The difference in performance is consistent across all five evaluation characters. The proposed model is able to recover the language styles of specific characters more accurately than the baselines. The proposed model is also able to recommend tailored responses traceable to specific characters, which is not possible with the baselines. The proposed model is able to effectively use HLAs to improve upon dialogue retrieval performance. The proposed model is able to recover the language styles of various characters regardless of the character's profile and identity, genre of the show, and context of the dialogue. The proposed model is able to", "answers": ["Metric difference between Aloha and best baseline score:\nHits@1/20: +0.061 (0.3642 vs 0.3032)\nMRR: +0.0572(0.5114 vs 0.4542)\nF1: -0.0484 (0.3901 vs 0.4385)\nBLEU: +0.0474 (0.2867 vs 0.2393)"], "all_classes": null, "length": 5151}
{"pred": "        0.5-1.0 improvement in terms of forward/reverse perplexity and Self-BLEU. The model also shows better stability and performance in dialogue generation. (0.5-1.0 improvement in terms of forward/reverse perplexity and Self-BLEU. The model also shows better stability and performance in dialogue generation.) (0.5-1.0 improvement in terms of forward/reverse perplexity and Self-BLEU. The model also shows better stability and performance in dialogue generation.) (0.5-1.0 improvement in terms of forward/reverse perplexity and Self-BLEU", "answers": ["ARAM has achieved improvement over all baseline methods using reverese perplexity and  slef-BLEU metric.  The maximum reverse perplexity improvement 936,16 is gained for EMNLP2017  WMT  dataset and  48,44 for COCO dataset.", "Compared to the baselines, ARAML does not do better in terms of perplexity on COCO and EMNLP 2017 WMT datasets, but it does by up to 0.27 Self-BLEU points on COCO and 0.35 Self-BLEU on EMNLP 2017 WMT. In terms of Grammaticality and Relevance, it scores better than the baselines on up to 75.5% and 73% of the cases respectively."], "all_classes": null, "length": 3796}
{"pred": "        The authors present evidence that the model can capture some biases in data annotation and collection by examining the results of the model and performing a manual inspection on a subset of the data. They record some of the mislabeled items by the model and find that many errors are due to biases from data collection and rules of annotation. They also find that the model can differentiate hate and offensive samples accurately by leveraging knowledge-aware language understanding that it has. (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes", "answers": ["The authors showed few tweets where neither and implicit hatred content exist but the model was able to discriminate"], "all_classes": null, "length": 4119}
{"pred": "        Yes, three baselines were tested to compare with the neural baseline. The first baseline is a simple majority-class baseline, which predicts that all questions are unanswerable. The second baseline is an SVM classifier with bag-of-words features, length of the question in words, and part-of-speech tags for the question. The third baseline is a CNN neural encoder. The results show that the neural baseline performs better than the other baselines. (Table 31) (Table 32) (Section 4.2.1) (Section 4.2.2) (Section 4.2.3) (", "answers": ["SVM, No-Answer Baseline (NA) , Word Count Baseline, Human Performance", "No-Answer Baseline (NA), Word Count Baseline, Human Performance"], "all_classes": null, "length": 3855}
{"pred": "        72782 unique words\n\n        6946 sentences\n\n        16225 unique words\n\n        64%, 16% and 20% of the total dataset into training set, development set and test set respectively\n\n        72782 unique words\n\n        6946 sentences\n\n        16225 unique words\n\n        64%, 16% and 20% of the total dataset into training set, development set and test set respectively\n\n        72782 unique words\n\n        6946 sentences\n\n        16225 unique words\n\n        64%, 16% and 20% of the total dataset into", "answers": ["Dataset contains 3606 total sentences and 79087 total entities.", "ILPRL contains 548 sentences, OurNepali contains 3606 sentences"], "all_classes": null, "length": 2843}
{"pred": "        Using dice loss, the F1 score for paraphrase identification improves by 0.58 for MRPC and 0.73 for QQP. ighter cross entropy loss, the F1 score for paraphrase identification improves by 0.58 for MRPC and 0.73 for QQP. ighter cross entropy loss, the F1 score for paraphrase identification improves by 0.58 for MRPC and 0.73 for QQP. ighter cross entropy loss, the F1 score for paraphrase identification improves by 0.58 for MRPC and 0.73 for QQP. ", "answers": ["Using DSC loss improves the F1 score by +0.58 for MRPC and +0.73 for QQP", "+0.58"], "all_classes": null, "length": 3566}
{"pred": "        The datasets used are EEG data from BIBREF0 and eye-tracking, self-paced reading time, and ERP data. (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable", "answers": ["Answer with content missing: (Whole Method and Results sections) The primary dataset we use is the ERP data collected and computed by Frank et al. (2015), and we also use behavioral data (eye-tracking data and self-paced reading times) from Frank et al. (2013) which were collected on the same set of 205 sentences.\nSelect:\n- ERP data collected and computed by Frank et al. (2015)\n- behavioral data (eye-tracking data and self-paced reading times) from Frank et al. (2013)", "the ERP data: BIBREF0"], "all_classes": null, "length": 1971}
{"pred": "        The subjects were presented with a series of images depicting different events, such as a person walking, a car driving, and a person eating. The images were designed to elicit event-related responses in the form of brain activity. The subjects were instructed to imagine themselves performing the actions depicted in the images. The researchers recorded the brain activity of the subjects while they imagined performing the actions. The brain activity was recorded using electroencephalography (EEG) and magnetoencephalography (MEG) techniques. The researchers then analyzed the brain activity to identify the event-related responses. The results of the analysis were presented in the", "answers": ["7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw)", "KARA ONE BIBREF17 , composed of multimodal data for stimulus-based, imagined and articulated speech state corresponding to 7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw)"], "all_classes": null, "length": 2379}
{"pred": "    Pointer-Gen, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN, Pointer-Gen+ARL-SEN.", "answers": ["Pointer-Gen, Pointer-Gen+Pos, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN"], "all_classes": null, "length": 4085}
{"pred": "    Traditional machine learning models and neural network based models. The traditional machine learning models include Naïve Bayes, Logistic Regression, Support Vector Machine, Random Forests, and Gradient Boosted Trees. The neural network based models include Convolutional Neural Networks, Recurrent Neural Networks, and their variants. The pre-trained GloVe representation is used for word-level features. The HybridCNN model is also investigated. The RNN models use bidirectional GRU cells and attention mechanisms. The LTC method is also applied to extract latent topic information from the hidden states of RNN. The context tweets are used as additional features for the neural network models", "answers": ["Naïve Bayes (NB), Logistic Regression (LR), Support Vector Machine (SVM), Random Forests (RF), Gradient Boosted Trees (GBT),  Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN)", "Naïve Bayes (NB), Logistic Regression (LR), Support Vector Machine (SVM), Random Forests (RF), Gradient Boosted Trees (GBT), CNN, RNN"], "all_classes": null, "length": 2074}
{"pred": "    The bi-directional language model is used to augment the sequence to sequence encoder and the uni-directional model is used to augment the decoder. Both use self-attention and the uni-directional model contains 6 transformer blocks, followed by a word classifier to predict the next word on the right. The bi-directional model solves a cloze-style token prediction task at training time. The model consists of two towers, the forward tower operates left-to-right and the tower operating right-to-left as backward tower; each tower contains 6 transformer blocks. The forward and backward representations are combined via a self-attention module and the output of this", "answers": ["uni-directional model to augment the decoder", "bi-directional language model to augment the sequence to sequence encoder ,  uni-directional model to augment the decoder"], "all_classes": null, "length": 1914}
{"pred": "        The weights are dynamically adjusted by multiplying the soft probability p with a decaying factor (1-p). The intuition is to push down the weight of easy examples. For easy examples whose probability is approaching 0 or 1, (1-p)p makes the model attach significantly less focus to them. The proposed method mimics the idea of focal loss for object detection in vision. Focal loss down-weights the loss assigned to well-classified examples by adding a (1-p)^β factor, leading the final loss to be (1-p)^βlog p. The proposed method uses a dynamic weight adjusting strategy, which associates each training", "answers": ["One can think $(1-p_{i1})$ as a weight associated with each example, which changes as training proceeds. The intuition of changing $p_{i1}$ to $(1-p_{i1}) p_{i1}$ is to push down the weight of easy examples. For easy examples whose probability are approaching 0 or 1, $(1-p_{i1}) p_{i1}$ makes the model attach significantly less focus to them. Figure FIGREF23 gives gives an explanation from the perspective in derivative: the derivative of $\\frac{(1-p)p}{1+(1-p)p}$ with respect to $p$ approaches 0 immediately after $p$ approaches 0, which means the model attends less to examples once they are correctly classified.", "associates each training example with a weight in proportion to $(1-p)$, and this weight dynamically changes as training proceeds"], "all_classes": null, "length": 3640}
{"pred": "        The results show that agents utilizing knowledge-graphs in addition to either enhanced exploration method far outperform the baseline A2C and KG-A2C. KG-A2C-chained and KG-A2C-Explore both pass the bottleneck of a score of 40, whereas A2C-Explore gets to the bottleneck but cannot surpass it. There are a couple of key insights that can be drawn from these results. The first is that the knowledge graph appears to be critical; it is theorized to help with partial observability. However the knowledge graph representation isn't sufficient in that the knowledge graph representation without enhanced exploration methods cannot surpass", "answers": ["Reward of 11.8 for the A2C-chained model, 41.8 for the KG-A2C-chained model, 40 for A2C-Explore and 44 for KG-A2C-Explore.", "KG-A2C-chained and KG-A2C-Explore both pass the bottleneck of a score of 40"], "all_classes": null, "length": 2443}
{"pred": "    An individual model consists of a Bayesian model for each language, and crosslingual latent variables to incorporate soft role agreement between aligned constituents. These latent variables capture correlations between roles in different languages, and regularize the parameter estimates of the monolingual models. The individual models are coupled together through the crosslingual latent variables. The generative process of the model is described in the article. The probability equations for the monolingual model are also given. The multilingual model is deficient, since the aligned roles are being generated twice. The new joint probability can be written as equation UID11 (Figure FIGREF7), which can be", "answers": ["Bayesian model of garg2012unsupervised as our base monolingual model"], "all_classes": null, "length": 3701}
{"pred": "        Non-standard pronunciation is identified by annotating the Spanish words interspersed in Mapudungun speech. The annotations label foreign words as such. (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no", "answers": ["Unanswerable", "Original transcription was labeled with additional labels in [] brackets with nonstandard pronunciation."], "all_classes": null, "length": 3018}
{"pred": "    A semicharacter architecture is a type of neural network that processes a sentence of words with misspelled characters, predicting the correct words at each step. The architecture treats the first and the last characters individually, and is agnostic to the ordering of the internal characters. Each word is represented by concatenating a one-hot vector of the first character, a one-hot representation of the last character, and a bag of characters representation of the internal characters. The architecture uses a BiLSTM cell to process each word, and the training target is the correct corresponding word. The model is optimized with cross-entropy loss. (12)", "answers": ["A semi-character based RNN (ScRNN) treats the first and last characters individually, and is agnostic to the ordering of the internal characters", "processes a sentence of words with misspelled characters, predicting the correct words at each step"], "all_classes": null, "length": 4186}
{"pred": "        Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish.       ยนตร์\n\n        Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish.       ยนตร์\n\n        Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish.       ยนตร์\n\n        Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian,", "answers": ["Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish", "Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish , Swedish"], "all_classes": null, "length": 2697}
{"pred": "The NCEL approach is effective overall, as it outperforms the state-of-the-art collective methods across five different datasets. The results demonstrate its good generalization ability and robustness to noisy data. The qualitative analysis also shows its effectiveness in identifying entities in challenging cases. Overall, the NCEL approach is a promising solution for collective entity linking. (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable", "answers": ["NCEL consistently outperforms various baselines with a favorable generalization ability"], "all_classes": null, "length": 4113}
{"pred": "        Yes, the data is de-identified. The audio recordings have an average duration of 9min 28s and have a verbatim transcript of 1,500 words on average. The sentences in the transcript are grounded to the audio with the timestamps of its first and last word. The transcript of the conversations are annotated with summaries and Medication Regimen tags (MR tags), both grounded using the timestamps of the sentences from the transcript deemed relevant by the expert annotators. The summaries are medically relevant and local. The MR tags are also local and are of the form {Medication Name, Dosage, Frequency}. If dosage", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4592}
{"pred": "        The baseline used was the error detection system by Rei2016, trained using the same FCE dataset. (The FCE dataset is a publicly available dataset of language learner essays, used for training error detection models.) (The Rei2016 system is a neural sequence labeling model for error detection, which receives a sequence of tokens as input and outputs a prediction for each position, indicating whether the token is correct or incorrect in the current context.) (The Rei2016 system is optimised by minimising categorical cross-entropy with respect to the correct labels.) (The Rei2016 system uses AdaDelta for calculating an adaptive learning rate during", "answers": ["error detection system by Rei2016", "error detection system by Rei2016"], "all_classes": null, "length": 2132}
{"pred": "        The annotated clinical notes were obtained from the 2010 i2b2/VA BIBREF0 and 2013 ShARe/CLEF Task 1 BIBREF1 challenges. These challenges ask participants to design an algorithm to tag a set of predefined entities of interest in clinical notes. These entity tagging tasks are also known as clinical Named Entity Recognition (NER). For example, the CE task in 2010 i2b2/VA defines three types of entities: \"problem\", \"treatment\", and \"test\". The CE task in 2013 ShARe/CLEF defines various types of disorder", "answers": ["clinical notes from the CE task in 2010 i2b2/VA", "clinical notes from the CE task in 2010 i2b2/VA "], "all_classes": null, "length": 3432}
{"pred": "        The masking process helps the decoder focus on the context of the summary draft, rather than the specific words in the summary draft. This allows the decoder to generate more context-aware and natural summaries. The masking process also helps the decoder learn to generate more diverse and informative summaries, as it forces the decoder to generate words that are not present in the summary draft. This helps the decoder learn to generate more diverse and informative summaries, as it forces the decoder to generate words that are not present in the summary draft. The masking process also helps the decoder learn to generate more fluent and coherent summaries, as it forces the decoder to generate words that are", "answers": ["ecause this process is similar to the cloze task in BERT's pre-train process, therefore by using the ability of the contextual language model the decoder can generate more fluent and natural sequences."], "all_classes": null, "length": 3919}
{"pred": "        The dataset used is the PPDB dataset. (Unanswerable) (Unanswerable) (Unanswerable) (Unanswerable) (Unanswerable) (Unanswerable) (Unanswerable) (Unanswerable) (Unanswerable) (Unanswerable) (Unanswerable) (Unanswerable) (Unanswerable) (Unanswerable) (Unanswerable) (Unanswerable) (Unanswerable) (Unanswerable) (Unanswerable) (Unanswerable) (Unanswerable) (Unanswerable) (Unanswerable) (Unanswer", "answers": ["Unanswerable", " Paraphrase Database (PPDB) ,  book corpus", "Unanswerable"], "all_classes": null, "length": 1902}
{"pred": "    Term Frequency-Inverse Document Frequency (TF-IDF) features are used to extract and corroborate useful keywords from pathology cancer reports. These features are based on the frequency of terms in a document and the inverse document frequency of those terms. The TF-IDF features are used to create a feature vector for each pathology report, which is then used to train a classification model to predict the primary diagnosis. The TF-IDF features are also used to extract the top 50 keywords from a random report and highlight them using different colors based on their associated topic. The keywords are extracted using latent Dirichlet allocation (LDA) and the topics", "answers": ["Unanswerable"], "all_classes": null, "length": 2108}
{"pred": "        The dataset is annotated based on a hierarchical model of depression-related symptoms. Each tweet is annotated as no evidence of depression or evidence of depression. If there is evidence of depression, the tweet is further annotated with one or more depressive symptoms, such as depressed mood, disturbed sleep, or fatigue or loss of energy. The annotations are binarized as positive or negative classes.\r\r\n\r\r\n        The dataset is encoded with 7 feature groups, including lexical features, syntactic features, emotion features, demographic features, sentiment features, personality traits, and LIWC features. The feature values are binarized as present or absent.\r\r\n\r\r\n        The dataset", "answers": ["no evidence of depression, depressed mood, disturbed sleep, fatigue or loss of energy", "The annotations are based on evidence of depression and further annotated by the depressive symptom if there is evidence of depression"], "all_classes": null, "length": 1947}
{"pred": "    The eight NER tasks they evaluated on are: BC5CDR, CHEBI, CHEMDNER, CLINICAL, DDI, GENIA, JNLPBA, and MIMIC. These tasks cover a range of biomedical domains, including diseases, chemicals, clinical notes, drug-drug interactions, and more. The evaluation was done on the same setup as the BioBERT paper, using the same datasets and hyperparameters. The results show that GreenBioBERT outperforms general-domain BERT on all tasks, with varying effect sizes. Depending on the points of reference, it covers an average of 52%", "answers": ["BC5CDR-disease, NCBI-disease, BC5CDR-chem, BC4CHEMD, BC2GM, JNLPBA, LINNAEUS, Species-800", "BC5CDR-disease, NCBI-disease, BC5CDR-chem, BC4CHEMD, BC2GM, JNLPBA, LINNAEUS, Species-800"], "all_classes": null, "length": 2800}
{"pred": "        The training data was translated using the machine translation platform Apertium.<translation platform Apertium.<translation platform Apertium.<translation platform Apertium.<translation platform Apertium.<translation platform Apertium.<translation platform Apertium.<translation platform Apertium.<translation platform Apertium.<translation platform Apertium.<translation platform Apertium.<translation platform Apertium.<translation platform Apertium.<translation platform Apertium.<translation platform Apertium.<translation platform Apertium.<translation platform Apertium.<translation platform Apertium.<translation platform Apertium.<translation platform Apertium", "answers": ["using the machine translation platform Apertium ", "machine translation platform Apertium BIBREF5"], "all_classes": null, "length": 2423}
{"pred": "    They used a multinomial Naive Bayes classifier.", "answers": ["AllWords model by counting the frequencies of all the remaining words and training a multinomial Naive Bayes classifier"], "all_classes": null, "length": 4177}
{"pred": "        The baseline for this task was a very simple logistic regression classifier with default parameters, where the input instances were represented by a single feature: the length of the sentence. The performance of this baseline on the SLC task is shown in Tables TABREF33 and TABREF34. davidjl\n\n        The baseline for this task was a very simple logistic regression classifier with default parameters, where the input instances were represented by a single feature: the length of the sentence. The performance of this baseline on the SLC task is shown in Tables TABREF33 and TABREF34. davidjl\n\n        The baseline for this task was a very simple logistic regression", "answers": ["The baseline system for the SLC task is a very simple logistic regression classifier with default parameters. The baseline for the FLC task generates spans and selects one of the 18 techniques randomly.", "SLC task is a very simple logistic regression classifier, FLC task generates spans and selects one of the 18 techniques randomly"], "all_classes": null, "length": 3001}
{"pred": "        The baselines they compare with are the conditional random fields (CRF) model and the pipeline method. The CRF model uses features like POS tags, n-grams, label transitions, word suffixes, and relative position to the end of the text to make predictions. The pipeline method uses a classifier for pun detection and a separate system for pun location. The authors also compare their results with previous works that did not employ joint learning. (10 lines)       \n\n        The baselines they compare with are the conditional random fields (CRF) model and the pipeline method. The CRF model uses features like POS tags, n", "answers": ["They compare with the following models: by Pedersen (2017), by Pramanick and Das (2017), by Mikhalkova and Karyakin (2017),  by Vadehra (2017), Indurthi and Oota (2017), by Vechtomova (2017), by (Cai et al., 2018), and CRF."], "all_classes": null, "length": 2991}
{"pred": "        The political bias of different sources is included in the model by assigning a political bias label to different US outlets following the procedure described in BIBREF2. In order to assess the robustness of the model, classification experiments are performed by training only on left-biased (or right-biased) outlets of both disinformation and mainstream domains and testing on the entire set of sources. Additionally, classification experiments are performed by excluding particular sources that outweigh the others in terms of samples to avoid over-fitting. The political bias of different sources is included in the model to account for the different behaviors of conservatives and liberals on online social platforms", "answers": ["By assigning a political bias label to each news article and training only on left-biased or right-biased outlets of both disinformation and mainstream domains", "we also account for political biases inherent to different news sources, referring to the procedure proposed in BIBREF2 to label different outlets. Overall we show that we are able to classify credible vs non-credible diffusion networks (and consequently news articles) with high accuracy (AUROC up to 94%), even when accounting for the political bias of sources (and training only on left-biased or right-biased articles). We observe that the layer of mentions alone conveys useful information for the classification, denoting a different usage of this functionality when sharing news belonging to the two news domains. We also show that most discriminative features, which are relative to the breadth and depth of largest cascades in different layers, are the same across the two countries."], "all_classes": null, "length": 4882}
{"pred": "        The ancient Chinese dataset comes from ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era. They used plain and accurate words to express what happened at that time, and thus ensure the generality of the translated materials.", "answers": ["ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era", "Ancient Chinese history records in several dynasties and articles written by celebrities during 1000BC-200BC collected from the internet "], "all_classes": null, "length": 3722}
{"pred": "    English\n\n   ...\n\n   ...\n\n   ...\n\n   ...\n\n   ...\n\n   ...\n\n   ...\n\n   ...\n\n   ...\n\n   ...\n\n   ...\n\n   ...\n\n   ...\n\n   ...\n\n   ...\n\n   ...\n\n   ...\n\n   ...\n\n   ...\n\n   ...\n\n   ...\n\n   ...\n\n   ...\n\n   ...\n\n   ...\n\n   ...\n\n   ...\n\n   ...\n\n   ...\n\n   ...\n\n   ...\n\n   ...\n\n   ...\n\n   ...\n\n   ...\n\n   ...\n\n   ...\n\n   ...\n\n   ...\n\n   ...\n\n   ...\n\n   ...\n\n   ...\n\n   ...\n\n   ...\n\n   ...\n\n   ...\n\n   ...\n\n   ...\n\n   ...\n\n   ...\n\n   ...\n\n   ...\n\n   ...\n\n   ...\n\n   ...\n\n   ...\n\n   ...\n\n   ...\n\n   ...\n\n   ...\n\n   ...\n\n   ", "answers": ["English", "English ", "English"], "all_classes": null, "length": 2240}
{"pred": "        The Chinese datasets used were the Penn Treebank (PTB) and the Penn Chinese Treebank (PCTB). The PTB is a collection of English sentences, while the PCTB is a collection of Chinese sentences. Both datasets are widely used in natural language processing research. The PTB is used for English grammar induction, while the PCTB is used for Chinese grammar induction. The PTB is also used for training neural language models, while the PCTB is used for training neural language models for Chinese. The PTB and PCTB are both available online for research purposes. The PTB is available at", "answers": ["Answer with content missing: (Data section) Chinese with version 5.1 of the Chinese Penn Treebank (CTB)"], "all_classes": null, "length": 2545}
{"pred": "        3 layers (user, topic, and comment) ", "answers": ["eight layers"], "all_classes": null, "length": 4487}
{"pred": "        The dataset used in this paper is the European network of nature protected sites Natura 2000 dataset, which contains information about the distribution of 100 species across Europe. Additionally, the European network of nature protected sites Natura 2000 dataset is used to predict soil type and CORINE land cover classes at levels 1, 2 and level 3. Finally, the crowdsourced dataset from the ScenicOrNot website is used to predict people's subjective opinions of landscape beauty in Britain. The dataset is used to evaluate the effectiveness of embeddings for predicting environmental features. (unanswerable) (yes) (no) (", "answers": [" the same datasets as BIBREF7", "same datasets as BIBREF7"], "all_classes": null, "length": 4661}
{"pred": "    The paper uses two clinical datasets: NUBes-PHI and MEDDOCAN. NUBes-PHI is a corpus of real medical reports written in Spanish and annotated with sensitive information. MEDDOCAN is a synthetic corpus of clinical cases enriched with sensitive information by health documentalists. The paper also uses a baseline system based on regular expressions and dictionaries to detect sensitive information. The paper compares the performance of the baseline system, a CRF classifier, a spaCy entity tagger, and a BERT-based system. The BERT-based system outperforms the other systems in terms of recall and F1-score. The paper", "answers": ["MEDDOCAN, NUBes-PHI", "MEDDOCAN, NUBes "], "all_classes": null, "length": 4528}
{"pred": "    Unigrams and Pragmatic features, Stylistic patterns, and patterns related to situational disparity. (BIBREF0, BIBREF1, BIBREF2, BIBREF3) (BIBREF4, BIBREF5) (BIBREF6, BIBREF7) (BIBREF8) (BIBREF9, BIBREF10) (BIBREF11) (BIBREF12) (BIBREF13) (BIBREF14) (BIBREF15) (BIBREF16) (BIBREF17) (BIBREF", "answers": ["Unanswerable"], "all_classes": null, "length": 3543}
{"pred": "    The metrics used to establish that this makes chatbots more knowledgeable and better at learning and conversation are average MCC and average +ve F1 score. These metrics are used to evaluate the predictive performance of the chatbot. The average MCC measures the quality of binary classification, while the average +ve F1 score measures the accuracy of predicting positive instances. These metrics are used to compare the performance of the chatbot with baseline models. The results show that the chatbot outperforms the baseline models in terms of predictive performance. The chatbot's ability to formulate query-specific inference strategies is also evaluated using a measure called Coverage. The results show", "answers": ["Coverage, Avg. MCC and avg. +ve F1 score", "strategy formulation ability, we introduce a measure called Coverage( INLINEFORM0 ), To evaluate the predictive performance, we use Avg. MCC and avg. +ve F1 score"], "all_classes": null, "length": 5869}
{"pred": "        No, they do not employ their indexing-based method to create a sample of a QA Wikipedia dataset. Instead, they use the existing datasets from the corpora they analyze. They then use their indexing-based method to map the answer contexts from all corpora to the same version of Wikipedia so they can be coherently used for answer retrieval. They also use their indexing-based method to create a silver-standard dataset for answer retrieval and triggering. (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes", "answers": ["Yes", "No"], "all_classes": null, "length": 1910}
{"pred": "    Galatasaray and Fenerbahçe are the targets. (Fenerbahçe is also known as Fenerbahçe Spor Kulübü.) (Galatasaray is also known as Galatasaray Spor Kulübü.) (Fenerbahçe is also known as Fenerbahçe Spor Kulübü.) (Galatasaray is also known as Galatasaray Spor Kulübü.) (Fenerbahçe is also known as Fenerbahçe Spor Kulübü.) (Galatasaray is also known as Galatasaray Spor Kulübü.) (Fenerbahçe is", "answers": ["Galatasaray, Fenerbahçe", "Galatasaray , Fenerbahçe "], "all_classes": null, "length": 2234}
{"pred": "        The experiments conducted are automatic evaluations and human evaluations. The automatic evaluations include sentiment delta, sentiment accuracy, BLEU score, geometric mean, and harmonic mean. The human evaluations include irony accuracy, sentiment preservation, and content preservation. The experiments are conducted to evaluate the performance of the model in generating ironic sentences. davidjl\n\n        The experiments conducted are automatic evaluations and human evaluations. The automatic evaluations include sentiment delta, sentiment accuracy, BLEU score, geometric mean, and harmonic mean. The human evaluations include irony accuracy, sentiment preservation, and content preservation. The experiments are conducted to evaluate the performance of the model in generating ironic sentences. davidjl\n\n", "answers": ["Irony Classifier, Sentiment Classifier for Irony, Sentiment Classifier for Non-irony, transformation from ironic sentences to non-ironic sentences"], "all_classes": null, "length": 4600}
{"pred": "Gaussian-masked directional multi-head attention works by adjusting the weight between characters and their adjacent character to a larger value which stands for the effect of adjacent characters. The Gaussian weight only relies on the distance between characters and is calculated using the cumulative distribution function of Gaussian. The Gaussian weight matrix is used to combine the Gaussian weight to the self-attention. The Gaussian-masked attention ensures that the relationship between two characters with long distances is weaker than adjacent characters. The Gaussian-masked attention is used in the forward and backward encoders to capture information of two directions which correspond to two parts divided by the gap. The central encoder is a special directional", "answers": ["pays attentions to adjacent characters and casts a localness relationship between the characters as a fixed Gaussian weight assuming the weight relies on the distance between characters", "Gaussian-masked directional attention can be described as a function to map queries and key-value pairs to the representation of input, Gaussian-masked directional attention expects to pay attention to the adjacent characters of each positions and cast the localness relationship between characters as a fix Gaussian weight for attention, Gaussian weight only relys on the distance between characters"], "all_classes": null, "length": 3612}
{"pred": "        Facebook status update messages.", "answers": ["Facebook status update messages", "Facebook status update messages"], "all_classes": null, "length": 4005}
{"pred": "    The baseline features are the features extracted from the fully-connected layer of the baseline CNN. These features are used to train the final softmax layer of the baseline CNN. The baseline features are the features that the baseline CNN learns to identify sarcastic and non-sarcastic tweets. These features are the inherent semantics of the sarcastic corpus that the baseline CNN learns to extract. The baseline features are the features that the baseline CNN learns to identify sarcastic and non-sarcastic tweets. These features are the inherent semantics of the sarcastic corpus that the baseline CNN learns to extract. The baseline features are the features that the baseline CNN learns", "answers": [" The features extracted from CNN."], "all_classes": null, "length": 4855}
{"pred": "        The number of clusters and the type of word vectors were varied in the experiments on the four tasks. The number of clusters was varied between 250 and 1000, and the type of word vectors was varied between skipgram, cbow, and GloVe. The hyperparameters were varied to determine the optimal settings for each task. The optimal settings were determined based on the performance of the learning algorithms on the test data. The optimal settings for each task are presented in the results tables.       ยนตร\n\n        The number of clusters and the type of word vectors were varied in the experiments on the four tasks. The number of clusters was", "answers": ["number of clusters, seed value in clustering, selection of word vectors, window size and dimension of embedding", "different number of clusters, different embeddings"], "all_classes": null, "length": 2753}
{"pred": "        0.716, 0.718, 0.716, 0.718, 0.716, 0.718, 0.716, 0.718, 0.716, 0.718, 0.716, 0.718, 0.716, 0.718, 0.716, 0.718, 0.716, 0.718, 0.716, 0.718, 0.716, 0.718, 0.716, 0.718, 0.716, 0", "answers": ["column Ens Test in Table TABREF19"], "all_classes": null, "length": 2424}
{"pred": "        53 documents, with an average of 156.1 sentences per document and 167,739 words in total. The corpus consists of 8,275 sentences and 167,739 words in total. The number of annotated entities is summarized in Table TABREF24. The most frequently annotated type of entity is findings, with an average length of 2.6 tokens. The number of tokens per entity ranges from one token for all types to 5 tokens for cases (average length 3.1), nine tokens for conditions (average length 2.0), 16 tokens for factors (average length 2.5", "answers": ["8,275 sentences and 167,739 words in total", "The corpus comprises 8,275 sentences and 167,739 words in total."], "all_classes": null, "length": 2669}
{"pred": "    Yes, it is possible to convert a cloze-style questions to a naturally-looking questions by replacing the missing spans of text with the correct answers. This process is known as cloze completion. However, it is important to note that the resulting questions may not be as natural as the original questions, and may require additional context to be fully understood. Additionally, the process of converting cloze-style questions to naturally-looking questions may introduce biases and errors that need to be addressed. Therefore, it is important to carefully consider the trade-offs between the benefits and drawbacks of this approach when using it in practice.", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 2764}
{"pred": "    Text categorization and sentiment classification.", "answers": ["text classification for themes including sentiment, web-page, science, medical and healthcare"], "all_classes": null, "length": 3591}
{"pred": "        The previous methods are term frequency models, rule-based methods, and learned methods. The model is compared to the best previous models on the TREC and GARD datasets. The model also achieves state-of-the-art performance on the MLBioMedLAT dataset. (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no", "answers": ["bag-of-words model, CNN"], "all_classes": null, "length": 5838}
{"pred": "        The training sets of these versions of ELMo are significantly larger compared to the previous ones. The previous versions of ELMo were trained on corpora of 20 million tokens, while the new versions of ELMo are trained on corpora of 270 million tokens (for Latvian) and 280 million tokens (for Estonian). The larger training sets allow the models to capture more semantic and contextual information, resulting in better performance on downstream tasks. (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswerable) (yes) (no)", "answers": ["By 14 times.", "up to 1.95 times larger"], "all_classes": null, "length": 3294}
{"pred": "        6946 sentences\n\n        16225 unique words\n\n        3 major classes (Person, Location, Organization)\n\n        299 post-positions\n\n        10% F1 score improvement after lemmatizing post-positions\n\n        95.14% accuracy of POS-tagged BiLSTM model\n\n        64%, 16%, 20% of total dataset into training, development, and test sets\n\n        72782 unique words in word embeddings\n\n        300 dimensions for word embeddings\n\n        10 window context for word embeddings\n\n        5 rare words dropped for word embeddings\n\n        1-", "answers": ["3606", "6946"], "all_classes": null, "length": 2835}
{"pred": "        Eusboost and MWMOTE techniques.       \n\n        Eusboost and MWMOTE techniques.       \n\n        Eusboost and MWMOTE techniques.       \n\n        Eusboost and MWMOTE techniques.       \n\n        Eusboost and MWMOTE techniques.       \n\n        Eusboost and MWMOTE techniques.       \n\n        Eusboost and MWMOTE techniques.       \n\n        Eusboost and MWMOTE techniques.       \n\n        Eusboost and MWMOTE techniques.       \n\n        Eusboost and MWMOTE techniques.       \n\n        Eusboost and MWMOTE", "answers": ["MLP", "Eusboost, MWMOTE"], "all_classes": null, "length": 2474}
{"pred": "Yes, their NER model learns NER from both text and images. The model takes as input a sequence of tokens, each of which comprises a word token, a character sequence, and an image, in their respective representation. The model then uses a modality attention module to combine representations from each modality to produce an entity label for each token. The modality attention module learns a unified representation space for multiple available modalities (words, characters, images, etc.), and produces a single vector representation with aggregated knowledge among multiple modalities, based on their weighted importance. The model is trained via log-likelihood maximization for the training", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3784}
{"pred": "        Yes, they evaluate only on English datasets. They use the Wall Street Journal (WSJ) portion of the Penn Treebank for both POS tagging and dependency parsing. The skip-gram word embeddings are trained on the one billion word language modeling benchmark dataset in addition to the WSJ corpus. The experiments are conducted on the WSJ corpus, which is a collection of news articles from the Wall Street Journal. The authors do not mention evaluating on any other datasets. Therefore, it can be concluded that they only evaluate on English datasets. (Note: This answer is based on the information provided in the article and may not reflect the actual evaluation", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 4320}
{"pred": "        32% (in Batch 2)       \n\n        32% (in Batch 2)       \n\n        32% (in Batch 2)       \n\n        32% (in Batch 2)       \n\n        32% (in Batch 2)       \n\n        32% (in Batch 2)       \n\n        32% (in Batch 2)       \n\n        32% (in Batch 2)       \n\n        32% (in Batch 2)       \n\n        32% (in Batch 2)       \n\n        32% (in Batch ", "answers": ["0.5115", "0.6103"], "all_classes": null, "length": 6810}
{"pred": "    The datasets they evaluate on are the Wall Street Journal (WSJ) portion of the Penn Treebank for both POS tagging and dependency grammar induction. They use skip-gram word embeddings trained on the one billion word language modeling benchmark dataset and the WSJ corpus. They also use fastText embeddings trained on the WSJ corpus. davidjl\n\nQuestion: What is the experimental setup for the unsupervised POS tagging task?\n\nAnswer: overposting\n\n    The experimental setup for the unsupervised POS tagging task is as follows: They train and test on the entire WSJ corpus (49208 sentences, 1M tokens). They use 45", "answers": [" Wall Street Journal (WSJ) portion of the Penn Treebank", "Unanswerable"], "all_classes": null, "length": 4327}
{"pred": "        The authors cite several studies that show that engineers often face challenges when applying DNN models to specific NLP tasks. These challenges include the need to choose from multiple frameworks, models and optimization techniques, which can be time-consuming and error-prone. The authors also provide examples of these challenges, such as the need to choose between different embedding layers, neural network layers, loss functions, and metrics. The authors conclude that these challenges often hinder the productivity of engineers and result in less optimal solutions to their given tasks. (unanswerable) (yes) (no) (unanswerable) (yes) (no) (unanswer", "answers": ["By conducting a survey among engineers", "Unanswerable"], "all_classes": null, "length": 1692}
{"pred": "        On SimpleQuestions and WebQSP. davidjl\n\n        On SimpleQuestions and WebQSP. davidjl\n\n        On SimpleQuestions and WebQSP. davidjl\n\n        On SimpleQuestions and WebQSP. davidjl\n\n        On SimpleQuestions and WebQSP. davidjl\n\n        On SimpleQuestions and WebQSP. davidjl\n\n        On SimpleQuestions and WebQSP. davidjl\n\n        On SimpleQuestions and WebQSP. davidjl\n\n        On SimpleQuestions and WebQSP. davidjl\n\n        On SimpleQuestions and WebQSP. davidjl\n\n        On SimpleQuestions and WebQSP. davidjl\n\n        On SimpleQuestions and WebQ", "answers": ["SimpleQuestions, WebQSP", "WebQSP, SimpleQuestions"], "all_classes": null, "length": 4524}
